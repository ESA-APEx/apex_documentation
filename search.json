[
  {
    "objectID": "propagation/enhancement.html",
    "href": "propagation/enhancement.html",
    "title": "Algorithm Enhancement",
    "section": "",
    "text": "APEx supports scientists and algorithm developers in transforming their research code into a performant and stable piece of software that can be run in an operational setting. These enhancement activities ensure code quality, maintainability and performance, whether applied to the source code itself or its service implementation.\nTo ensure performant execution on an APEx-compliant EO platform, offering openEO or OGC API Processes, additional optimisation dedicated to the platform capabilities may be needed. The exact requirements, steps, and effort involved in this process can vary heavily from case to case due to the variations in initial implementations and operational requirements.\nIn the case of open source algorithms, and with the agreement of the algorithm provider, the code of the enhanced version of the algorithm will be published in a dedicated APEx GitHub repository, and algorithm providers are encouraged to use it as a basis for further development of the algorithm. This approach ensures that the version hosted on APEx can be kept up to date.\nThe algorithm enhancement is executed in two phases. In the first phase, the algorithm provider submits their existing implementation, including source code, input data, and instructions to build, install and execute their code, together with details of any external dependencies to the APEx enhancement team. The enhancement team will then analyse the algorithm and estimate the effort for the envisioned activities, which results in an offer. In the second phase, on acceptance of the offer by the provider and by ESA, the enhancement is performed. As a result, the algorithm provider receives an optimised version of their algorithm code with high code quality. Optionally, the enhancement procedure can include a porting or packaging step, supported by the APEx Algorithm Porting and Algorithm Onboarding activities, where the algorithm is implemented as an openEO User Defined Process (UDP) or Application Package and hosted on an existing platform. This approach ensures that the enhanced algorithm remains available as an on-demand service.",
    "crumbs": [
      "Algorithm Services",
      "Algorithm Enhancement"
    ]
  },
  {
    "objectID": "propagation/enhancement.html#support-overview",
    "href": "propagation/enhancement.html#support-overview",
    "title": "Algorithm Enhancement",
    "section": "Support Overview",
    "text": "Support Overview\n\nFeatures\nThe algorithm enhancement activities offer the following features:\n\nAnalysis of the current implementation and performance.\nOptimisation with respect to computational efficiency. This may include switching to a distributed execution model or moving suitable parts of the algorithm to GPU and improving code quality.\nOptimisation considering the capabilities of target processing platforms and use of existing services.\nExploitation of algorithms available on APEx where appropriate, e.g., replacing custom functions with equivalent pre-optimized functions.\nDelivery of an enhanced implementation that is ready to be registered in the APEx Algorithm Services Catalogue.\n\n\n\nHow to get started?\nTo get started with APEx’s algorithm enhancement activities, the first step is to deliver the current algorithm implementation, together with the complementary information that is required to make use of it. The necessary components are:\n\nthe source code\ndocumentation of dependencies\nbuild, installation and execution instructions\ninput data and expected output data\nperformance issue and optimisation goal\n\nOnce this information has been provided, the APEx enhancement team will perform a two-phase analysis. The analysis consists of a briefer, standardised analysis phase, followed by a more thorough cost and effort estimate. On acceptance, the enhancement team will begin work to enhance the algorithm. The exact process of enhancement depends on the nature of the initial algorithm and its implementation.\nIf the algorithm was originally implemented in accordance with the APEx Interoperability and Compliance Guidelines, the process can be more straightforward, as, in this case, there will be more overlap between libraries and design patterns used in the initial implementation and those required for an optimised APEx version. The enhancement process will be different, and fewer changes will have to be made to the code base.\n\n\nIntellectual Property Rights\nThe intellectual property rights to their algorithm implementation remain with the algorithm authors. If the algorithm is an implementation of published research, the APEx team reserves itself the non-exclusive right to run and further develop the enhanced implementation. The algorithm provider has the right and is encouraged to use the enhanced code as a basis for further algorithm development. For proprietary algorithms, the rights to publish the source code and run the algorithm remain with the original property rights holder..",
    "crumbs": [
      "Algorithm Services",
      "Algorithm Enhancement"
    ]
  },
  {
    "objectID": "propagation/service_development.html",
    "href": "propagation/service_development.html",
    "title": "Algorithm Service Development Options",
    "section": "",
    "text": "APEx currently offers two main options for projects to develop their on-demand services:\nThese APEx-compliant technologies allow algorithms to be hosted on an APEx-compliant algorithm hosting platform and make them available for execution through web services. These technologies promote seamless reuse and integration of existing EO algorithms. Additionally, by leveraging web services and cloud-based approaches, these technologies simplify the execution of EO algorithms, shielding users from underlying complexities, such as data access, data processing optimisation, and other technical challenges.\nAPEx remains committed to future innovation and is open to integrating additional specifications, provided they align with FAIR principles and facilitate algorithm execution through web services.\nThe sections below provide a brief overview of each technology and outline various scenarios for their potential application.",
    "crumbs": [
      "Interoperability and Compliance Guidelines",
      "Algorithm Service Development Options"
    ]
  },
  {
    "objectID": "propagation/service_development.html#openeo-user-defined-processes",
    "href": "propagation/service_development.html#openeo-user-defined-processes",
    "title": "Algorithm Service Development Options",
    "section": "openEO User Defined Processes",
    "text": "openEO User Defined Processes\nWhen an EO application can be expressed in terms of the standardised openEO processes (combined in a process graph), it can also be parametrised so that it effectively becomes a service that can be executed by an openEO backend. This is what we call a User Defined Process (UDP) [2].\n\nWhen to use openEO User Defined Processes?\nThis option is a good choice when writing EO algorithms from scratch or when using Python libraries that support numpy or XArray data structures, which includes many machine learning frameworks. Therefore, it is also fairly common for projects to port their existing pure-Python algorithm into an openEO process graph. While this porting involves learning to understand the concepts of datacube-based processing and the openEO processes, it does provide some benefits.\nThe standardised, datacube-based processing approach is designed to significantly reduce the responsibilities of the EO data scientist writing the algorithm. The openEO backend takes care of data access, solving performance and stability issues while also dealing with the multiple heterogeneous formats that are used in earth observation. Backends will automatically parallelise the processing work using state-of-the-art technologies, as provided by Apache Spark or Pangeo.\nMany common maintenance operations are performed by the openEO backend rather than by the EO service provider. This includes integrating performance improvements, new versions of software libraries with bug fixes, or adjusting to changes in the EO datasets.\nVarious complex preprocessing steps may be offered by the backend. Examples include backscatter computation for Sentinel-1 or cloud masking and compositing of optical data. Use cases like machine learning training data extraction are also supported.\nWhen EO algorithms are expressed as process graphs, they also become easier to share and analyse by peers if the project chooses to make them public. Transparent algorithms can greatly help to increase confidence in the results or even to receive suggestions for improvements.\n\n\nExample use cases\nA set of platform-agnostic example cases that showcase how openEO supports various cases can be found in the openEO community examples repository. APEx recommends browsing the notebooks available there to get a sense of what openEO can do or even to find a starting point for the project’s own algorithm.\nIn addition to that, we list testimonials of ESA projects that successfully used openEO in the past.\n\nESA WorldCereal\nThe ESA WorldCereal project effectively built a system on top of openEO that enables the training of custom machine learning models for crop type detection anywhere in the world. The created models can then be used within openEO to generate maps at a large scale. The system is very easy to use because all the complex computation and data access is performed in the cloud.\nThe WorldCereal workflow for crop type map production contains these steps:\n\nGeneration on monthly Sentinel-2 median composites for 1 year of input data.\nGeneration of monthly Sentinel-1 backscatter composites from raw GRD input for 1 year of input data filtered by orbit direction.\nLoading DEM, slope, and AGERA5 datasets.\nComputation of features based on all the above inputs using a foundation model.\nInference using a (user-defined) CatBoost model.\nOutput to FAIR GeoTIFFs with STAC metadata.\n\n\n\nESA ANIN\nThe ESA ANIN project computed a combined drought index for South Africa using openEO. It successfully combined data from a variety of sources and applied rule-based techniques to compute the result.\n\n\nESA WorldWater\nThe ESA WorldWater project converted their decision algorithm based on Sentinel-1 and Sentinel-2 into openEO. This allowed them to run their water detection algorithm anywhere in the world using a cloud-based system.\n\n\nESA Luisa\nLuisa used openEO to compute biophysical parameters such as LAI and fAPAR from Sentinel-2 data over various test areas in Africa.",
    "crumbs": [
      "Interoperability and Compliance Guidelines",
      "Algorithm Service Development Options"
    ]
  },
  {
    "objectID": "propagation/service_development.html#ogc-application-package-and-ogc-api-processes",
    "href": "propagation/service_development.html#ogc-application-package-and-ogc-api-processes",
    "title": "Algorithm Service Development Options",
    "section": "OGC Application Package and OGC API Processes",
    "text": "OGC Application Package and OGC API Processes\nThe OGC API Processes specification [3] allows you to expose any type of (processing) web service. As a service designer, you get full freedom and responsibility in the definition of your service. This large degree of freedom also implies that the definition of your service will affect its interoperability with other services and tools.\nAPEx recommends that these services are built based on the OGC Application Package best practice [4]. This allows service providers to easily host their applications on a compatible hosting platform, taking away key IT challenges.\n\nWhen to use OGC Application Packages?\nAn OGC Application Package is a good choice when you have an existing piece of software that you want to make available as a service and do not aim to make substantial changes. In effect, it is a packaged version of your software, using (Docker) containers, which is a well-known technology for most IT professionals. A large amount of existing EO applications are known to already use containers. The container is invoked by a workflow written in the ‘common workflow language’ (CWL) [5], which is a thin wrapper around your software.\nIf concepts like containers are already familiar to the project, and they prefer to have full control and responsibility over their software, including all details concerning how they read raw EO products, then this might be the right choice. Of course, the APEx Algorithm Services exist to help you with the integration of your service into the APEx ecosystem.\nTo offer your application as an on-demand service, the project will need to define clear constraints on possible input parameters and make sure that their software is well-tested to be able to handle variations in input parameters. When changes occur, for instance, to raw EO data sources, the project will need to update its software accordingly, or it might no longer be compatible with the provided inputs. Likewise, if improvements are made to dependencies of the project’s software, they will have the choice of updating its software or sticking to the old versions.",
    "crumbs": [
      "Interoperability and Compliance Guidelines",
      "Algorithm Service Development Options"
    ]
  },
  {
    "objectID": "propagation/index.html",
    "href": "propagation/index.html",
    "title": "From Earth Observation Applications to Services",
    "section": "",
    "text": "One of the primary goals of APEx is to facilitate the transition of Earth Observation (EO) R&D results into reusable, scalable, cloud-based on-demand services. Currently, ESA EOP R&D projects generate a variety of results, including, for example, EO-derived value-added products (raster or vector files, typically level-3 and level-4), workflows and algorithms, or conventional (open-source) desktop software solutions. The APEx Algorithm Services focus on transforming and optimising (i.e., propagating) resulting algorithms, applications, workflows and toolboxes and ensuring their compliance with FAIR data principles. The preservation of these project outcomes is managed through the ESA Project Results Repository (PRR), supported by other APEx services, such as the APEx Data Catalogue.\nThe vision for the ESA APEx initiative is to adopt FAIR data principles and ensure that project results remain Findable and Accessible and that they become Interoperable and Reusable. For value-added products, this mainly involves ingestion in an ESA-controlled persistent storage solution and exposing these products accordingly (using STAC and other technologies).\nMaintaining workflows and algorithms in a FAIR-compliant way requires the adoption of cloud service concepts and implementations. Therefore, APEx envisioned a solution for workflows and algorithms that would make them available in a public cloud (platform) in co-location with the key EO data archives as on-demand services using standardised web service APIs. Users can then trigger these web services with their user-specific parametrisation to produce results for an area and time frame of interest.\nHowever, developing on-demand services can be challenging for many projects and depends heavily on the complexity of a workflow/algorithm and its initial implementation. APEx intends to facilitate the transition to cloud-based services by providing guidelines, best practices, and tools that support EO R&D projects in overcoming these challenges.\nThe algorithm services support provided by APEx involves various subcomponents, and their relevance depends on a case-by-case basis. The APEx Algorithm Services comprise:\n\nRefactoring source code to an openEO process graph\nPackaging of source code to an OGC Application Package\nIntegration of a service into the APEx Algorithm Services Catalogue\nCloudification of desktop toolboxes through a tailored approach\nTechnical performance optimisation, both on the source code or service level\nDeployment and hosting of a service in an existing EO platform\nOnboarding of a service commercial offering onto the ESA Network of Resources\nPreserving and ensuring long-term accessibility via the ESA Project Results Repository (PRR)\n\nThis support can also be visualised in a chart, illustrated in Figure 1. This chart outlines the different pathways for projects to transform and optimise their algorithms into on-demand services using the APEx Algorithm Services.\nThe main goal of the Algorithm Services is to guide projects in defining their pathway and providing support throughout the various steps. This is accomplished through an initial analysis of the project’s needs, resulting in a tailored pathway that assists in the transformation from source code to on-demand EO services.\n\n\n\n\n\n\nflowchart TD \"\"\n    subgraph Legend\n      APEX_SERVICE[Activity supported through APEx]\n    end\n    A((Project Source Code)) --&gt; B{Experiencing Performance Issues?}\n    B --&gt;|Yes| C[Algorithm Enhancement Support]\n    B --&gt;|No| D{Need help with implementation of APEx Guidelines?}\n    D --&gt;|Yes| E[Support for Porting or Packaging Your Algorithm]\n    D --&gt;|No| F{Expose your Algorithm as an On-Demand Service?}\n    F --&gt;|Yes| G[Support with Deploying your Algorithm on a Hosting Platform]\n    G --&gt; H(((On-Demand EO Service)))\n    H --&gt; I{Use EO Services for Large-Scale Processing?}\n    I --&gt;|Yes| J[Tools and Environment for Upscaling]\n\n    class APEX_SERVICE,C,E,G,J apexService\n    click C href \"./enhancement.html\" _blank\n    click E href \"./porting.html\" _blank\n    click G href \"./onboarding.html\" _blank\n    click J href \"./upscaling.html\" _blank\n    classDef default font-size:10pt;\n\n\n\n\nFigure 1: APEx Algorithm Support Pathways",
    "crumbs": [
      "Algorithm Services"
    ]
  },
  {
    "objectID": "propagation/usecases.html",
    "href": "propagation/usecases.html",
    "title": "Use Cases for APEx EO Service Support",
    "section": "",
    "text": "The APEx Algorithm Services aim for FAIR compliance of algorithms/applications and are designed to enhance the efficiency, interoperability, and scalability of EO workflows and algorithms. Below are several use cases that demonstrate how APEx can support EO projects.",
    "crumbs": [
      "Algorithm Services",
      "Use Cases"
    ]
  },
  {
    "objectID": "propagation/usecases.html#use-case-1-transforming-functionality-of-an-existing-toolbox-into-on-demand-apex-compliant-services",
    "href": "propagation/usecases.html#use-case-1-transforming-functionality-of-an-existing-toolbox-into-on-demand-apex-compliant-services",
    "title": "Use Cases for APEx EO Service Support",
    "section": "Use Case 1: Transforming functionality of an existing toolbox into on-demand, APEx-compliant services",
    "text": "Use Case 1: Transforming functionality of an existing toolbox into on-demand, APEx-compliant services\n\nScenario\nBoth of the following scenarios are supported:\n\nA project discovered a toolbox to perform EO data processing tasks and wants to integrate a specific set of features from this toolbox into a cloud-based service implementation, typically without a clear preference for either an openEO processing graph or use it as an OGC Application Package.\nESA receives a request from the EO community to make the functionality of a specific EO toolbox available as cloud-based services.\n\n\n\nSolution\nThe APEx Toolbox Cloudification activities aim to convert a subset of features (e.g. analytical or processing functionality that’s unique to the toolbox) from an existing toolbox into cloud-based services that comply with APEx standards, enabling seamless integration with cloud resources and existing platforms. It is important to note that the cloudification efforts focus on the data transformation and analytics functions provided by the toolbox and do not include user interface elements or data I/O (which would typically be replaced with existing cloud service functionality).\n\n\nOutcome\nAPEx performs an initial analysis of the toolbox, defines a cloudification strategy and estimates the effort and budget required. Once ESA approves the request, the APEx team performs the development work. Key features of the toolbox become accessible via an openEO process or as an OGC Application Package, allowing the project and/or EO community to utilise these features as reusable, on-demand services on existing hosting platforms.",
    "crumbs": [
      "Algorithm Services",
      "Use Cases"
    ]
  },
  {
    "objectID": "propagation/usecases.html#use-case-2-technical-algorithm-performance-enhancement",
    "href": "propagation/usecases.html#use-case-2-technical-algorithm-performance-enhancement",
    "title": "Use Cases for APEx EO Service Support",
    "section": "Use Case 2: Technical algorithm performance enhancement",
    "text": "Use Case 2: Technical algorithm performance enhancement\n\nScenario\nA project is experiencing performance issues with an algorithm. Both of the following scenarios are supported:\n\nThe algorithm is available at the service level, i.e., it is already implemented in openEO or packaged as an OGC Application Package.\nThe algorithm is available at the native source code level, i.e., it has not yet been implemented into openEO or packaged as an OGC Application Package.\n\n\n\nExamples\n\nThe project has created a workflow/algorithm as an openEO processing graph. Executing this workflow takes too long and incurs high execution costs, not making it viable for running at the intended scale.\nThe project created a new machine learning model in Python that performs inference based on EO data. However, the training and/or inference steps take too long, hindering scalability for processing larger areas of interest.\nThe project uses an established EO algorithm that lacks performance or consumes too much RAM in the application case and that would greatly benefit from transitioning into a cloud-service or GPU-based implementation\n\n\n\nSolution\nAPEx offers support to optimise algorithms through a structured analysis and improvement process. This may include steps such as refactoring or algorithm porting. In the case of scenario 2, the APEx team will also assess whether performance can be enhanced by integrating the algorithm into APEx-compliant technologies, such as openEO or OGC Application Package. If so, the APEx Algorithm Porting support can be recommended to enable the transformation of an algorithm into an openEO UDP or package it as an OGC Application Package. The algorithm optimisation can happen at native source code (e.g. Python, Fortran) or service implementation level (i.e., openEO, Application Package).\n\n\nOutcome\nThe algorithm’s performance is significantly improved.",
    "crumbs": [
      "Algorithm Services",
      "Use Cases"
    ]
  },
  {
    "objectID": "propagation/usecases.html#use-case-3-transforming-an-existing-eo-data-processing-algorithm-workflow-or-application-into-an-openeo-udp-or-ogc-application-package",
    "href": "propagation/usecases.html#use-case-3-transforming-an-existing-eo-data-processing-algorithm-workflow-or-application-into-an-openeo-udp-or-ogc-application-package",
    "title": "Use Cases for APEx EO Service Support",
    "section": "Use Case 3: Transforming an existing EO data processing algorithm, workflow or application into an openEO UDP or OGC Application Package",
    "text": "Use Case 3: Transforming an existing EO data processing algorithm, workflow or application into an openEO UDP or OGC Application Package\n\nScenario\nA project has developed an algorithm, workflow or application using non-APEx-compliant technology and seeks to convert it into an openEO User Defined Process (UDP) or package it as an OGC Application Package.\n\n\nExamples\n\nThe project has created an EO data processing pipeline for generating a new air quality index and wants to encapsulate it in an openEO UDP or OGC Application Package to facilitate its execution and reuse by the user community.\n\n\n\nSolution\nAPEx offers a set of initial guidelines and online resources to help projects integrate their algorithm, processing workflow or application into an APEx-compliant technology. Should additional assistance be required, the APEx can provide further Algorithm Porting support. After an initial analysis, APEx experts will propose an appropriate APEx-compliant technology and assist the project in its implementation.\n\n\n\n\n\n\nWant to share your algorithm with the APEx community?\n\n\n\nPlease refer to Use Case 4 to learn how APEx can support you in sharing your algorithm with the community.\n\n\n\n\nOutcome\nThe algorithm, workflow or application is successfully transformed and is now available on a cloud platform and in the APEx Algorithm Services Catalogue as an openEO UDP or OGC API Process.",
    "crumbs": [
      "Algorithm Services",
      "Use Cases"
    ]
  },
  {
    "objectID": "propagation/usecases.html#use-case-4-make-an-algorithm-available-in-the-apex-algorithm-services-catalogue",
    "href": "propagation/usecases.html#use-case-4-make-an-algorithm-available-in-the-apex-algorithm-services-catalogue",
    "title": "Use Cases for APEx EO Service Support",
    "section": "Use Case 4: Make an algorithm available in the APEx Algorithm Services Catalogue",
    "text": "Use Case 4: Make an algorithm available in the APEx Algorithm Services Catalogue\n\nScenario\nA project has implemented an algorithm as an openEO User Defined Process (UDP) or packaged it as an OGC Application Package (or had it ported to one of these two options) and wants to make it available to the APEx community.\n\n\n\n\n\n\nYour algorithm is not yet available as an openEO UDP or packaged as an OGC Application Package?\n\n\n\nIf your algorithm has not yet been implemented in an APEx-compliant technology, please refer to the previous use case to discover how APEx can provide support.\n\n\n\n\nExamples\n\nThe project has created an openEO UDP or OGC Application Package that calculates a new air quality index and wants to share this service with the EO community.\nThe project has developed a new machine learning model encapsulated in an openEO UDP or Application Package and wants to ensure it runs stably and within a controlled cost range.\nThe project has developed an OGC Application Package and is seeking support to host it on an APEx-compliant platform.\n\n\n\nSolution\nAPEx will provide the necessary tools and comprehensive support for integrating an algorithm into the APEx Algorithm Services Catalogue, ensuring its availability for reuse within the EO community. The support also includes:\n\nAssistance in selecting and onboarding an APEx-compliant algorithm hosting platform to guarantee that an algorithm is available for execution as an on-demand service.\nProvision of guidelines and tools for automated testing and benchmarking of the service.\nPublishing guidelines and documentation for registering an algorithm in the APEx Algorithm Services Catalogue.\n\n\n\nOutcome\nThe algorithm is available in the APEx Algorithm Services Catalogue, enhancing its visibility and usability within the EO community as an APEx-compliant service",
    "crumbs": [
      "Algorithm Services",
      "Use Cases"
    ]
  },
  {
    "objectID": "propagation/usecases.html#use-case-5-generating-large-scale-value-added-products-or-maps-based-on-on-demand-eo-services",
    "href": "propagation/usecases.html#use-case-5-generating-large-scale-value-added-products-or-maps-based-on-on-demand-eo-services",
    "title": "Use Cases for APEx EO Service Support",
    "section": "Use Case 5: Generating large-scale value-added products or maps based on on-demand EO services",
    "text": "Use Case 5: Generating large-scale value-added products or maps based on on-demand EO services\n\nScenario\nA project is currently testing and validating a data processing pipeline based on openEO or OGC Application Package over a limited geographical area and aims to upscale this pipeline for large-scale data processing.\n\n\n\n\n\n\nIs your pipeline not yet implemented in openEO or packaged as an OGC Application Package?\n\n\n\nAPEx provides a set of initial guidelines and online resources to help you get started. If your project requires additional assistance in making the data processing pipeline APEx-compliant, additional APEx Algorithm Porting support is available.\n\n\n\n\nExamples\n\nThe project has developed a workflow as an openEO processing graph or OGC API Process to generate a new air quality index. So far, the testing and validation of this workflow have been carried out over a limited geographical area, and the project now aims to generate the index at the country/regional level.\nThe project has discovered an algorithm in the APEx Algorithm Services Catalogue for generating a new air quality index and aims to execute this algorithm on all historical data for the entire continent of Europe.\n\n\n\nSolution\nAPEx offers tools and support to assist projects in automating and scaling their data processing pipelines. These tools may include:\n\nProject-Specific Workspace\nA JupyterLab environment allowing users to start and monitor long-running jobs.\nIntelligent Job Distribution Tools\nTools for managing multiple job executions\n\n\n\nOutcome\nThe project team receives support and tools that enable them to start processing, monitor progress, and successfully generate large-area value-added products.",
    "crumbs": [
      "Algorithm Services",
      "Use Cases"
    ]
  },
  {
    "objectID": "propagation/toolboxcloud.html",
    "href": "propagation/toolboxcloud.html",
    "title": "Toolbox Cloudification",
    "section": "",
    "text": "The toolbox cloudification activities within APEx make individual data processing procedures from existing toolboxes available as cloud-based algorithms compliant with APEx. This makes it possible to freely combine functionalities from multiple toolboxes as well as other APEx-compliant algorithms. The service focuses solely on the data-processing capabilities of toolboxes and does not include visualisation features. Instead of the local processing scheme of a traditional toolbox, key operations can be performed in the cloud. Furthermore, functions from various toolboxes can be combined in a single workflow by using one of the APEx-compliant technologies.\nProjects may select functionalities from a toolbox that they would like to have access to as APEx-compliant algorithms. The proposed functionalities and their toolbox will then be reviewed by ESA and APEx experts. Effort estimation and an offer will be made following the review. On acceptance of the offer, the requested functionalities will be provided as an Application Package according to the OGC best practice or as an openEO User Defined Process (UDP).\nTo ensure interoperability between procedures from different toolboxes, it is necessary to share data using standard data formats when writing intermediary files to disk. Users should consider if the flexibility of this approach outweighs the performance drawbacks and request deployment as a single, integrated package instead if performance is critical.\nFunctionalities made available through toolbox cloudification are ready to be hosted as APEx-compliant on-demand services.",
    "crumbs": [
      "Algorithm Services",
      "Toolbox Cloudification"
    ]
  },
  {
    "objectID": "propagation/toolboxcloud.html#support-overview",
    "href": "propagation/toolboxcloud.html#support-overview",
    "title": "Toolbox Cloudification",
    "section": "Support Overview",
    "text": "Support Overview\n\nFeatures\nThe toolbox cloudification activities offer an analysis of the implementation of the toolbox and the requested processing capabilities. On the basis of this analysis, an implementation strategy is proposed and executed to provide the requested functionality outside the context of the toolbox as a fully cloud-integrated APEx-compliant service.\nProjects may benefit from toolbox cloudification by bringing key functions from traditional EO toolboxes into the cloud, where they can be integrated into large-scale processing workflows. If a toolbox is migrated to APEx, it shall be kept up to date. This requires maintenance. The procedure to support the evolution of toolboxes is yet to be defined.\n\n\nHow to get started?\nTo initiate a toolbox cloudification activity, a proposal specifies a candidate toolbox and selects key functionality for cloudification. This proposal is then submitted to the APEx team. It should contain a short justification explaining the need for the selected functionality. ESA reviews the proposal and submits selected proposals to the APEx team for analysis and price offer.\nIn a first phase, the APEx team makes an initial analysis of the candidate toolbox and an assessment of the associated proposal. On this basis, the team provides an assessment report to ESA. The report includes a final recommendation of the key functionality to be cloudified, a description of the specific cloudification approach, and an effort and cost estimation. ESA decides whether to accept the offer.\nOn acceptance, a team of suitably qualified APEx experts undertakes the cloudification process itself, recasting or reimplementing the required functionality in cloud-hosted toolbox modules. In order for the cloudified modules to be hosted through APEx, they will be implemented on one of the APEx-compliant technologies, such as one or more openEO UDPs and/or Application Packages. The more suitable variant depends largely on the original implementation of the toolbox functionality. The newly created modules can be deployed on an APEx-compliant hosting platform and thus become available for direct use and integration into larger cloud-hosted processing workflows.",
    "crumbs": [
      "Algorithm Services",
      "Toolbox Cloudification"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to the APEx Documentation Portal",
    "section": "",
    "text": "Application Propagation Environment (APEx) is a European Space Agency (ESA) initiative that aims to facilitate the uptake of Earth Observation (EO) data and services.\nIt brings significant benefits to various stakeholders involved in the EO ecosystem. By facilitating a structured approach to dynamic instantiation of working environments, algorithm onboarding, enhancement, and sharing, APEx serves to boost the reusability and impact of EO-based research outcomes. The following sections outline the different features and capabilities that APEx provides to different stakeholders.\n\n\n\nCurated list of EO services\nAPEx curates a catalog of earth observation services in its APEx Algorithm Services Catalogue, that generate value-added products on demand. These services can include capabilities such as deforestation, floods or generate higher level information. These services are built and maintained by various EO projects, which guarantees that they have gone through an extensive review and validation process. Additionally, advanced APEx tooling continuously monitors these services to ensure that they remain available, even after the initial project has ended.\nService compatibility with existing cloud platforms\nEach onboarded service in the catalogue complies with a set of Interoperability and Compliance Guidelines. This ensures that these services can be seamlessly reused on existing platforms and integrated into APEx-compliant technologies, such as openEO or OGC Application Packages.\n\n\n\n\nFor EO projects, APEx provides a wide range of services that support project execution and ensure that results remain accessible even after project completion.\n\nDynamic Instantiation of Software and Tools\nAPEx can support your project by offering access to essential IT components. These include project websites, data visualization tools, workspaces, and development environments — all dynamically instantiated for your project to use. Learn more about APEx Instantiation Services and how to customize them to fit your project’s specific requirements.\nSupport for Service Development\nDuring your project, APEx assists in developing efficient on-demand services through its propagation services. These services can support your project with maintaining continuous availability, enhancing, and upscaling EO algorithms. Explore detailed instructions on leveraging interoperability standards like openEO and OGC Application Package to develop and onboard algorithms with best-practice interoperability. Additionally, explore how APEx can provide access guidance on improving algorithm performance, facilitating intercomparison exercises for scientific accuracy, and implementing cloudification strategies for existing toolboxes.\n\n\n\n\n\nGuidelines for ensuring compliance with ESA projects\nAPEx provides clear guidelines that enable platform providers to prepare their systems for hosting and executing EO services developed by ESA projects. By adhering to these guidelines, you can ensure that your platform is aligned with ESA’s standards and ready for seamless integration in upcoming projects.\nIncreased activity and visibility\nBy making your platform compliant with APEx, you can attract more users as it will be promoted as an APEx-compliant platform. This compliance not only enhances your platform’s visibility but also makes your offerings more appealing to other projects.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#for-users",
    "href": "index.html#for-users",
    "title": "Welcome to the APEx Documentation Portal",
    "section": "",
    "text": "Curated list of EO services\nAPEx curates a catalog of earth observation services in its APEx Algorithm Services Catalogue, that generate value-added products on demand. These services can include capabilities such as deforestation, floods or generate higher level information. These services are built and maintained by various EO projects, which guarantees that they have gone through an extensive review and validation process. Additionally, advanced APEx tooling continuously monitors these services to ensure that they remain available, even after the initial project has ended.\nService compatibility with existing cloud platforms\nEach onboarded service in the catalogue complies with a set of Interoperability and Compliance Guidelines. This ensures that these services can be seamlessly reused on existing platforms and integrated into APEx-compliant technologies, such as openEO or OGC Application Packages.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#for-eo-projects",
    "href": "index.html#for-eo-projects",
    "title": "Welcome to the APEx Documentation Portal",
    "section": "",
    "text": "For EO projects, APEx provides a wide range of services that support project execution and ensure that results remain accessible even after project completion.\n\nDynamic Instantiation of Software and Tools\nAPEx can support your project by offering access to essential IT components. These include project websites, data visualization tools, workspaces, and development environments — all dynamically instantiated for your project to use. Learn more about APEx Instantiation Services and how to customize them to fit your project’s specific requirements.\nSupport for Service Development\nDuring your project, APEx assists in developing efficient on-demand services through its propagation services. These services can support your project with maintaining continuous availability, enhancing, and upscaling EO algorithms. Explore detailed instructions on leveraging interoperability standards like openEO and OGC Application Package to develop and onboard algorithms with best-practice interoperability. Additionally, explore how APEx can provide access guidance on improving algorithm performance, facilitating intercomparison exercises for scientific accuracy, and implementing cloudification strategies for existing toolboxes.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#for-eo-platforms-providers",
    "href": "index.html#for-eo-platforms-providers",
    "title": "Welcome to the APEx Documentation Portal",
    "section": "",
    "text": "Guidelines for ensuring compliance with ESA projects\nAPEx provides clear guidelines that enable platform providers to prepare their systems for hosting and executing EO services developed by ESA projects. By adhering to these guidelines, you can ensure that your platform is aligned with ESA’s standards and ready for seamless integration in upcoming projects.\nIncreased activity and visibility\nBy making your platform compliant with APEx, you can attract more users as it will be promoted as an APEx-compliant platform. This compliance not only enhances your platform’s visibility but also makes your offerings more appealing to other projects.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "guides/catalog_openeo.html",
    "href": "guides/catalog_openeo.html",
    "title": "APEx STAC catalogue integration with openEO",
    "section": "",
    "text": "The project catalogue provided by APEx can be used in combination with an openEO-based platform.\nDo note that openEO is an open standard, and not all platforms may support all use cases. Also the produced STAC metadata may have differences that are relevant to your project. Therefore we always recommend to check the envisioned use cases with your provider of choice.",
    "crumbs": [
      "Guides",
      "Linking APEx STAC catalog with an openEO service"
    ]
  },
  {
    "objectID": "guides/catalog_openeo.html#supported-use-cases",
    "href": "guides/catalog_openeo.html#supported-use-cases",
    "title": "APEx STAC catalogue integration with openEO",
    "section": "Supported use cases",
    "text": "Supported use cases\n\nScripted upload of openEO generated results into APEx catalog (supported by any openEO instance).\nDirect publishing of results to object storage (requires support for openEO ‘workspace’ API).\nDirect publishing of results to STAC catalogue (requires support for openEO ‘workspace’ API, and specifically merging into existing catalogue.)",
    "crumbs": [
      "Guides",
      "Linking APEx STAC catalog with an openEO service"
    ]
  },
  {
    "objectID": "guides/catalog_openeo.html#use-case-1-scripted-upload",
    "href": "guides/catalog_openeo.html#use-case-1-scripted-upload",
    "title": "APEx STAC catalogue integration with openEO",
    "section": "Use case 1: Scripted upload",
    "text": "Use case 1: Scripted upload\nThis use case is most broadly supported because it simply takes STAC metadata of openEO job results, downloads it, and then ingests it into a catalogue.\nThe most important drawbacks of this method:\n\nData is first downloaded from openEO backend and then uploaded again, which can take time and bandwidth, and may even be interrupted when facing network instability.\nExtra project specific code is needed to perform the task.\n\n\nimport openeo\nfrom osgeo_utils.gdal2tiles import GlobalMercator\n\ncon = openeo.connect(\"openeo.dataspace.copernicus.eu\").authenticate_oidc()\n\nAuthenticated using refresh token.\n\n\n\n\ntilegrid = GlobalMercator()\nbounds = tilegrid.TileLatLonBounds(233, 214, 9)\nresolution = tilegrid.Resolution(12)\nprint(bounds)\nprint(resolution)\n\n\ncube = con.load_collection(\n    \"SENTINEL2_L2A\",\n    bands=[\"B04\", \"B03\", \"B02\"],\n    temporal_extent=\"2019-08-19\",\n    spatial_extent={\n        \"west\": bounds[1],\n        \"south\": abs(bounds[2]),#abs is weird, does GlobalMercator return wrong latitude values?\n        \"east\": bounds[3],\n        \"north\": abs(bounds[0]),\n    }\n)\ncube.result_node().update_arguments(featureflags={\"tilesize\": 256})#force block size in output tiff\nresult = cube.resample_spatial(resolution=resolution,projection=\"EPSG:3857\")\njob = result.execute_batch(\"gran_canaria.tiff\",title=\"Gran Canaria\",filename_prefix=\"gran_canaria\")\n\n(-28.30438068296276, -16.171874999999993, -27.68352808378777, -15.468750000000012)\n38.21851414258813\nAuthenticated using refresh token.\n0:00:00 Job 'j-241106f371e448088b97aeb8d0c9c19f': send 'start'\n0:00:14 Job 'j-241106f371e448088b97aeb8d0c9c19f': created (progress 0%)\n0:00:21 Job 'j-241106f371e448088b97aeb8d0c9c19f': created (progress 0%)\n0:00:27 Job 'j-241106f371e448088b97aeb8d0c9c19f': running (progress N/A)\n0:00:36 Job 'j-241106f371e448088b97aeb8d0c9c19f': running (progress N/A)\n0:00:46 Job 'j-241106f371e448088b97aeb8d0c9c19f': running (progress N/A)\n0:00:58 Job 'j-241106f371e448088b97aeb8d0c9c19f': running (progress N/A)\n0:01:14 Job 'j-241106f371e448088b97aeb8d0c9c19f': running (progress N/A)\n0:01:33 Job 'j-241106f371e448088b97aeb8d0c9c19f': running (progress N/A)\n0:01:58 Job 'j-241106f371e448088b97aeb8d0c9c19f': running (progress N/A)\n0:02:28 Job 'j-241106f371e448088b97aeb8d0c9c19f': finished (progress 100%)\n\n\n\njob = con.job(\"j-241106f371e448088b97aeb8d0c9c19f\")\njob\n\n\n    \n    \n        \n    \n    \n\n\n\nimport pystac\n\nstac_metadata_dict = job.get_results().get_metadata()\nstac_metadata_dict[\"id\"] = \"gran_canaria_rgb\"\n#remove collection assets, we will rely on item links\ndel stac_metadata_dict[\"assets\"]\ncollection = pystac.Collection.from_dict(stac_metadata_dict)\ncollection\n\n\n\n\n\n    \n        \n            \n                \n                    \n        \n            type\n            \"Collection\"\n        \n    \n                \n            \n                \n                    \n        \n            id\n            \"gran_canaria_rgb\"\n        \n    \n                \n            \n                \n                    \n        \n            stac_version\n            \"1.0.0\"\n        \n    \n                \n            \n                \n                    \n        \n            description\n            \"Results for batch job j-241106f371e448088b97aeb8d0c9c19f\"\n        \n    \n                \n            \n                \n                    \n        links[] 9 items\n        \n            \n        \n            \n                \n        \n            0\n            \n        \n            \n                \n        \n            rel\n            \"root\"\n        \n    \n            \n        \n            \n                \n        \n            href\n            \"https://openeo.dataspace.copernicus.eu/openeo/1.2/jobs/j-241106f371e448088b97aeb8d0c9c19f/results\"\n        \n    \n            \n        \n            \n                \n        \n            type\n            \"application/json\"\n        \n    \n            \n        \n            \n                \n        \n            title\n            \"Gran Canaria\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            1\n            \n        \n            \n                \n        \n            rel\n            \"derived_from\"\n        \n    \n            \n        \n            \n                \n        \n            href\n            \"/eodata/Sentinel-2/MSI/L2A_N0500/2019/08/19/S2A_MSIL2A_20190819T115221_N0500_R123_T28RCR_20230507T090239.SAFE\"\n        \n    \n            \n        \n            \n                \n        \n            type\n            \"application/json\"\n        \n    \n            \n        \n            \n                \n        \n            title\n            \"Derived from /eodata/Sentinel-2/MSI/L2A_N0500/2019/08/19/S2A_MSIL2A_20190819T115221_N0500_R123_T28RCR_20230507T090239.SAFE\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            2\n            \n        \n            \n                \n        \n            rel\n            \"derived_from\"\n        \n    \n            \n        \n            \n                \n        \n            href\n            \"/eodata/Sentinel-2/MSI/L2A_N0500/2019/08/19/S2A_MSIL2A_20190819T115221_N0500_R123_T28RCS_20230507T090239.SAFE\"\n        \n    \n            \n        \n            \n                \n        \n            type\n            \"application/json\"\n        \n    \n            \n        \n            \n                \n        \n            title\n            \"Derived from /eodata/Sentinel-2/MSI/L2A_N0500/2019/08/19/S2A_MSIL2A_20190819T115221_N0500_R123_T28RCS_20230507T090239.SAFE\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            3\n            \n        \n            \n                \n        \n            rel\n            \"derived_from\"\n        \n    \n            \n        \n            \n                \n        \n            href\n            \"/eodata/Sentinel-2/MSI/L2A_N0500/2019/08/19/S2A_MSIL2A_20190819T115221_N0500_R123_T28RDR_20230507T090239.SAFE\"\n        \n    \n            \n        \n            \n                \n        \n            type\n            \"application/json\"\n        \n    \n            \n        \n            \n                \n        \n            title\n            \"Derived from /eodata/Sentinel-2/MSI/L2A_N0500/2019/08/19/S2A_MSIL2A_20190819T115221_N0500_R123_T28RDR_20230507T090239.SAFE\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            4\n            \n        \n            \n                \n        \n            rel\n            \"derived_from\"\n        \n    \n            \n        \n            \n                \n        \n            href\n            \"/eodata/Sentinel-2/MSI/L2A_N0500/2019/08/19/S2A_MSIL2A_20190819T115221_N0500_R123_T28RDS_20230507T090239.SAFE\"\n        \n    \n            \n        \n            \n                \n        \n            type\n            \"application/json\"\n        \n    \n            \n        \n            \n                \n        \n            title\n            \"Derived from /eodata/Sentinel-2/MSI/L2A_N0500/2019/08/19/S2A_MSIL2A_20190819T115221_N0500_R123_T28RDS_20230507T090239.SAFE\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            5\n            \n        \n            \n                \n        \n            rel\n            \"self\"\n        \n    \n            \n        \n            \n                \n        \n            href\n            \"https://openeo.dataspace.copernicus.eu/openeo/1.2/jobs/j-241106f371e448088b97aeb8d0c9c19f/results\"\n        \n    \n            \n        \n            \n                \n        \n            type\n            \"application/json\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            6\n            \n        \n            \n                \n        \n            rel\n            \"canonical\"\n        \n    \n            \n        \n            \n                \n        \n            href\n            \"https://openeo.dataspace.copernicus.eu/openeo/1.2/jobs/j-241106f371e448088b97aeb8d0c9c19f/results/MzJjYzdkZGItZjdlMS00YjFjLTk3OTYtZjlmZTM5Y2I4ZmVi/6ff5dbb18b4c82a3e150efc966d678c3?expires=1731512886\"\n        \n    \n            \n        \n            \n                \n        \n            type\n            \"application/json\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            7\n            \n        \n            \n                \n        \n            rel\n            \"card4l-document\"\n        \n    \n            \n        \n            \n                \n        \n            href\n            \"http://ceos.org/ard/files/PFS/SR/v5.0/CARD4L_Product_Family_Specification_Surface_Reflectance-v5.0.pdf\"\n        \n    \n            \n        \n            \n                \n        \n            type\n            \"application/pdf\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            8\n            \n        \n            \n                \n        \n            rel\n            \"item\"\n        \n    \n            \n        \n            \n                \n        \n            href\n            \"https://openeo.dataspace.copernicus.eu/openeo/1.2/jobs/j-241106f371e448088b97aeb8d0c9c19f/results/items/MzJjYzdkZGItZjdlMS00YjFjLTk3OTYtZjlmZTM5Y2I4ZmVi/7aebb091eb09284f2e14bfbadade4036/gran_canaria_2019-08-19Z.tif?expires=1731512886\"\n        \n    \n            \n        \n            \n                \n        \n            type\n            \"application/geo+json\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n                \n            \n                \n                    \n        stac_extensions[] 4 items\n        \n            \n        \n            \n                \n        \n            0\n            \"https://stac-extensions.github.io/eo/v1.1.0/schema.json\"\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            1\n            \"https://stac-extensions.github.io/file/v2.1.0/schema.json\"\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            2\n            \"https://stac-extensions.github.io/processing/v1.1.0/schema.json\"\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            3\n            \"https://stac-extensions.github.io/projection/v1.1.0/schema.json\"\n        \n    \n            \n        \n    \n        \n    \n                \n            \n                \n                    \n        \n            openeo:status\n            \"finished\"\n        \n    \n                \n            \n                \n                    \n        \n            title\n            \"Gran Canaria\"\n        \n    \n                \n            \n                \n                    \n        \n            extent\n            \n        \n            \n                \n        \n            spatial\n            \n        \n            \n                \n        bbox[] 1 items\n        \n            \n        \n            \n                \n        0[] 4 items\n        \n            \n        \n            \n                \n        \n            0\n            -16.171874999999993\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            1\n            27.68352808378777\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            2\n            -15.468750000000012\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            3\n            28.30438068296276\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n            \n        \n            \n                \n        \n            temporal\n            \n        \n            \n                \n        interval[] 1 items\n        \n            \n        \n            \n                \n        0[] 2 items\n        \n            \n        \n            \n                \n        \n            0\n            \"2019-08-19T00:00:00Z\"\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            1\n            \"2019-08-20T00:00:00Z\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n                \n            \n                \n                    \n        \n            license\n            \"proprietary\"\n        \n    \n                \n            \n                \n                    \n        providers[] 1 items\n        \n            \n        \n            \n                \n        \n            0\n            \n        \n            \n                \n        \n            name\n            \"VITO\"\n        \n    \n            \n        \n            \n                \n        \n            description\n            \"This data was processed on an openEO backend maintained by VITO.\"\n        \n    \n            \n        \n            \n                \n        roles[] 1 items\n        \n            \n        \n            \n                \n        \n            0\n            \"processor\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n            \n                \n        \n            processing:expression\n            \n        \n            \n                \n        \n            expression\n            \n        \n            \n                \n        \n            loadcollection1\n            \n        \n            \n                \n        \n            arguments\n            \n        \n            \n                \n        bands[] 3 items\n        \n            \n        \n            \n                \n        \n            0\n            \"B04\"\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            1\n            \"B03\"\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            2\n            \"B02\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n            \n                \n        \n            featureflags\n            \n        \n            \n                \n        \n            tilesize\n            256\n        \n    \n            \n        \n    \n        \n    \n            \n        \n            \n                \n        \n            id\n            \"SENTINEL2_L2A\"\n        \n    \n            \n        \n            \n                \n        \n            spatial_extent\n            \n        \n            \n                \n        \n            east\n            -15.468750000000012\n        \n    \n            \n        \n            \n                \n        \n            north\n            28.30438068296276\n        \n    \n            \n        \n            \n                \n        \n            south\n            27.68352808378777\n        \n    \n            \n        \n            \n                \n        \n            west\n            -16.171874999999993\n        \n    \n            \n        \n    \n        \n    \n            \n        \n            \n                \n        temporal_extent[] 2 items\n        \n            \n        \n            \n                \n        \n            0\n            \"2019-08-19\"\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            1\n            \"2019-08-20\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n            \n        \n            \n                \n        \n            process_id\n            \"load_collection\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n            \n                \n        \n            resamplespatial1\n            \n        \n            \n                \n        \n            arguments\n            \n        \n            \n                \n        \n            align\n            \"upper-left\"\n        \n    \n            \n        \n            \n                \n        \n            data\n            \n        \n            \n                \n        \n            from_node\n            \"loadcollection1\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n            \n                \n        \n            method\n            \"near\"\n        \n    \n            \n        \n            \n                \n        \n            projection\n            \"EPSG:3857\"\n        \n    \n            \n        \n            \n                \n        \n            resolution\n            38.21851414258813\n        \n    \n            \n        \n    \n        \n    \n            \n        \n            \n                \n        \n            process_id\n            \"resample_spatial\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n            \n                \n        \n            saveresult1\n            \n        \n            \n                \n        \n            arguments\n            \n        \n            \n                \n        \n            data\n            \n        \n            \n                \n        \n            from_node\n            \"resamplespatial1\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n            \n                \n        \n            format\n            \"GTiff\"\n        \n    \n            \n        \n            \n                \n        \n            options\n            \n        \n            \n                \n        \n            filename_prefix\n            \"gran_canaria\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n            \n        \n            \n                \n        \n            process_id\n            \"save_result\"\n        \n    \n            \n        \n            \n                \n        \n            result\n            True\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n            \n        \n            \n                \n        \n            format\n            \"openeo\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n            \n                \n        \n            processing:facility\n            \"openEO Geotrellis backend\"\n        \n    \n            \n        \n            \n                \n        \n            processing:software\n            \n        \n            \n                \n        \n            Geotrellis backend\n            \"0.48.2a1\"",
    "crumbs": [
      "Guides",
      "Linking APEx STAC catalog with an openEO service"
    ]
  },
  {
    "objectID": "guides/catalog_openeo.html#convert-online-openeo-collection-to-local-collection",
    "href": "guides/catalog_openeo.html#convert-online-openeo-collection-to-local-collection",
    "title": "APEx STAC catalogue integration with openEO",
    "section": "Convert online openEO collection to local collection",
    "text": "Convert online openEO collection to local collection\nThis step cleans up links to avoid that they point to the openEO API, which in some cases requires authentication.\nThere’s an open issue to integrate this in the API.\n\n#remove collection and canoncial links\ncollection.remove_links(rel=\"collection\")\ncollection.remove_links(rel=\"canonical\")\n\nitems = list(collection.get_stac_objects(rel=pystac.RelType.ITEM))\nfor i in items:\n    i.remove_links(rel=\"collection\")\n    i.remove_links(rel=\"canonical\")\n    i.\n\n\n\ncollection.set_self_href(\"/tmp/collection.json\")\ncollection.normalize_hrefs('/tmp/', skip_unresolved=True)\n\ncollection.license = \"CC-BY-4.0\"\n\ndef asset_transform(name,a):\n    a.href = \"/tmp/\" + name\n    return a\n\n#this step can transform asset hrefs as well, \n#c2=catalog.map_assets(asset_transform)\n\n\n\ncollection.save(catalog_type=pystac.CatalogType.SELF_CONTAINED)\ncollection\n\n\n\n\n\n    \n        \n            \n                \n                    \n        \n            type\n            \"Collection\"\n        \n    \n                \n            \n                \n                    \n        \n            id\n            \"gran_canaria_rgb\"\n        \n    \n                \n            \n                \n                    \n        \n            stac_version\n            \"1.0.0\"\n        \n    \n                \n            \n                \n                    \n        \n            description\n            \"Results for batch job j-241106f371e448088b97aeb8d0c9c19f\"\n        \n    \n                \n            \n                \n                    \n        links[] 8 items\n        \n            \n        \n            \n                \n        \n            0\n            \n        \n            \n                \n        \n            rel\n            \"root\"\n        \n    \n            \n        \n            \n                \n        \n            href\n            \"/tmp/collection.json\"\n        \n    \n            \n        \n            \n                \n        \n            type\n            \"application/json\"\n        \n    \n            \n        \n            \n                \n        \n            title\n            \"Gran Canaria\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            1\n            \n        \n            \n                \n        \n            rel\n            \"derived_from\"\n        \n    \n            \n        \n            \n                \n        \n            href\n            \"/eodata/Sentinel-2/MSI/L2A_N0500/2019/08/19/S2A_MSIL2A_20190819T115221_N0500_R123_T28RCR_20230507T090239.SAFE\"\n        \n    \n            \n        \n            \n                \n        \n            type\n            \"application/json\"\n        \n    \n            \n        \n            \n                \n        \n            title\n            \"Derived from /eodata/Sentinel-2/MSI/L2A_N0500/2019/08/19/S2A_MSIL2A_20190819T115221_N0500_R123_T28RCR_20230507T090239.SAFE\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            2\n            \n        \n            \n                \n        \n            rel\n            \"derived_from\"\n        \n    \n            \n        \n            \n                \n        \n            href\n            \"/eodata/Sentinel-2/MSI/L2A_N0500/2019/08/19/S2A_MSIL2A_20190819T115221_N0500_R123_T28RCS_20230507T090239.SAFE\"\n        \n    \n            \n        \n            \n                \n        \n            type\n            \"application/json\"\n        \n    \n            \n        \n            \n                \n        \n            title\n            \"Derived from /eodata/Sentinel-2/MSI/L2A_N0500/2019/08/19/S2A_MSIL2A_20190819T115221_N0500_R123_T28RCS_20230507T090239.SAFE\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            3\n            \n        \n            \n                \n        \n            rel\n            \"derived_from\"\n        \n    \n            \n        \n            \n                \n        \n            href\n            \"/eodata/Sentinel-2/MSI/L2A_N0500/2019/08/19/S2A_MSIL2A_20190819T115221_N0500_R123_T28RDR_20230507T090239.SAFE\"\n        \n    \n            \n        \n            \n                \n        \n            type\n            \"application/json\"\n        \n    \n            \n        \n            \n                \n        \n            title\n            \"Derived from /eodata/Sentinel-2/MSI/L2A_N0500/2019/08/19/S2A_MSIL2A_20190819T115221_N0500_R123_T28RDR_20230507T090239.SAFE\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            4\n            \n        \n            \n                \n        \n            rel\n            \"derived_from\"\n        \n    \n            \n        \n            \n                \n        \n            href\n            \"/eodata/Sentinel-2/MSI/L2A_N0500/2019/08/19/S2A_MSIL2A_20190819T115221_N0500_R123_T28RDS_20230507T090239.SAFE\"\n        \n    \n            \n        \n            \n                \n        \n            type\n            \"application/json\"\n        \n    \n            \n        \n            \n                \n        \n            title\n            \"Derived from /eodata/Sentinel-2/MSI/L2A_N0500/2019/08/19/S2A_MSIL2A_20190819T115221_N0500_R123_T28RDS_20230507T090239.SAFE\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            5\n            \n        \n            \n                \n        \n            rel\n            \"card4l-document\"\n        \n    \n            \n        \n            \n                \n        \n            href\n            \"http://ceos.org/ard/files/PFS/SR/v5.0/CARD4L_Product_Family_Specification_Surface_Reflectance-v5.0.pdf\"\n        \n    \n            \n        \n            \n                \n        \n            type\n            \"application/pdf\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            6\n            \n        \n            \n                \n        \n            rel\n            \"item\"\n        \n    \n            \n        \n            \n                \n        \n            href\n            \"/tmp/gran_canaria_2019-08-19Z.tif/gran_canaria_2019-08-19Z.tif.json\"\n        \n    \n            \n        \n            \n                \n        \n            type\n            \"application/geo+json\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            7\n            \n        \n            \n                \n        \n            rel\n            \"self\"\n        \n    \n            \n        \n            \n                \n        \n            href\n            \"/tmp/collection.json\"\n        \n    \n            \n        \n            \n                \n        \n            type\n            \"application/json\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n                \n            \n                \n                    \n        stac_extensions[] 4 items\n        \n            \n        \n            \n                \n        \n            0\n            \"https://stac-extensions.github.io/eo/v1.1.0/schema.json\"\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            1\n            \"https://stac-extensions.github.io/file/v2.1.0/schema.json\"\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            2\n            \"https://stac-extensions.github.io/processing/v1.1.0/schema.json\"\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            3\n            \"https://stac-extensions.github.io/projection/v1.1.0/schema.json\"\n        \n    \n            \n        \n    \n        \n    \n                \n            \n                \n                    \n        \n            openeo:status\n            \"finished\"\n        \n    \n                \n            \n                \n                    \n        \n            title\n            \"Gran Canaria\"\n        \n    \n                \n            \n                \n                    \n        \n            extent\n            \n        \n            \n                \n        \n            spatial\n            \n        \n            \n                \n        bbox[] 1 items\n        \n            \n        \n            \n                \n        0[] 4 items\n        \n            \n        \n            \n                \n        \n            0\n            -16.171874999999993\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            1\n            27.68352808378777\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            2\n            -15.468750000000012\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            3\n            28.30438068296276\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n            \n        \n            \n                \n        \n            temporal\n            \n        \n            \n                \n        interval[] 1 items\n        \n            \n        \n            \n                \n        0[] 2 items\n        \n            \n        \n            \n                \n        \n            0\n            \"2019-08-19T00:00:00Z\"\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            1\n            \"2019-08-20T00:00:00Z\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n                \n            \n                \n                    \n        \n            license\n            \"CC-BY-4.0\"\n        \n    \n                \n            \n                \n                    \n        providers[] 1 items\n        \n            \n        \n            \n                \n        \n            0\n            \n        \n            \n                \n        \n            name\n            \"VITO\"\n        \n    \n            \n        \n            \n                \n        \n            description\n            \"This data was processed on an openEO backend maintained by VITO.\"\n        \n    \n            \n        \n            \n                \n        roles[] 1 items\n        \n            \n        \n            \n                \n        \n            0\n            \"processor\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n            \n                \n        \n            processing:expression\n            \n        \n            \n                \n        \n            expression\n            \n        \n            \n                \n        \n            loadcollection1\n            \n        \n            \n                \n        \n            arguments\n            \n        \n            \n                \n        bands[] 3 items\n        \n            \n        \n            \n                \n        \n            0\n            \"B04\"\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            1\n            \"B03\"\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            2\n            \"B02\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n            \n                \n        \n            featureflags\n            \n        \n            \n                \n        \n            tilesize\n            256\n        \n    \n            \n        \n    \n        \n    \n            \n        \n            \n                \n        \n            id\n            \"SENTINEL2_L2A\"\n        \n    \n            \n        \n            \n                \n        \n            spatial_extent\n            \n        \n            \n                \n        \n            east\n            -15.468750000000012\n        \n    \n            \n        \n            \n                \n        \n            north\n            28.30438068296276\n        \n    \n            \n        \n            \n                \n        \n            south\n            27.68352808378777\n        \n    \n            \n        \n            \n                \n        \n            west\n            -16.171874999999993\n        \n    \n            \n        \n    \n        \n    \n            \n        \n            \n                \n        temporal_extent[] 2 items\n        \n            \n        \n            \n                \n        \n            0\n            \"2019-08-19\"\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            1\n            \"2019-08-20\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n            \n        \n            \n                \n        \n            process_id\n            \"load_collection\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n            \n                \n        \n            resamplespatial1\n            \n        \n            \n                \n        \n            arguments\n            \n        \n            \n                \n        \n            align\n            \"upper-left\"\n        \n    \n            \n        \n            \n                \n        \n            data\n            \n        \n            \n                \n        \n            from_node\n            \"loadcollection1\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n            \n                \n        \n            method\n            \"near\"\n        \n    \n            \n        \n            \n                \n        \n            projection\n            \"EPSG:3857\"\n        \n    \n            \n        \n            \n                \n        \n            resolution\n            38.21851414258813\n        \n    \n            \n        \n    \n        \n    \n            \n        \n            \n                \n        \n            process_id\n            \"resample_spatial\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n            \n                \n        \n            saveresult1\n            \n        \n            \n                \n        \n            arguments\n            \n        \n            \n                \n        \n            data\n            \n        \n            \n                \n        \n            from_node\n            \"resamplespatial1\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n            \n                \n        \n            format\n            \"GTiff\"\n        \n    \n            \n        \n            \n                \n        \n            options\n            \n        \n            \n                \n        \n            filename_prefix\n            \"gran_canaria\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n            \n        \n            \n                \n        \n            process_id\n            \"save_result\"\n        \n    \n            \n        \n            \n                \n        \n            result\n            True\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n            \n        \n            \n                \n        \n            format\n            \"openeo\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n            \n                \n        \n            processing:facility\n            \"openEO Geotrellis backend\"\n        \n    \n            \n        \n            \n                \n        \n            processing:software\n            \n        \n            \n                \n        \n            Geotrellis backend\n            \"0.48.2a1\"",
    "crumbs": [
      "Guides",
      "Linking APEx STAC catalog with an openEO service"
    ]
  },
  {
    "objectID": "guides/catalog_openeo.html#upload-to-stac-api",
    "href": "guides/catalog_openeo.html#upload-to-stac-api",
    "title": "APEx STAC catalogue integration with openEO",
    "section": "Upload to STAC API",
    "text": "Upload to STAC API\nThe collection metadata has now been cleaned up and can be added to the STAC API. Here we create the full collection, but it’s also possible to only add the generated item to a new collection.\nAt this point, it is also recommended to improve the metadata quality by adding additional metadata properties.\n\nImportant note\nAt this point, the actual Geotiff is still located on the openEO backend, and can be accessed by any tool via the signed url. This url will however expire, so for more permanent catalogues, the data file needs to be moved to a better location. To do this, simply download the tiff file and upload it to an online location of your preference.\n\nimport requests\nfrom owslib.ogcapi.records import Records\nfrom owslib.util import Authentication\n\nclass BearerAuth(requests.auth.AuthBase):\n    def __init__(self, token):\n        self.token = token\n    def __call__(self, r):\n        r.headers[\"authorization\"] = \"Bearer \" + self.token\n        return r\n    \nauth = BearerAuth(\"\"\"PROVIDE_TOKEN_HERE\"\"\")\n\nr = Records(\"https://catalogue.project-a.apex.esa.int\",auth=Authentication(auth_delegate=auth))\n\n\ncoll_dict = collection.to_dict()\n\ndefault_auth = {\n    \"_auth\": {\n        \"read\": [\"anonymous\"],\n        \"write\": [\"stac-openeo-admin\", \"stac-openeo-editor\"]\n    }\n}\n\ncoll_dict.update(default_auth)\n\nresponse = requests.post(\"https://catalogue.project-a.apex.esa.int/collections\", auth=auth,json=coll_dict)\nresponse\n\n&lt;Response [201]&gt;\n\n\n\nfor item in collection.get_all_items():\n    item_dict = item.to_dict()\n    print(item)\n    item_dict[\"collection\"] = collection.id\n    r.collection_item_create(collection.id, item_dict)\n\n{'type': 'Feature', 'stac_version': '1.0.0', 'id': 'gran_canaria_2019-08-19Z.tif', 'properties': {'datetime': '2019-08-19T00:00:00Z', 'proj:bbox': [-1800244.89, 3209132.196, -1721973.373, 3287403.712], 'proj:epsg': 3857, 'proj:shape': [2048, 2048]}, 'geometry': {'coordinates': [[[-16.17187499999999, 27.68352808378774], [-16.17187499999999, 28.304380682962755], [-15.468749999999991, 28.304380682962755], [-15.468749999999991, 27.68352808378774], [-16.17187499999999, 27.68352808378774]]], 'type': 'Polygon'}, 'links': [{'rel': 'root', 'href': '../collection.json', 'type': 'application/json', 'title': 'Gran Canaria'}, {'rel': 'self', 'href': '/tmp/gran_canaria_2019-08-19Z.tif/gran_canaria_2019-08-19Z.tif.json', 'type': 'application/json'}, {'rel': 'parent', 'href': '../collection.json', 'type': 'application/json', 'title': 'Gran Canaria'}], 'assets': {'gran_canaria_2019-08-19Z.tif': {'href': 'https://openeo.dataspace.copernicus.eu/openeo/1.2/jobs/j-241106f371e448088b97aeb8d0c9c19f/results/assets/MzJjYzdkZGItZjdlMS00YjFjLTk3OTYtZjlmZTM5Y2I4ZmVi/3f3276dfba5bb7235326dea697f556c1/gran_canaria_2019-08-19Z.tif?expires=1731512915', 'type': 'image/tiff; application=geotiff', 'title': 'gran_canaria_2019-08-19Z.tif', 'eo:bands': [{'center_wavelength': 0.6646, 'name': 'B04'}, {'center_wavelength': 0.5598, 'name': 'B03'}, {'center_wavelength': 0.4924, 'name': 'B02'}], 'proj:bbox': [-1800244.89, 3209132.196, -1721973.373, 3287403.712], 'proj:epsg': 3857, 'proj:shape': [2048, 2048], 'raster:bands': [{'name': 'B04', 'statistics': {'maximum': 18494.0, 'mean': 765.91613554958, 'minimum': -301.0, 'stddev': 534.21168059182, 'valid_percent': 100.0}}, {'name': 'B03', 'statistics': {'maximum': 20682.0, 'mean': 699.08895897867, 'minimum': -560.0, 'stddev': 449.31294931756, 'valid_percent': 100.0}}, {'name': 'B02', 'statistics': {'maximum': 21562.0, 'mean': 678.19460010522, 'minimum': -980.0, 'stddev': 412.96003458047, 'valid_percent': 100.0}}], 'roles': ['data']}}, 'bbox': [-16.17187499999999, 27.68352808378774, -15.468749999999991, 28.304380682962755], 'stac_extensions': ['https://stac-extensions.github.io/eo/v1.1.0/schema.json', 'https://stac-extensions.github.io/file/v2.1.0/schema.json', 'https://stac-extensions.github.io/projection/v1.1.0/schema.json'], 'collection': 'j-241106f371e448088b97aeb8d0c9c19f', 'epsg': 3857}\n\n\n\n\nrequests.delete(\"https://catalogue.project-a.apex.esa.int/collections/\" + collection.id, auth=auth)\n\n&lt;Response [204]&gt;",
    "crumbs": [
      "Guides",
      "Linking APEx STAC catalog with an openEO service"
    ]
  },
  {
    "objectID": "guides/eoap_writer_guide.html",
    "href": "guides/eoap_writer_guide.html",
    "title": "Guide for writing EOAP processes",
    "section": "",
    "text": "The Earth Observation Application Package (EOAP) is one out of two standardised options that APEx offers to expose algorithms as a service.\nEOAP gives some concrete steps and guidelines to ensure that your EO data processing software is embedded in a way that can benefit from APEx-compliant algorithm hosting platforms. A number of these Platforms are implementing EOAP guidelines, which include the provision of:\n\nAn interpreter for the Common Workflow Language (CWL)\nThe “OGC API - Processes” interface\nThe EO data discovery & staging protocols for the online access to EO data repositories.\n\nThese capabilities are available on the hosting platform(s), and will altogether make your EOAP-based service work well for your users.\nNote that all the guidance material provided hereafter has APEx in mind, but can also serve as a generic guide for Earth Observation applications. All APEx recommendations are made to support the FAIR principles, ensuring that your service can be easily shared within a broader community. These guidelines also facilitate the integration of your service into the APEx Algorithm Services Catalogue.\nFor best practice illustrations and examples, on how to package Earth Observation applications, the EOAP Guide is published here:\nEarth Observation Application Package (EOAP) resources and guides\nFor background information, the OGC Best Practice document is published on the OGC website:\nOGC Best Practice for Earth Observation Application Package (EOAP)\nMoreover, consider that APEx offers Algorithm Porting and Algorithm Onboarding support, to help you with packaging your algorithm into an EOAP and onboarding it onto an APEx-compliant hosting platform.\n\n\nThe “OGC APIs” is a broad set of standards designed for providing and using geospatial data on the web, and for integrating this data with other type of information. Within these standards, EOAP uses “OGC API - Processes” to expose packaged algorithms as services on a cloud platform.\nFor more background information, the “OGC API - Processes - Part 1: Core Standard” is published on the OGC API website:\nOGC API - Processes.\n\n\n\nThe EOAP Guide contains several examples to get you started with EOAP development and the creation of OGC API Processes.\n\n\nOne tutorial focuses on creating OGC API Processes using the ZOO-Project. This is particularly relevant in the APEx context, where EOAPs are made accessible through OGC API Processes.\nThis example is designed for developers, scientists, and Earth observation enthusiasts who aim to:\n\nFamiliarize themselves with the ZOO-Project implementation of OGC API - Processes.\nIndependently deploy and run their Application Package as a web service.\n\nAdditionally, APEx can provide expert support to assist with these steps.\nResources:\n\nOGC API - Processes with ZOO Project | Documentation.\nOGC API - Processes with ZOO Project | Repository.",
    "crumbs": [
      "Guides",
      "Creating EOAP based services"
    ]
  },
  {
    "objectID": "guides/eoap_writer_guide.html#sec-eoap-writing",
    "href": "guides/eoap_writer_guide.html#sec-eoap-writing",
    "title": "Guide for writing EOAP processes",
    "section": "",
    "text": "The Earth Observation Application Package (EOAP) is one out of two standardised options that APEx offers to expose algorithms as a service.\nEOAP gives some concrete steps and guidelines to ensure that your EO data processing software is embedded in a way that can benefit from APEx-compliant algorithm hosting platforms. A number of these Platforms are implementing EOAP guidelines, which include the provision of:\n\nAn interpreter for the Common Workflow Language (CWL)\nThe “OGC API - Processes” interface\nThe EO data discovery & staging protocols for the online access to EO data repositories.\n\nThese capabilities are available on the hosting platform(s), and will altogether make your EOAP-based service work well for your users.\nNote that all the guidance material provided hereafter has APEx in mind, but can also serve as a generic guide for Earth Observation applications. All APEx recommendations are made to support the FAIR principles, ensuring that your service can be easily shared within a broader community. These guidelines also facilitate the integration of your service into the APEx Algorithm Services Catalogue.\nFor best practice illustrations and examples, on how to package Earth Observation applications, the EOAP Guide is published here:\nEarth Observation Application Package (EOAP) resources and guides\nFor background information, the OGC Best Practice document is published on the OGC website:\nOGC Best Practice for Earth Observation Application Package (EOAP)\nMoreover, consider that APEx offers Algorithm Porting and Algorithm Onboarding support, to help you with packaging your algorithm into an EOAP and onboarding it onto an APEx-compliant hosting platform.\n\n\nThe “OGC APIs” is a broad set of standards designed for providing and using geospatial data on the web, and for integrating this data with other type of information. Within these standards, EOAP uses “OGC API - Processes” to expose packaged algorithms as services on a cloud platform.\nFor more background information, the “OGC API - Processes - Part 1: Core Standard” is published on the OGC API website:\nOGC API - Processes.\n\n\n\nThe EOAP Guide contains several examples to get you started with EOAP development and the creation of OGC API Processes.\n\n\nOne tutorial focuses on creating OGC API Processes using the ZOO-Project. This is particularly relevant in the APEx context, where EOAPs are made accessible through OGC API Processes.\nThis example is designed for developers, scientists, and Earth observation enthusiasts who aim to:\n\nFamiliarize themselves with the ZOO-Project implementation of OGC API - Processes.\nIndependently deploy and run their Application Package as a web service.\n\nAdditionally, APEx can provide expert support to assist with these steps.\nResources:\n\nOGC API - Processes with ZOO Project | Documentation.\nOGC API - Processes with ZOO Project | Repository.",
    "crumbs": [
      "Guides",
      "Creating EOAP based services"
    ]
  },
  {
    "objectID": "guides/udp_description_template.html",
    "href": "guides/udp_description_template.html",
    "title": "Description",
    "section": "",
    "text": "Description\nA concise description of the algorithm, its purpose, and its characteristics that is understandable to a broad audience. The inner workings of the algorithm do not need to be explained, but the expected input and output should be clear.\n\n\nPerformance characteristics\nProvide a general description of the relative cost of the algorithm, and expected running time. Explain if high costs are to be expected, or if this can be safely run over large areas. Compare to similar algorithms if possible. The discussion here can remain qualitative, quantitative figures can be given in the example.\n\n\nExamples\nShow images or plots of the algorithm in action, and provide links to STAC metadata files with actual output assets. Include resource usage metrics provided by the platform.\n\n\nLiterature references\nLink to more technical documentation or publications on this algorithm or validation studies.\n\n\nKnown limitations\nList: - Limitations in quality, that are inherent to the algorithm. - Limitations to when and where the algorithm can be applied. - Upper and lower bounds for parameter values, for instance the maximum area that can be processed in one job.\n\n\nKnown artifacts\nUsing text and figures, illustrate known artifacts. This helps to manage expectations for your users, and allows them to implement additional processing steps to mitigate or avoid these artifacts."
  },
  {
    "objectID": "interoperability/index.html",
    "href": "interoperability/index.html",
    "title": "Interoperability and Compliance Guidelines",
    "section": "",
    "text": "Welcome to the comprehensive guide on APEx interoperability and compliance guidelines. This guide has been compiled to offer a detailed understanding of how algorithms and hosting environments can be integrated into APEx. These guidelines provide useful insights into the integration process. They are designed to make it easier for users to navigate through the complexities of making their unique algorithms and environments APEx compliant.\nFor certain APEx services, we have developed specific guidelines. These are tailored to enable you to seamlessly use the services within your unique environment, thereby ensuring that you achieve the maximum benefit from APEx.\nYou can find more detailed guidelines on the following pages:\n\nAlgorithm Provider Guidelines Discover how to make your algorithm APEx compliant and enable its use within APEx services and the larger EO community.\nAlgorithm Hosting Environments Explore the guidelines for integrating your algorithm hosting environment into APEx services.\nGeospatial Explorer Learn how to integrate data sources and configure your dashboards.",
    "crumbs": [
      "Interoperability and Compliance Guidelines"
    ]
  },
  {
    "objectID": "interoperability/algohostingenv.html",
    "href": "interoperability/algohostingenv.html",
    "title": "Algorithm Hosting Platforms Guidelines",
    "section": "",
    "text": "APEx-compliant application algorithms are executed as services on a compatible algorithm hosting platform. Currently, APEx supports platforms based on either openEO [5] or OGC Application Package [2] technologies. To be considered an APEx-compliant algorithm hosting platform, the platform must meet the requirements outlined in this document section.\nThe aim of these requirements is:\n\nTo ensure that services developed in EO projects continue to work after the project has ended.\nTo align algorithm hosting platforms that aim to offer EO project services to make them more comparable. The alignment targets the API level and the (high-level) pricing model, giving platform providers full freedom to select technologies and architectures that suit their needs.\nTo allow APEx to perform automated checks on the developed services, guaranteeing that they work and produce the expected result at the expected cost.\n\nTable 1 provides an overview of the requirements for operators of algorithm hosting platforms to ensure their compatibility with the APEx standards.\n\n\n\nTable 1: Interoperability requirements for algorithm hosting environments\n\n\n\n\n\n\n\n\n\n\nID\nRequirement\nDescription\n\n\n\n\nHOST-REQ-01\nThe algorithm hosting platform shall support the generation of cloud-native datasets, such as Cloud Optimized GeoTIFF [1], CF-Compliant netCDF [2].\nSupport of GeoZarr [3] will become a requirement as soon as a clear standard is available, check with APEx for guidance on current variants.\n\n\nHOST-REQ-02\nThe algorithm hosting platform shall support the generation of metadata in a STAC [4] format for both raster and vector data, including applicable STAC extensions.\nThis requirement ensures that the outputs are compatible with modern geospatial data standards, enhancing interoperability with other platforms and services.\n\n\nHOST-REQ-03\nThe algorithm hosting platform shall support at least one of these API’s:\n\nOpenEO API version 1.2 [5]\nAt least API profile L1B-minimal-batch-jobs [6]\nAt least processes profiles L1-minimal [7]\nOGC API Processes [8], supporting the deployment of applications defined by the OGC Best Practice for Earth Observation Application Package [9].\n\nThe support for the proposed technologies ensures that APEx-compliant algorithms can be hosted on the algorithm hosting platform and made available as on-demand services.\n\n\nHOST-REQ-04\nThe operator of the algorithm hosting platform shall expose application workflows as an on-demand services that can be called via an interoperable API by a 3rd party that wants to integrate the results into its own workflow.\nBy offering on-demand services via an interoperable API, the platform enables seamless automation and integration of the services across different applications and workflows.\n\n\nHOST-REQ-05\nThe algorithm hosting platform shall be registered in the ESA Network of Resources.\nTo curate services beyond the project lifetime, the APEx consortium is likely to require an active subscription on the platform. For paid subscriptions, this can only be requested via NoR sponsoring. This may not be necessary if the platform is willing to grant non-paid subscriptions.\n\n\nHOST-REQ-06\nThe algorithm hosting platform shall provide an SLA that guarantees support beyond the project lifetime.\nThis ensures the long-term sustainability and reliability of the algorithm hosting platform, providing assurance to users that they can rely on continued support even after the project’s completion.\n\n\nHOST-REQ-07\nThe operator of the algorithm hosting platform shall announce major changes to the SLA, including decommissioning of the platform, to APEx and the NoR, with a lead time of 1 year.\nSuch communication is important to ensure that stakeholders, including APEx and the NoR, are given adequate notice of major changes that could impact the availability or functionality of the algorithm hosting platform. This approach allows for proper planning, adjustment, and mitigation of potential disruptions, ensuring continuity of services for users.",
    "crumbs": [
      "Interoperability and Compliance Guidelines",
      "Algorithm Hosting Platforms Guidelines"
    ]
  },
  {
    "objectID": "interoperability/algohostingenv.html#requirements",
    "href": "interoperability/algohostingenv.html#requirements",
    "title": "Algorithm Hosting Platforms Guidelines",
    "section": "",
    "text": "APEx-compliant application algorithms are executed as services on a compatible algorithm hosting platform. Currently, APEx supports platforms based on either openEO [5] or OGC Application Package [2] technologies. To be considered an APEx-compliant algorithm hosting platform, the platform must meet the requirements outlined in this document section.\nThe aim of these requirements is:\n\nTo ensure that services developed in EO projects continue to work after the project has ended.\nTo align algorithm hosting platforms that aim to offer EO project services to make them more comparable. The alignment targets the API level and the (high-level) pricing model, giving platform providers full freedom to select technologies and architectures that suit their needs.\nTo allow APEx to perform automated checks on the developed services, guaranteeing that they work and produce the expected result at the expected cost.\n\nTable 1 provides an overview of the requirements for operators of algorithm hosting platforms to ensure their compatibility with the APEx standards.\n\n\n\nTable 1: Interoperability requirements for algorithm hosting environments\n\n\n\n\n\n\n\n\n\n\nID\nRequirement\nDescription\n\n\n\n\nHOST-REQ-01\nThe algorithm hosting platform shall support the generation of cloud-native datasets, such as Cloud Optimized GeoTIFF [1], CF-Compliant netCDF [2].\nSupport of GeoZarr [3] will become a requirement as soon as a clear standard is available, check with APEx for guidance on current variants.\n\n\nHOST-REQ-02\nThe algorithm hosting platform shall support the generation of metadata in a STAC [4] format for both raster and vector data, including applicable STAC extensions.\nThis requirement ensures that the outputs are compatible with modern geospatial data standards, enhancing interoperability with other platforms and services.\n\n\nHOST-REQ-03\nThe algorithm hosting platform shall support at least one of these API’s:\n\nOpenEO API version 1.2 [5]\nAt least API profile L1B-minimal-batch-jobs [6]\nAt least processes profiles L1-minimal [7]\nOGC API Processes [8], supporting the deployment of applications defined by the OGC Best Practice for Earth Observation Application Package [9].\n\nThe support for the proposed technologies ensures that APEx-compliant algorithms can be hosted on the algorithm hosting platform and made available as on-demand services.\n\n\nHOST-REQ-04\nThe operator of the algorithm hosting platform shall expose application workflows as an on-demand services that can be called via an interoperable API by a 3rd party that wants to integrate the results into its own workflow.\nBy offering on-demand services via an interoperable API, the platform enables seamless automation and integration of the services across different applications and workflows.\n\n\nHOST-REQ-05\nThe algorithm hosting platform shall be registered in the ESA Network of Resources.\nTo curate services beyond the project lifetime, the APEx consortium is likely to require an active subscription on the platform. For paid subscriptions, this can only be requested via NoR sponsoring. This may not be necessary if the platform is willing to grant non-paid subscriptions.\n\n\nHOST-REQ-06\nThe algorithm hosting platform shall provide an SLA that guarantees support beyond the project lifetime.\nThis ensures the long-term sustainability and reliability of the algorithm hosting platform, providing assurance to users that they can rely on continued support even after the project’s completion.\n\n\nHOST-REQ-07\nThe operator of the algorithm hosting platform shall announce major changes to the SLA, including decommissioning of the platform, to APEx and the NoR, with a lead time of 1 year.\nSuch communication is important to ensure that stakeholders, including APEx and the NoR, are given adequate notice of major changes that could impact the availability or functionality of the algorithm hosting platform. This approach allows for proper planning, adjustment, and mitigation of potential disruptions, ensuring continuity of services for users.",
    "crumbs": [
      "Interoperability and Compliance Guidelines",
      "Algorithm Hosting Platforms Guidelines"
    ]
  },
  {
    "objectID": "interoperability/algohostingenv.html#openeo-api-specific-requirements",
    "href": "interoperability/algohostingenv.html#openeo-api-specific-requirements",
    "title": "Algorithm Hosting Platforms Guidelines",
    "section": "openEO API specific requirements",
    "text": "openEO API specific requirements\nThe algorithm hosting platform implementing openEO API [10] shall support applications defined as openEO User Defined Processes (UDP) [11]. In the context of APEx, these will be executed as openEO batch jobs. The minimal requirements for an openEO algorithm hosting platform to support this feature are described in the profile called L1B-minimal-batch-jobs [6]. The openEO API, provided by the algorithm hosting platform, shall support at least the L1-minimal processes profile [7].\n\nOpen Source UDPs\nFor open-source UDPs, APEx will use the openEO remote UDP extension [12], which is currently under review by the openEO community. This extension enables APEx to centrally store and manage the UDP definitions, using the algorithm hosting platform only for the actual processing. This is to ensure consistent management of UDP definitions, safeguarding them from loss if the algorithm hosting platform is decommissioned.\nWe assume that this is the default case because EO projects have a strong preference for open-source software.\n\n\nPrivate UDPs\nThe specification of openEO allows platforms to expose custom processes that function like openEO UDPs but do not expose the underlying process graph. This situation may arise for two main reasons:\n\nThe process graph exists, but the project is not required to share it publicly. In this case, we assume that this is supported by ESA, and it is considered good practice to use openEO.\nThe process graph does not exist, and the process triggers an arbitrary processing system. In this case, the OGC Application Package approach might be a better alternative.",
    "crumbs": [
      "Interoperability and Compliance Guidelines",
      "Algorithm Hosting Platforms Guidelines"
    ]
  },
  {
    "objectID": "interoperability/algohostingenv.html#ogc-application-package-specific-requirements",
    "href": "interoperability/algohostingenv.html#ogc-application-package-specific-requirements",
    "title": "Algorithm Hosting Platforms Guidelines",
    "section": "OGC Application Package Specific Requirements",
    "text": "OGC Application Package Specific Requirements\nEO Application Packages, defined according to the OGC Best Practice [9], use the Common Workflow Language (CWL) [13] for process descriptions and are often containerised (e.g., using Docker) to ensure portability and consistency. To host and execute these packages effectively, platforms must meet a set of key requirements, particularly around data handling and interoperability, such as:\n\nSupport for Application Deployment: Platforms must be capable of interpreting CWL-based workflows and deploying the associated containerised environments.\nData Staging Using STAC: Efficient stage-in (input data retrieval) and stage-out (output data publication) are essential for EO workflows. Platforms should integrate with STAC to:\n\nDiscover input datasets through STAC-compliant APIs or catalogues.\nFacilitate the publication of results in a STAC-compliant format for downstream use.\nEnsure metadata-rich, standardised access to EO data, streamlining workflows.\n\nResource and Process Management: Platforms should provide scalable computational resources to execute applications efficiently. Resource management might include job scheduling, monitoring, and optimising containerised environments.\nStandardised API Support: Platforms should implement the OGC API - Processes [8] to allow consistent interaction with workflows, enabling users to manage jobs and execute processes\n\nGuidelines and recommendations to set up a cloud environment that is compliant with this approach can be obtained directly from the EOEPCA project. The project includes documentation, software and deployment configurations (e.g. helm charts) to cover the needed platform requirements.",
    "crumbs": [
      "Interoperability and Compliance Guidelines",
      "Algorithm Hosting Platforms Guidelines"
    ]
  },
  {
    "objectID": "interoperability/algohostingenv.html#upscaling-recommendations",
    "href": "interoperability/algohostingenv.html#upscaling-recommendations",
    "title": "Algorithm Hosting Platforms Guidelines",
    "section": "Upscaling Recommendations",
    "text": "Upscaling Recommendations\nAPEx provides specific recommendations for leveraging the algorithm hosting platform to support large-scale data processing activities. For a detailed overview of best practices, please visit the APEx Upscaling support page.",
    "crumbs": [
      "Interoperability and Compliance Guidelines",
      "Algorithm Hosting Platforms Guidelines"
    ]
  },
  {
    "objectID": "interoperability/algohostingenv.html#service-level-commitments",
    "href": "interoperability/algohostingenv.html#service-level-commitments",
    "title": "Algorithm Hosting Platforms Guidelines",
    "section": "Service Level Commitments",
    "text": "Service Level Commitments\nThe service level commits for algorithm providers and algorithm hosting operators will be further documented by the work done in the APEx project.",
    "crumbs": [
      "Interoperability and Compliance Guidelines",
      "Algorithm Hosting Platforms Guidelines"
    ]
  },
  {
    "objectID": "interoperability/geospatial_explorer.html",
    "href": "interoperability/geospatial_explorer.html",
    "title": "Geospatial Explorer",
    "section": "",
    "text": "This section outlines the requirements for the interoperability of the APEx Geospatial Explorer services. These requirements must be met to ensure the correct configuration and operation of a dashboard and its instantiation.\nA technical challenge of the Geospatial Explorer service being provided by APEx is that it is to be instantiated on demand, with functional requirements potentially varying amongst each service. A proposed solution to this challenge is the use of a well-defined configuration schema, provided in the form of JSON, that outlines the interactive features and data sources to be used.\nThis approach will allow APEx to define and update the schema required in the interoperability guidelines, which will then enable requesters of the service to configure the Geospatial Explorer on an individual project level with minimal external intervention.\nThe schema will be versioned as it will change throughout the APEx project as the functional capabilities of the Geospatial Explorer mature. This does allow for improvements and extra features to be easily added to the application, and best practices shall be followed to avoid any breaking changes between versions.",
    "crumbs": [
      "Interoperability and Compliance Guidelines",
      "Geospatial Explorer"
    ]
  },
  {
    "objectID": "interoperability/geospatial_explorer.html#requirements",
    "href": "interoperability/geospatial_explorer.html#requirements",
    "title": "Geospatial Explorer",
    "section": "Requirements",
    "text": "Requirements\nTable 1 outlines the requirements for configuring an instance of the Geospatial Explorer application.\n\n\n\nTable 1: Interoperability requirements for the Geospatial Explorer application\n\n\n\n\n\n\n\n\n\n\nID\nRequirement\nDescription\n\n\n\n\nEXPLORER-REQ-01\nConfiguration of the Geospatial Explorer shall adhere to the provided JSON schema.\nThe application is configured through a hosted JSON object which is fetched when the client-side application loads. For the application to correctly render, a valid configuration JSON that follows the outlined schema must be provided pre-instantiation. During early development the schema will be subject to change and provided with limited validation.\n\n\nEXPLORER-REQ-02\nRaster sources defined within the config shall be provided as either:\n\nA well formed URL to a hosted Cloud Optimised GeoTiff (COG) [1].\nA WMS [2] or WMTS [3] endpoint following the relevant OGC specifications.\nA template URL following the XYZ(*) format serving PNG or JPEG tiles .\n\nThe use of these formats ensures that raster data can be visualized using widely recognized and well-established standards.\n\n\nEXPLORER-REQ-03\nVector sources defined within the config shall be provided as either:\n\nA well formed URL to a valid GeoJSON [4] resource.\nA public WFS [5] or OGC API Features [6] endpoint following the relevant OGC specifications.\n\nThe use of these formats ensures that vector data can be visualized using widely recognized and well-established standards.\n\n\nEXPLORER-REQ-04\nAll external sources shall be publicly available.\nCurrently the application does not support fetching sources from hosts that require authentication.\n\n\nEXPLORER-REQ-05\nAll external sources shall be available utilising the HTTPS protocol.\nIt is best practice to utilise HTTPS where possible. Whilst not as of yet blocked specifically, non-secure resources may not function correctly.\n\n\nEXPLORER-REQ-06\nAll datasets shall be projected to CRS EPSG:3857 (Web Mercator).\nOther projections may work but will not be explicitly supported in the initial versions of the application. Support for these projections will be considered and reviewed on a case-by-case basis.\n\n\nEXPLORER-REQ-07\nLegends should be configured by providing:\n\nAn inline configuration using either the ‘Swatch’ or ‘Gradient’ outlined in the schema.\nA URL to a browser supported image resource (PNG, JPEG, SVG).\nA WMS [2] getLegendGraphic request (If supported by a WMS source).\n\nThis guideline ensures that the legend is fully compatible with the visualisation library used in the Geospatial Explorer.\n\n\n\n\n\n\n(*) The XYZ approach refers to a de facto API standard for URL structuring (Z = zoom level, X and Y = grid references). While there is no dedicated specification for XYZ, it is widely used (e.g., OpenStreetMap).",
    "crumbs": [
      "Interoperability and Compliance Guidelines",
      "Geospatial Explorer"
    ]
  },
  {
    "objectID": "interoperability/geospatial_explorer.html#configuration-schema",
    "href": "interoperability/geospatial_explorer.html#configuration-schema",
    "title": "Geospatial Explorer",
    "section": "Configuration Schema",
    "text": "Configuration Schema\nThe service configuration will be based on a schema that provides administrators with the expected structure and contents of the configuration. Taking this approach enables:\n\nAutomated and dynamic instantiation of the service with differing functionality.\nConfiguration validation.\nDefinition of a “contract” for easier documentation of features and their configuration.\n\nThe sections below briefly outline the structure of the configuration schema and provide a preliminary description of each field/property within. The schema currently consists of four top-level fields and all properties are written in camel case.\n\nLayout - layout\nAn object with properties that modify the elements that the application will render. These properties and elements relate specifically to non-geospatial layout components, like navigation and footer.\nCurrently supported properties:\n\nNavigation - navigation\nAn object which supports two properties:\n\nlogo: A URL string that points to a logo image asset.\ntitle: A string to be used as a title for the application.\n\n\n\n\nInterface Groups - interfaceGroups\nAn optional array of strings to be used as names/keys. This is currently used to configure the grouping of layer UI elements, such as the layer cards. This will be expanded in later versions.\n\n\nExclusivity Sets - exclusivitySets\nAn optional array of strings to be used as names/keys. This is currently not in use but is a placeholder for future work.\n\n\nSources - sources\nAn array of objects. Each object outlines a particular data source to be configured for display within the application, with properties detailing both the geospatial and user interface configuration.\nCurrently supported properties within a source object are:\n\nName - name\nA string that is used to identify layers in both the user interface and OpenLayers state\n\n\nIs Active - isActive\nA boolean. Determines if a layer is currently shown on the map. Setting this to true will show the layer on the map when the application loads.\n\n\nLayout - layout\nAn object to determine which interface elements are rendered for the layer. Supports two properties:\n\nLayer Card - layerCard\nAn object that will determine if a layer card should be rendered for this layer and what other interface elements should be rendered within the card. This is currently the main way to interact with a layer within the application. The layer card can show a toggle for the layer, a selection of buttons or controls for the layer, legends and attribution text. This currently supports the following properties:\n\ntoggleable: A boolean that determines if a toggle switch to enable/disable the layer should be rendered.\ncontrols: An optional array of strings that configure which buttons to render in the layer card for interaction with the layer.\nlegend: An optional object that can be configured to show static or dynamic legend elements within the layer card when active.\nattribution: An optional object to render some text or a link for use with attribution of layer datasets.\n\n\n\nInterface Group - interfaceGroup\nAn optional string that is used to identify which interface group this layer belongs to.\n\n\n\nData - data\nAn object that configures the data to be displayed in the layer. This currently supports the following properties:\n\nurl: A required URL string that points to the dataset’s publicly available resource.\ntype: A required string that identifies what kind of dataset is requested. This can be one of the following: ’ wms’, ‘wmts’, ‘cog’, ‘xyz’, ‘wfs’ or ‘geojson’.\nlayers: Only required for sources of type: ‘wms’ and ‘wmts’. A string that describes the layer to be requested from the external service.\ntypeName: Only required for sources of type: ‘wfs’. A string that describes the type to be requested from the external service.\nzIndex: Optional integer that determines rendering order within the map. It can be used to override the default rendering of Open Layers.\nexclusivitySet: Optional string used to identify a group of other layers that should be disabled when this layer is enabled. They must share the same string.\nprojection: Optional EPSG code string that describes the projection of the dataset to Open Layers. If the projection is supported (and doesn’t match the map’s configured projection), it will attempt to reproject the data.\nstyle: Open Layers style object that is passed through to the library to modify the rendering of the layer within the map.\nnormalise: Only required for sources of type: ‘cog’. Boolean that configures the map to normalise the raster pixel values to between 0 and 1. False by default.\nimages: Only required for sources of type: ‘cog’. An array of objects that contain a URL property pointing to a COG resource. Replaces the ‘url’ property for this source type. Allows loading multiple GeoTiffs into one layer.",
    "crumbs": [
      "Interoperability and Compliance Guidelines",
      "Geospatial Explorer"
    ]
  },
  {
    "objectID": "interoperability/geospatial_explorer.html#example-schema",
    "href": "interoperability/geospatial_explorer.html#example-schema",
    "title": "Geospatial Explorer",
    "section": "Example Schema",
    "text": "Example Schema\nThe following is an example of a working JSON file that may be used to configure the application.\n\n\n\n\n\n\nWarning\n\n\n\nThe API is an early draft that will be subject to significant ongoing changes. The following is for illustrative purposes only.\n\n\n{\n    \"layout\": {\n        \"navigation\": {\n            \"logo\": \"https://www.esa.int/extension/pillars/design/pillars/images/ESA_Logo.svg\",\n            \"title\": \"APEx Geospatial Explorer\"\n        }\n    },\n    \"interfaceGroups\": [\n        \"Vectors\",\n        \"Landcover\",\n        \"Basemaps\",\n        \"STAC\"\n    ],\n    \"exclusivitySets\": [\n        \"Landcover\"\n    ],\n    \"sources\": [\n        {\n            \"name\": \"Open Street Map\",\n            \"isActive\": true,\n            \"layout\": {\n                \"interfaceGroup\": \"Basemaps\"\n            },\n            \"data\": {\n                \"url\": \"https://tile.openstreetmap.org/{z}/{x}/{y}.png\",\n                \"type\": \"xyz\",\n                \"zIndex\": 0\n            }\n        },\n        {\n            \"name\": \"WMS Layer\",\n            \"isActive\": false,\n            \"layout\": {\n                \"layerCard\": {\n                    \"toggleable\": true,\n                    \"controls\": [\n                        \"zoomToCenter\"\n                    ],\n                    \"legend\": {\n                        \"type\": \"swatch\",\n                        \"visible\": true,\n                        \"data\": [\n                            {\n                                \"color\": \"rgb(0, 100, 0)\",\n                                \"label\": \"Tree cover\"\n                            },\n                            {\n                                \"color\": \"rgb(255, 187, 34)\",\n                                \"label\": \"Shrubland\"\n                            },\n                            {\n                                \"color\": \"rgb(255, 255, 76)\",\n                                \"label\": \"Grassland\"\n                            },\n                            {\n                                \"color\": \"rgb(240, 150, 255)\",\n                                \"label\": \"Cropland\"\n                            },\n                            {\n                                \"color\": \"rgb(255, 0, 0)\",\n                                \"label\": \"Built up\"\n                            },\n                            {\n                                \"color\": \"rgb(180, 180, 180)\",\n                                \"label\": \"Bare\"\n                            },\n                            {\n                                \"color\": \"rgb(240, 240, 240)\",\n                                \"label\": \"Snow and ice\"\n                            },\n                            {\n                                \"color\": \"rgb(0, 100, 200)\",\n                                \"label\": \"Permanent water bodies\"\n                            },\n                            {\n                                \"color\": \"rgb(0, 150, 160)\",\n                                \"label\": \"Herbaceous wetland\"\n                            },\n                            {\n                                \"color\": \"rgb(0, 207, 117)\",\n                                \"label\": \"Mangroves\"\n                            },\n                            {\n                                \"color\": \"rgb(250, 230, 160)\",\n                                \"label\": \"Moss and lichen\"\n                            }\n                        ]\n                    }\n                },\n                \"interfaceGroup\": \"Landcover\"\n            },\n            \"data\": {\n                \"url\": \"https://services.terrascope.be/wms/v2\",\n                \"layers\": \"WORLDCOVER_2021_MAP\",\n                \"exclusivitySet\": \"Landcover\",\n                \"zIndex\": 2,\n                \"type\": \"wms\"\n            }\n        },\n        {\n            \"name\": \"WMTS Layer\",\n            \"isActive\": false,\n            \"layout\": {\n                \"layerCard\": {\n                    \"toggleable\": true,\n                    \"controls\": [\n                        \"zoomToCenter\"\n                    ],\n                    \"legend\": {\n                        \"type\": \"swatch\",\n                        \"visible\": true,\n                        \"data\": [\n                            {\n                                \"color\": \"rgb(0, 100, 0)\",\n                                \"label\": \"Tree cover\"\n                            },\n                            {\n                                \"color\": \"rgb(255, 187, 34)\",\n                                \"label\": \"Shrubland\"\n                            },\n                            {\n                                \"color\": \"rgb(255, 255, 76)\",\n                                \"label\": \"Grassland\"\n                            },\n                            {\n                                \"color\": \"rgb(240, 150, 255)\",\n                                \"label\": \"Cropland\"\n                            },\n                            {\n                                \"color\": \"rgb(255, 0, 0)\",\n                                \"label\": \"Built up\"\n                            },\n                            {\n                                \"color\": \"rgb(180, 180, 180)\",\n                                \"label\": \"Bare\"\n                            },\n                            {\n                                \"color\": \"rgb(240, 240, 240)\",\n                                \"label\": \"Snow and ice\"\n                            },\n                            {\n                                \"color\": \"rgb(0, 100, 200)\",\n                                \"label\": \"Permanent water bodies\"\n                            },\n                            {\n                                \"color\": \"rgb(0, 150, 160)\",\n                                \"label\": \"Herbaceous wetland\"\n                            },\n                            {\n                                \"color\": \"rgb(0, 207, 117)\",\n                                \"label\": \"Mangroves\"\n                            },\n                            {\n                                \"color\": \"rgb(250, 230, 160)\",\n                                \"label\": \"Moss and lichen\"\n                            }\n                        ]\n                    }\n                },\n                \"interfaceGroup\": \"Landcover\"\n            },\n            \"data\": {\n                \"url\": \"https://services.terrascope.be/wmts/v2\",\n                \"layers\": \"WORLDCOVER_2021_MAP\",\n                \"exclusivitySet\": \"Landcover\",\n                \"zIndex\": 2,\n                \"type\": \"wmts\"\n            }\n        },\n        {\n            \"name\": \"GeoJSON Layer\",\n            \"isActive\": false,\n            \"layout\": {\n                \"layerCard\": {\n                    \"toggleable\": true\n                },\n                \"interfaceGroup\": \"Vectors\"\n            },\n            \"data\": {\n                \"url\": \"https://dataworks.calderdale.gov.uk/download/24qmx/p18/Public%20Rights%20of%20Way%20-%20Jan%202022%20-%20JSON.json\",\n                \"zIndex\": 10,\n                \"type\": \"geojson\"\n            }\n        },\n        {\n            \"name\": \"WFS Layer\",\n            \"isActive\": false,\n            \"layout\": {\n                \"layerCard\": {\n                    \"toggleable\": true,\n                    \"controls\": [\n                        \"zoomToCenter\"\n                    ]\n                },\n                \"interfaceGroup\": \"Vectors\"\n            },\n            \"data\": {\n                \"url\": \"http://ogc.bgs.ac.uk/digmap625k_gsml_cgi_gs/wfs\",\n                \"zIndex\": 10,\n                \"type\": \"wfs\",\n                \"typeName\": \"gsmlp:GeologicUnitView\"\n            }\n        },\n        {\n            \"name\": \"GTIF - Above Ground Biomass\",\n            \"isActive\": false,\n            \"layout\": {\n                \"layerCard\": {\n                    \"toggleable\": true,\n                    \"attribution\": {\n                        \"text\": \"Forest Carbon Monitoring\",\n                        \"url\": \"https://www.forestcarbonplatform.org/\"\n                    },\n                    \"legend\": {\n                        \"visible\": true,\n                        \"type\": \"gradient\",\n                        \"data\": {\n                            \"startColor\": \"rgb(255, 255, 255)\",\n                            \"endColor\": \"rgb(0, 100, 0)\"\n                        }\n                    }\n                }\n            },\n            \"data\": {\n                \"zIndex\": 3,\n                \"type\": \"cog\",\n                \"normalize\": false,\n                \"images\": [\n                    {\n                        \"url\": \"https://eox-gtif-public.s3.eu-central-1.amazonaws.com/FCM/v2/JR/FCM_AGB-2021_Austria_20m_EPSG3857-COG.tif\"\n                    }\n                ],\n                \"style\": {\n                    \"color\": [\n                        \"interpolate\",\n                        [\n                            \"linear\"\n                        ],\n                        [\n                            \"band\",\n                            1\n                        ],\n                        0,\n                        \"rgba(0, 0, 0, 0)\",\n                        1,\n                        \"rgb(255, 255, 255)\",\n                        100,\n                        \"rgb(0, 100, 0)\"\n                    ]\n                }\n            }\n        },\n        {\n            \"name\": \"STAC Item\",\n            \"isActive\": false,\n            \"layout\": {\n                \"layerCard\": {\n                    \"toggleable\": true\n                },\n                \"interfaceGroup\": \"STAC\"\n            },\n            \"data\": {\n                \"url\": \"https://s3.us-west-2.amazonaws.com/sentinel-cogs/sentinel-s2-l2a-cogs/10/T/ES/2022/7/S2A_10TES_20220726_0_L2A/S2A_10TES_20220726_0_L2A.json\",\n                \"zIndex\": 5,\n                \"type\": \"stac\"\n            }\n        },\n        {\n            \"name\": \"World Soil\",\n            \"isActive\": false,\n            \"layout\": {\n                \"layerCard\": {\n                    \"toggleable\": true\n                }\n            },\n            \"data\": {\n                \"zIndex\": 3,\n                \"projection\": \"EPSG:3035\",\n                \"type\": \"cog\",\n                \"normalize\": false,\n                \"images\": [\n                    {\n                        \"url\": \"http://localhost:5173/europe_aggr-orgc_00-020_mean_100_201803-202010.tif\"\n                    }\n                ],\n                \"style\": {\n                    \"color\": [\n                        \"interpolate\",\n                        [\n                            \"linear\"\n                        ],\n                        [\n                            \"band\",\n                            1\n                        ],\n                        0,\n                        \"rgba(0, 0, 0, 0)\",\n                        22,\n                        \"rgb(10,10,40)\",\n                        5344,\n                        \"rgb(173,216,230)\"\n                    ]\n                }\n            }\n        }\n    ]\n}",
    "crumbs": [
      "Interoperability and Compliance Guidelines",
      "Geospatial Explorer"
    ]
  },
  {
    "objectID": "instantiation/documentation.html",
    "href": "instantiation/documentation.html",
    "title": "Documentation Portal",
    "section": "",
    "text": "The APEx Documentation Portal enables projects to effortlessly create and host a documentation portal utilising the open source Quarto framework. Quarto is a comprehensive open-source system designed for scientific and technical documentation, using Markdown as the primary editing language and supporting the visualisation of both Jupyter and R notebooks. The proposed approach is complementary to hosting documents in PDF or Word format on the web portal. It offers significant advantages over these traditional formats, especially for technical writing with scientific plotting and code snippets. It also allows for introducing interactive elements in documentation. Projects that do not feel comfortable with an approach based on Markdown for editing and Git for versioning are still free to use those traditional formats.\nQuarto generates fully customisable online documentation portals tailored for scientific publications, featuring interactive visualisations. It supports various scientific elements, including formulas, diagrams, and code. Additionally, Quarto can produce PDF or Word documents, making it ideal for scenarios where user documentation needs to be transformed into a project deliverable.\nThe content of the documentation portal is managed through either a new or existing GitHub repository. This allows portal managers and editors to edit the content in a well-known and familiar environment. Although some technical documents may be stored externally, Quarto enables referencing of these documents within the documentation portal. Additionally, thanks to the capabilities of GitHub, projects have the option to employ their own QA processes by leveraging branching, issues, and pull requests. Using GitHub Actions, the latest changes to the content will be automatically synchronised to the documentation portal.",
    "crumbs": [
      "Project Tools",
      "Documentation Portal"
    ]
  },
  {
    "objectID": "instantiation/documentation.html#overview",
    "href": "instantiation/documentation.html#overview",
    "title": "Documentation Portal",
    "section": "",
    "text": "The APEx Documentation Portal enables projects to effortlessly create and host a documentation portal utilising the open source Quarto framework. Quarto is a comprehensive open-source system designed for scientific and technical documentation, using Markdown as the primary editing language and supporting the visualisation of both Jupyter and R notebooks. The proposed approach is complementary to hosting documents in PDF or Word format on the web portal. It offers significant advantages over these traditional formats, especially for technical writing with scientific plotting and code snippets. It also allows for introducing interactive elements in documentation. Projects that do not feel comfortable with an approach based on Markdown for editing and Git for versioning are still free to use those traditional formats.\nQuarto generates fully customisable online documentation portals tailored for scientific publications, featuring interactive visualisations. It supports various scientific elements, including formulas, diagrams, and code. Additionally, Quarto can produce PDF or Word documents, making it ideal for scenarios where user documentation needs to be transformed into a project deliverable.\nThe content of the documentation portal is managed through either a new or existing GitHub repository. This allows portal managers and editors to edit the content in a well-known and familiar environment. Although some technical documents may be stored externally, Quarto enables referencing of these documents within the documentation portal. Additionally, thanks to the capabilities of GitHub, projects have the option to employ their own QA processes by leveraging branching, issues, and pull requests. Using GitHub Actions, the latest changes to the content will be automatically synchronised to the documentation portal.",
    "crumbs": [
      "Project Tools",
      "Documentation Portal"
    ]
  },
  {
    "objectID": "instantiation/documentation.html#showcase-scenarios",
    "href": "instantiation/documentation.html#showcase-scenarios",
    "title": "Documentation Portal",
    "section": "Showcase Scenarios",
    "text": "Showcase Scenarios\nA documentation portal can support several project scenarios, including:\n\nUser Manuals and Guides\nDevelop and store user manuals and guides for any tools or software created during the project. This ensures that users can effectively utilise the provided tools.\nTraining and Onboarding\nCreate and store training materials, tutorials, and onboarding guides for new team members or external visitors. This helps them quickly understand the project scope, tools, and processes.\nTechnical Project Documentation\nMaintain detailed technical documentation, including software code samples, algorithm descriptions, system architectures, and hardware specifications. This is essential for sharing technical information within the project.\nResearch and Analysis Documentation\nDocument methodologies, analytical processes, and research findings. This helps ensure the reproducibility of results and provides a reference for future research.\nProject Planning and Management\nStore project plans, timelines, milestones, and deliverables in the documentation portal. This keeps everyone informed about project progress and deadlines.",
    "crumbs": [
      "Project Tools",
      "Documentation Portal"
    ]
  },
  {
    "objectID": "instantiation/documentation.html#what-does-apex-offer",
    "href": "instantiation/documentation.html#what-does-apex-offer",
    "title": "Documentation Portal",
    "section": "What does APEx offer",
    "text": "What does APEx offer\nAPEx provides a simple template to start your documentation. As you make changes, automation tools publish the documentation to a location that seamlessly integrates it in your project portal. This works best in combination with the ‘project portal’ service, ensuring maximal integration.",
    "crumbs": [
      "Project Tools",
      "Documentation Portal"
    ]
  },
  {
    "objectID": "instantiation/documentation.html#what-software-is-used-under-the-hood",
    "href": "instantiation/documentation.html#what-software-is-used-under-the-hood",
    "title": "Documentation Portal",
    "section": "What software is used under the hood",
    "text": "What software is used under the hood\nThe open source Quarto software is used, and you will need to learn the basics of that system. In our own experience, people familiar with Markdown will have no issues with making basic edits. More advanced features are well-documented, and often turn out to be easier to use than the equivalent in office tools. Many features are simply not at all supported by such tools.\n\n\n\n\n\n\nStay Tuned\n\n\n\nAdditional information will be shared on this page as the project progresses.",
    "crumbs": [
      "Project Tools",
      "Documentation Portal"
    ]
  },
  {
    "objectID": "instantiation/catalog.html",
    "href": "instantiation/catalog.html",
    "title": "Product Catalogue",
    "section": "",
    "text": "The Product Catalogue enables projects to create their own SpatioTemporal Asset Catalogue (STAC), which serves as a comprehensive metadata collection for efficient organisation and discovery of geospatial data assets. This information is presented in a user-friendly JSON format.\nInstantiating a project-specific catalogue gives projects the freedom to use it for a range of purposes, from experimental to operational use cases. Assets that are considered project deliverables will normally be added to the centralised project results repository.\nSTAC structures data around assets, collections, and catalogues, simplifying search and access based on attributes such as time, location, and other metadata. The concept of a STAC catalogue emphasises simplicity, operating as a REST-based web service with a single HTTP endpoint for communication. This API-centric design ensures easy integration into existing applications and tools, including other APEx services.",
    "crumbs": [
      "Project Tools",
      "Product Catalogue"
    ]
  },
  {
    "objectID": "instantiation/catalog.html#overview",
    "href": "instantiation/catalog.html#overview",
    "title": "Product Catalogue",
    "section": "",
    "text": "The Product Catalogue enables projects to create their own SpatioTemporal Asset Catalogue (STAC), which serves as a comprehensive metadata collection for efficient organisation and discovery of geospatial data assets. This information is presented in a user-friendly JSON format.\nInstantiating a project-specific catalogue gives projects the freedom to use it for a range of purposes, from experimental to operational use cases. Assets that are considered project deliverables will normally be added to the centralised project results repository.\nSTAC structures data around assets, collections, and catalogues, simplifying search and access based on attributes such as time, location, and other metadata. The concept of a STAC catalogue emphasises simplicity, operating as a REST-based web service with a single HTTP endpoint for communication. This API-centric design ensures easy integration into existing applications and tools, including other APEx services.",
    "crumbs": [
      "Project Tools",
      "Product Catalogue"
    ]
  },
  {
    "objectID": "instantiation/catalog.html#showcase-scenarios",
    "href": "instantiation/catalog.html#showcase-scenarios",
    "title": "Product Catalogue",
    "section": "Showcase Scenarios",
    "text": "Showcase Scenarios\nThe product catalogue serves several beneficial scenarios for projects:\n\nData Accessibility\nThe service offers easy access to a wide range of geospatial data sources, enabling project teams to quickly locate and utilise relevant data without the need to manage large datasets locally.\nTool Integration\nBy adhering to the STAC standard, the service enhances interoperability among different data sources, software tools and processing platforms such as openEO and OGC Application Packages. This ensures seamless integration of datasets from various providers, supporting comprehensive analysis and decision-making. This interoperability extends to other APEx tools such as the Geospatial Explorer, Interactive Development Environments, User Workspaces….\nEfficient Search and Discovery\nUsers can efficiently search and discover datasets based on specific criteria such as time, location, and data type. This capability streamlines research, planning, and operational tasks by providing quick access to relevant information.\nEnhanced Collaboration\nThe service promotes collaboration by providing a centralised platform where project stakeholders can access and share geospatial data and analyses. This collaborative environment fosters innovation and knowledge sharing across disciplines and organisations.",
    "crumbs": [
      "Project Tools",
      "Product Catalogue"
    ]
  },
  {
    "objectID": "instantiation/catalog.html#what-does-apex-offer",
    "href": "instantiation/catalog.html#what-does-apex-offer",
    "title": "Product Catalogue",
    "section": "What does APEx offer?",
    "text": "What does APEx offer?\n\nSTAC Catalogue API\nYou will get a dedicated STAC catalog, hosted and managed by APEx. This gives you full control over the metadata, without having to worry about IT aspects such as security, backup or monitoring the service. The catalog supports creating both private and public collections.\n\nKey Considerations\n\nData storage: The STAC catalog does not include data storage. It is recommended to request object storage access from one of the cloud providers in the Network of Resources (NoR).\nMetadata Creation: The process of creating metadata is not automated. You will need to provide the metadata in JSON format, giving you complete flexibility in choosing the STAC extensions that best suit your requirements.\n\n\n\nUnderlying Technology\n\nSoftware: The catalog is powered by stac-fastapi-elasticsearch-opensearch.\nDatabase: It uses a managed OpenSearch instance hosted on Open Telekom Cloud.\n\nWhile the APEx team handles software upgrades when necessary, please note that support for custom software development is not provided.\n\n\n\nSTAC Browser\nIn addition to the STAC Catalog API, APEx provides a dedicated STAC Browser to support the visual exploration of your catalog’s content. This browser is seamlessly integrated with the instantiated API and is automatically configured upon initialization to display the results published in your STAC API.\n\n\n\nSTAC Browser - Collections\n\n\n\n\n\nSTAC Browser - Collection Details\n\n\n\nUnderlying Technology\nThe STAC browser is built on the well-known Radiant Earth’s STAC browser, an open-source project designed to enable interactive exploration of STAC catalogs. This web application can be customized to meet specific user needs, offering flexibility for a wide range of applications.",
    "crumbs": [
      "Project Tools",
      "Product Catalogue"
    ]
  },
  {
    "objectID": "instantiation/catalog.html#how-to-integrate-the-catalog-in-your-project-architecture",
    "href": "instantiation/catalog.html#how-to-integrate-the-catalog-in-your-project-architecture",
    "title": "Product Catalogue",
    "section": "How to integrate the catalog in your project architecture",
    "text": "How to integrate the catalog in your project architecture\nThe role of the STAC catalog, is in essence to facilitate data discovery and exchange between different components in your project. By the use of a shared metadata language, components can be decoupled and still work together. It allows different project partners also to work together, even if they use different software stacks.\nWe list some common cases below.\n\nData set viewing\nData viewing is one of the most basic use cases. Most viewer implementations traditionally require to ingest data in a viewer specific database. However, when metadata is sufficiently rich, and the viewer understands STAC, it can directly query the STAC catalog, as long as it’s sufficiently responsive for the low latency requirements of the viewer.\n\n\nData processing\nIn the most basic case, a processing component simply stores final products in the catalog. Sufficiently advanced processing systems will immediately generate STAC metadata, and may even allow writing into the catalog API directly. They also record data provenance, which is a key aspect of FAIR data principles.\nIn a more advanced case, the processing workflow requires storing intermediate products, or preprocessed ancillary data sets. Also then, the STAC catalog can be used to keep track of these data sets. By doing so, the processing component can simply read required inputs from STAC, and store (intermediate) results back into the catalog.\n\n\nMachine learning datasets\nA more advanced but increasingly common use case is to use a catalog to track reference data sets, as well as training patches in a STAC catalog. Here we again have a pattern where various components like the EO processing system, model training scripts and reference data management system all work together around a central metadata store.",
    "crumbs": [
      "Project Tools",
      "Product Catalogue"
    ]
  },
  {
    "objectID": "instantiation/catalog.html#examples",
    "href": "instantiation/catalog.html#examples",
    "title": "Product Catalogue",
    "section": "Examples",
    "text": "Examples\nThe ‘Guides’ section has a number of concrete Jupyter notebooks showing how you can interact with the catalog API.\n\n\n\n\n\n\nStay Tuned\n\n\n\nAdditional information will be shared on this page as the project progresses.",
    "crumbs": [
      "Project Tools",
      "Product Catalogue"
    ]
  },
  {
    "objectID": "instantiation/index.html",
    "href": "instantiation/index.html",
    "title": "Dynamic software components for your project",
    "section": "",
    "text": "The APEx Project Tools are designed to provide managed, configurable environments that support the collaboration, development, exploration, and visualisation of application project results. These tools are crucial for facilitating the effective sharing, maintenance, and utilisation of project outcomes within the Earth Observation (EO) community, ensuring that they remain readily accessible and usable for extended periods.\nAPEx aims to cater to the diverse and multifaceted needs of the EO community by delivering Software as a Service (SaaS) products. It manages the delivery of single-user or shared work environments within a unified cloud infrastructure, facilitating a wide range of user tasks, including development, hosting, execution, and exploratory analysis of EO applications.\nAt the core of the APEx Project Tools is the ability to manage and deliver these environments effectively. Project-wide community-oriented tools, such as the portal, catalogue, documentation portal, and user forum, are managed directly by Kubernetes and typically instantiated once per project. Conversely, single-user-specific workspaces allow the instantiation of tools by a single user at any time. These tools, including the User Workspace, Interactive Development Environment (IDE), and, in certain cases, dashboards and web applications, are managed by JupyterHub. JupyterHub orchestrates the launching and management of these software deployments, ensuring isolated and customisable environments for individual users.\nA key feature of the APEx Project Tools is the ability to provide managed, configurable environments. These environments can be tailored to the specific needs of different projects and users, supporting various tasks, including development, hosting, execution, and exploratory analysis of EO applications. This flexibility and configurability allow these projects to focus on their primary research objectives without being bogged down by the technical complexities of setting up and maintaining their workspaces.\nAnother critical component of the APEx Instantiation Services is the seamless integration with the ESA’s Network of Resources (NoR). The focus is on developing new tools, which are those that do not currently exist in the NoR. These new tools are to be developed and onboarded into the NoR, making them available as payable services. For tools components that already exist, such as the ELLIP IDE, the goal is to integrate these existing services. The technical challenge lies in achieving seamless integration and alignment of business models, ensuring that newly developed and existing services work harmoniously within the APEx.\nThe specific components of the APEx Project Tools include:\n\nGeospatial Explorer\nProvide a data-driven user interface to display and visualise geospatial and tabular data from a range of supported web services based on a configuration defined by a dashboard administrator.\nProject Portals\nCreating a project website based on Drupal, working in synergy with other APEx services.\nUser Workspaces\nOffering secure and personalised work environments with data-sharing mechanisms.\nInteractive Development Environments\nLeveraging Code Server (VS Code in the browser) tailored specifically for EO tasks.\nProduct Catalogues\nFeaturing SpatioTemporal Asset Catalog (STAC) catalogues and streamlined data ingestion processes.\nDocumentation Portals\nSupporting customisation and interactive visualisation using the Quarto framework.\nUser Forums\nProviding a community-building platform based on open-source software Discourse.\n\nThe APEx Project Tools will be essential for ensuring that the results of EO projects are effectively shared and utilised, fostering greater collaboration and innovation within the EO community. By providing robust, scalable, and user-friendly environments, the APEx Project Tools help maximise the impact of EO research and applications.",
    "crumbs": [
      "Project Tools"
    ]
  },
  {
    "objectID": "instantiation/usecases.html",
    "href": "instantiation/usecases.html",
    "title": "Use cases for APEx Project Tools",
    "section": "",
    "text": "The APEx Project Tools are designed to support a wide range of activities within the EO community, from project promotion and user support to data visualisation, data analytics, and data processing. By providing managed, configurable environments, these services facilitate the effective sharing, maintenance, and utilisation of project results, ensuring they remain readily accessible and usable for extended periods.\nThis section provides an initial outline of the primary use cases for APEx Project Tools, demonstrating how they address the specific needs and challenges faced by users and project managers in the EO domain. Each use case highlights the relevant components of the APEx tools, detailing how they work together to provide robust solutions for various tasks.\nFrom promoting EO projects through interactive portals to supporting users with personalised workspaces and comprehensive documentation, the APEx Project Tools are designed to enhance productivity and foster collaboration. Data visualisation tools enable users to interpret and communicate insights effectively, while advanced data analytics environments provide the computational power and tools necessary for complex analyses. Data processing workflows are streamlined through secure, scalable environments, ensuring that users can handle large volumes of EO data efficiently.\nThe following use cases provide detailed scenarios of how the APEx Project Tools can be leveraged to meet the diverse needs of the EO community, maximising the impact of EO research and applications.",
    "crumbs": [
      "Project Tools",
      "Use Cases"
    ]
  },
  {
    "objectID": "instantiation/usecases.html#use-case-1---project-promotion",
    "href": "instantiation/usecases.html#use-case-1---project-promotion",
    "title": "Use cases for APEx Project Tools",
    "section": "Use Case 1 - Project Promotion",
    "text": "Use Case 1 - Project Promotion\n\nDescription\nPromote EO projects by providing a centralised, customisable portal that showcases project outcomes, visualisations, and data products. This use case supports the dissemination and accessibility of project results to a broader audience, including stakeholders, policymakers, and the general public.\n\n\nKey Components\n\nProject Portal\nProvides dynamic instantiation using Drupal, allowing project managers to create and maintain a visually appealing and informative site.\nGeospatial Explorer\nCreate an online dashboard to present key metrics and visualisations.\nDocumentation Portal\nCustomizable portals using the Quarto framework for detailed project documentation and interactive visualisations.\nUser Forum\nSupports community engagement and discussions around the project outcomes.\n\n\n\nExpected Benefits\n\nIncreased visibility and impact of EO projects.\nEnhanced stakeholder engagement through accessible and interactive content.\nLong-term maintenance and accessibility of project results.",
    "crumbs": [
      "Project Tools",
      "Use Cases"
    ]
  },
  {
    "objectID": "instantiation/usecases.html#use-case-2--user-support",
    "href": "instantiation/usecases.html#use-case-2--user-support",
    "title": "Use cases for APEx Project Tools",
    "section": "Use Case 2- User Support",
    "text": "Use Case 2- User Support\n\nDescription\nProvide comprehensive support to users, enabling them to interact with EO data, tools, and services effectively. This use case focuses on offering personalised and collaborative environments where the project PI or their team can seek help, share knowledge, and collaborate on EO applications.\n\n\nKey Components\n\nUser Workspace\nSecure and personalised environments allow users to store, share, and manage their data and work.\nInteractive Development Environment (IDE)\nLeveraging VS Code Server, these environments are tailored for EO tasks and provide development tools.\nUser Forum\nAn on-demand service based on Discourse, facilitating user discussions, support queries, and community-driven solutions.\nDocumentation Portal\nComprehensive documentation and tutorial materials to assist users in navigating and utilising the services.\n\n\n\nExpected Benefits\n\nImproved user satisfaction and productivity through robust support mechanisms.\nEnhanced community engagement and knowledge sharing.\nEfficient troubleshooting and problem-solving through community forums.",
    "crumbs": [
      "Project Tools",
      "Use Cases"
    ]
  },
  {
    "objectID": "instantiation/usecases.html#use-case-3---data-visualization",
    "href": "instantiation/usecases.html#use-case-3---data-visualization",
    "title": "Use cases for APEx Project Tools",
    "section": "Use Case 3 - Data Visualization",
    "text": "Use Case 3 - Data Visualization\n\nUse Case Description\nEnable users to visualise EO data in meaningful and interactive ways. This use case supports both public and project-specific visualisations, helping users to interpret and communicate data insights effectively.\n\n\nKey Components\n\nGeospatial Explorer\nFunctional enhancements to provide interactive and real-time data visualisations, supporting both raster and tabular data visualisation.\nProject Portal\nIntegration of visualisations into project portals to showcase data insights.\nDocumentation Portal\nInteractive visualisations within documentation to explain data findings and methodologies.\n\n\n\nExpected Benefits\n\nEnhanced understanding and interpretation of EO data.\nImproved data communication and storytelling capabilities.\nIncreased engagement through interactive and visually appealing content.",
    "crumbs": [
      "Project Tools",
      "Use Cases"
    ]
  },
  {
    "objectID": "instantiation/usecases.html#use-case-4---data-analytics",
    "href": "instantiation/usecases.html#use-case-4---data-analytics",
    "title": "Use cases for APEx Project Tools",
    "section": "Use Case 4 - Data Analytics",
    "text": "Use Case 4 - Data Analytics\n\nDescription\nSupport advanced data analytics workflows, enabling users to analyse EO data efficiently. This use case focuses on providing powerful tools and environments for data processing and statistical analysis.\n\n\nKey Components\n\nInteractive Development Environment (IDE)\nEquipped with tools for data analysis, statistical computation, and machine learning, leveraging the capabilities of VS Code Server.\nUser Workspace\nSecure and scalable environments for conducting extensive data analysis.\nProduct Catalogue\nA STAC catalogue providing easy access to EO datasets and streamlining the data ingestion process.\nGeospatial Explorer\nIntegration with analytics tools to provide real-time visualisations of analytical results.\n\n\n\nExpected Benefits\n\nEnhanced analytical capabilities and data-driven insights.\nEfficient processing and analysis of large EO datasets.\nStreamlined workflows from data ingestion to analysis and visualisation.",
    "crumbs": [
      "Project Tools",
      "Use Cases"
    ]
  },
  {
    "objectID": "instantiation/usecases.html#use-case-5---data-processing",
    "href": "instantiation/usecases.html#use-case-5---data-processing",
    "title": "Use cases for APEx Project Tools",
    "section": "Use Case 5 - Data Processing",
    "text": "Use Case 5 - Data Processing\n\nDescription\nFacilitate the processing of EO data through scalable and customisable environments. This use case supports the execution of complex data processing workflows, enabling users to preprocess, transform, and analyse EO data effectively.\n\n\nKey Components\n\nUser Workspace\nProviding secure environments for executing data processing tasks on existing cloud-based processing platforms.\nInteractive Development Environment (IDE)\nSupporting data processing scripts and workflows, including pre-configured tools for EO tasks.\nProduct Catalogue\nStreamlined data ingestion and access to a wide range of EO datasets.\nGeospatial Explorer and Web Apps\nInteractively visualise results.\n\n\n\nExpected Benefits\n\nImproved efficiency in data processing workflows.\nScalable solutions for handling large volumes of EO data.\nEnhanced ability to preprocess and prepare data for analysis and visualisation.",
    "crumbs": [
      "Project Tools",
      "Use Cases"
    ]
  },
  {
    "objectID": "instantiation/forum.html",
    "href": "instantiation/forum.html",
    "title": "User Forum",
    "section": "",
    "text": "The User Forum enables projects to request their own user forum on demand, providing a critical tool for community building and user support. By offering a dedicated space for discussions, the forum allows the project, its stakeholders, and users to interact seamlessly, fostering a sense of community and collaboration. This interaction not only aids in addressing user concerns and questions but also encourages the sharing of knowledge and best practices among members.\nThe underlying technology for the forum service is Discourse, a widely used open-source forum software renowned for its robustness and flexibility. Discourse has been successfully implemented in various ESA projects, demonstrating its reliability and effectiveness. One of the standout features of Discourse is its comprehensive admin interface, which empowers projects to customise the user forum to meet their specific requirements. This customisation can include modifying the forum’s appearance, setting user permissions, and managing content. Moreover, Discourse supports a wide range of plugins that can significantly enhance the forum’s functionality, providing a versatile solution that can adapt to the evolving needs of any project.",
    "crumbs": [
      "Project Tools",
      "User Forum"
    ]
  },
  {
    "objectID": "instantiation/forum.html#overview",
    "href": "instantiation/forum.html#overview",
    "title": "User Forum",
    "section": "",
    "text": "The User Forum enables projects to request their own user forum on demand, providing a critical tool for community building and user support. By offering a dedicated space for discussions, the forum allows the project, its stakeholders, and users to interact seamlessly, fostering a sense of community and collaboration. This interaction not only aids in addressing user concerns and questions but also encourages the sharing of knowledge and best practices among members.\nThe underlying technology for the forum service is Discourse, a widely used open-source forum software renowned for its robustness and flexibility. Discourse has been successfully implemented in various ESA projects, demonstrating its reliability and effectiveness. One of the standout features of Discourse is its comprehensive admin interface, which empowers projects to customise the user forum to meet their specific requirements. This customisation can include modifying the forum’s appearance, setting user permissions, and managing content. Moreover, Discourse supports a wide range of plugins that can significantly enhance the forum’s functionality, providing a versatile solution that can adapt to the evolving needs of any project.",
    "crumbs": [
      "Project Tools",
      "User Forum"
    ]
  },
  {
    "objectID": "instantiation/forum.html#showcase-scenarios",
    "href": "instantiation/forum.html#showcase-scenarios",
    "title": "User Forum",
    "section": "Showcase Scenarios",
    "text": "Showcase Scenarios\nA user forum can support several project scenarios, including:\n\nKnowledge Sharing and Collaboration\nForums bring together experts, researchers, and enthusiasts to share knowledge and insights, fostering a collaborative environment. Users can post questions and receive answers from the community, helping to solve problems more efficiently.\nCommunity Building\nForums help build a sense of community among users who share a common interest in the project. Users can connect with others, potentially leading to additional collaborations and partnerships.\nFeedback and Improvement\nProject members can receive direct feedback from users, allowing for continuous improvement and refinement of the project tools and methodologies. Users can suggest new features or enhancements, ensuring the project evolves according to user needs.\nSupport and Training\nForums provide a platform for users to seek technical support and troubleshooting assistance. Users can participate in discussions to enhance their skills and knowledge.\nAwareness and Outreach\nForums can be used to disseminate information about project updates, events, and relevant news.\n\n\n\n\n\n\n\nStay Tuned\n\n\n\nAdditional information will be shared on this page as the project progresses.",
    "crumbs": [
      "Project Tools",
      "User Forum"
    ]
  },
  {
    "objectID": "eo_service_usage/openeo_usage.html",
    "href": "eo_service_usage/openeo_usage.html",
    "title": "openEO based services",
    "section": "",
    "text": "APEx services that follow the openEO standard are implemented as openEO processes.\nServices can be executed through the tools that are provided by the different processing platforms. For OpenEO based services, there is an online user interface, web editor, Client Libraries (JavaScript, Python, R) and API.\nThere are several ways to discover how a service can be executed. When publishing a service on the APEx, a service provider can choose to provide the following information in the service details:",
    "crumbs": [
      "On-demand EO services",
      "Using openEO service"
    ]
  },
  {
    "objectID": "eo_service_usage/openeo_usage.html#openeo-backends",
    "href": "eo_service_usage/openeo_usage.html#openeo-backends",
    "title": "openEO based services",
    "section": "openEO backends",
    "text": "openEO backends\nAn APEx openEO service is always associated with one or more openEO backends that have been validated to produce the correct results. So using one of these backends is the recommended approach. Thanks to openEO interoperability and standardization, it may certainly be possible that other backends can run the same service, but do make sure to validate the results in that case.\nTo ensure a smooth experience, backends need to be compliant with APEx guidelines. The currently known list of compliant backends is:\n\nCDSE openEO federation\nopenEO platform\n\nBoth are federations, which means that they in fact are backed by multiple openEO instances. This increases the convenience for the APEx user, avoiding to interact with a high number of backends.",
    "crumbs": [
      "On-demand EO services",
      "Using openEO service"
    ]
  },
  {
    "objectID": "eo_service_usage/openeo_usage.html#online-user-interface",
    "href": "eo_service_usage/openeo_usage.html#online-user-interface",
    "title": "openEO based services",
    "section": "Online user interface",
    "text": "Online user interface\nOpenEO provides an online user interface where users can execute services directly in a web browser. Through the graphical user interface, users can execute, link, and configure different services. More information on the usage of the online applications is presented in the table below.\n\n\n\nOpenEO\n\n\n\n\nAccess CDSE openEO federation\n\n\nAccess openEO platform\n\n\nDocumentation\n\n\n\n\n\n\n\n\n\nPro-tip: discovering API requests\n\n\n\nThese interface provide a great way to find out how to interact with an openEO backend at the level of the HTTP API. Just use the ‘developer tools’ of your favourite browser and inspect HTTP requests to the backend to find out the various options.",
    "crumbs": [
      "On-demand EO services",
      "Using openEO service"
    ]
  },
  {
    "objectID": "eo_service_usage/openeo_usage.html#client-libraries",
    "href": "eo_service_usage/openeo_usage.html#client-libraries",
    "title": "openEO based services",
    "section": "Client libraries",
    "text": "Client libraries\nOpenEO provides client libraries to support the creation and execution of JavaScript, Python and R services. The full client libraries documentation is available on the official OpenEO support pages:\n\nJavaScript\nPython\nR\n\nThe following example shows a code sample on how to execute a service through the OpenEO Python Client.\nimport openeo\n\n# Setup parameters\naoi = {\n    \"type\": \"Polygon\",\n    \"coordinates\": [\n        [\n            [\n                5.179324150085449,\n                51.2498689148547\n            ],\n            [\n                5.178744792938232,\n                51.24672597710759\n            ],\n            [\n                5.185289382934569,\n                51.24504696935156\n            ],\n            [\n                5.18676996231079,\n                51.245342479161295\n            ],\n            [\n                5.187370777130127,\n                51.24918393390799\n            ],\n            [\n                5.179324150085449,\n                51.2498689148547\n            ]\n        ]\n    ]\n}\ndate = '2020-06-01'\n\n# Setup connection with OpenEO\neoconn = openeo.connect(\"openeofed.dataspace.copernicus.eu\").authenticate_oidc()\n\n# Create a processing graph from the BIOMASS process using an active openEO connection\ntaskmap = eoconn.datacube_from_process(\"taskmap_generate\", namespace=\"https://github.com/ESA-APEx/apex_algorithms/raw/main/openeo_udp/worldcereal_inference.json\", bbox=aoi)\n\n# Execute the OpenEO request as a batch job\ntaskmap_job = taskmap.save_result(format='GTiff').send_job()\ntaskmap_job.start_and_wait().get_results()\nTo execute a service from the APEx through one of the OpenEO client libraries, it is important to use the datacube_from_process function. This accepts the ID and namespace of the service. Both are made available in the service description on the EOplaza. The full documentation on using the function is available on the official OpenEO documentation.\nIt is possible to combine the Python and R OpenEO libraries with the notebooks provided by PROBA-V MEP. Examples are available in the OpenEO GitHub repository.",
    "crumbs": [
      "On-demand EO services",
      "Using openEO service"
    ]
  },
  {
    "objectID": "eo_service_usage/openeo_usage.html#api",
    "href": "eo_service_usage/openeo_usage.html#api",
    "title": "openEO based services",
    "section": "API",
    "text": "API\nOpenEO provides a fully documented API for a more advanced way to integrate features in any existing application or workflow. The API can also be used to execute EOPlaza services. The documentation is available at:\n\n\n\nOpenEO\n\n\n\n\nhttps://openeo.org/documentation/1.0/developers/api/reference.html\n\n\n\nThe following example showcases how to use the OpenEO API to execute a synchronous request for the BioPAR service:\nPOST /openeo/1.2/result HTTP/1.1\nHost: openeocloud.vito.be\nContent-Type: application/json\nAuthorization: Bearer basic//basic.cHJvag==\nContent-Length: 4587\n{\n    \"process\": {\n        \"id\": \"biopar1\",\n        \"process_graph\": {\n            \"biopar1\": {\n                \"process_id\": \"biopar\",\n                \"namespace\": \"vito\",\n                \"arguments\": {\n                    \"bbox\": {\n                        \"west\": 5.15183687210083,\n                        \"east\": 5.153381824493408,\n                        \"south\": 51.18192559252128,\n                        \"north\": 51.18469636040683,\n                        \"crs\": \"EPSG:4326\"\n                    },\n                    \"time_range\": [\n                        \"2020-05-06\",\n                        \"2020-05-30\"\n                    ]\n                },\n                \"result\": true\n            }\n        }\n    }\n}",
    "crumbs": [
      "On-demand EO services",
      "Using openEO service"
    ]
  },
  {
    "objectID": "instantiation/geospatial_explorer.html",
    "href": "instantiation/geospatial_explorer.html",
    "title": "Geospatial Explorer",
    "section": "",
    "text": "The Geospatial Explorer will provide an interactive web front end that can be used for the display and visualisation of geospatial and tabular data ingested from web services following common interoperable protocols (e.g. OGC Standards, STAC, etc.). The Explorer will be data-driven, allowing administrators to define the configuration of the explorer in JSON (i.e. the data layers and functional operations possible for each layer). This configuration will determine how the user interface is rendered at run time and the resulting data and functionality that is exposed to the end user.\nTypical functions will include the ability to visualise EO data, derived products and associated vector layers (e.g. administrative boundaries), with control over layer ordering, transparency, product comparisons (split screen) and support for features such as cursor inspection, queries, distance measurements and visualisation of tabular data via integrated charts and graphs. The UI will also provide access to metadata records associated with the data that is rendered in the Explorer.\n\n\n\n\n\n\nStay Tuned\n\n\n\nAdditional information will be shared on this page as the project progresses.",
    "crumbs": [
      "Project Tools",
      "Geospatial Explorer"
    ]
  },
  {
    "objectID": "instantiation/geospatial_explorer.html#overview",
    "href": "instantiation/geospatial_explorer.html#overview",
    "title": "Geospatial Explorer",
    "section": "",
    "text": "The Geospatial Explorer will provide an interactive web front end that can be used for the display and visualisation of geospatial and tabular data ingested from web services following common interoperable protocols (e.g. OGC Standards, STAC, etc.). The Explorer will be data-driven, allowing administrators to define the configuration of the explorer in JSON (i.e. the data layers and functional operations possible for each layer). This configuration will determine how the user interface is rendered at run time and the resulting data and functionality that is exposed to the end user.\nTypical functions will include the ability to visualise EO data, derived products and associated vector layers (e.g. administrative boundaries), with control over layer ordering, transparency, product comparisons (split screen) and support for features such as cursor inspection, queries, distance measurements and visualisation of tabular data via integrated charts and graphs. The UI will also provide access to metadata records associated with the data that is rendered in the Explorer.\n\n\n\n\n\n\nStay Tuned\n\n\n\nAdditional information will be shared on this page as the project progresses.",
    "crumbs": [
      "Project Tools",
      "Geospatial Explorer"
    ]
  },
  {
    "objectID": "instantiation/user_workspace.html",
    "href": "instantiation/user_workspace.html",
    "title": "User Workspace",
    "section": "",
    "text": "The User Workspaces within the APEx Project Tools provide secure, personalised environments for individual users to perform a wide range of tasks, including development, data processing, visualisation, and analysis. These single-user environments are managed by JupyterHub and dynamically provisioned using Kubernetes, ensuring scalability, isolation, and ease of use. Key features and capabilities of the User Workspaces include:\n\nSecure, Isolated Environments\nEach user is provided with a dedicated namespace within the Kubernetes cluster, ensuring resource isolation and secure data management.\nCustomisable Workspaces\nWorkspaces can be tailored to user needs with specific configurations and tools, including pre-configured environments for tasks like data analysis and machine learning.\nSeamless Integration with APEx Tools and Services\nIntegration with other APEx tools, such as the Interactive Development Environment (IDE) and Product Catalogue, for enhanced functionality and data accessibility.\nRobust Data Management\nSecure storage and retrieval of data and integration with external data sources",
    "crumbs": [
      "Project Tools",
      "User Workspace"
    ]
  },
  {
    "objectID": "instantiation/user_workspace.html#overview",
    "href": "instantiation/user_workspace.html#overview",
    "title": "User Workspace",
    "section": "",
    "text": "The User Workspaces within the APEx Project Tools provide secure, personalised environments for individual users to perform a wide range of tasks, including development, data processing, visualisation, and analysis. These single-user environments are managed by JupyterHub and dynamically provisioned using Kubernetes, ensuring scalability, isolation, and ease of use. Key features and capabilities of the User Workspaces include:\n\nSecure, Isolated Environments\nEach user is provided with a dedicated namespace within the Kubernetes cluster, ensuring resource isolation and secure data management.\nCustomisable Workspaces\nWorkspaces can be tailored to user needs with specific configurations and tools, including pre-configured environments for tasks like data analysis and machine learning.\nSeamless Integration with APEx Tools and Services\nIntegration with other APEx tools, such as the Interactive Development Environment (IDE) and Product Catalogue, for enhanced functionality and data accessibility.\nRobust Data Management\nSecure storage and retrieval of data and integration with external data sources",
    "crumbs": [
      "Project Tools",
      "User Workspace"
    ]
  },
  {
    "objectID": "instantiation/user_workspace.html#showcase-scenarios",
    "href": "instantiation/user_workspace.html#showcase-scenarios",
    "title": "User Workspace",
    "section": "Showcase Scenarios",
    "text": "Showcase Scenarios\nThe User Workspaces support a variety of use cases, making them versatile tools for the EO community. Some typical scenarios include:\n\nDevelopment and Testing\nResearchers and developers can use the User Workspaces to develop and test new algorithms and models. For instance, a user might leverage the IDE integrated with JupyterHub to write and debug Python scripts for processing satellite imagery.\nData Analysis and Visualisation can perform exploratory data analysis and create visualisations using tools like JupyterLab or QGIS. For example, an analyst might use Jupyter notebooks to analyse climate data and visualise trends over time.\nEducational and Training Purposes\nThe User Workspaces can be used to create interactive tutorials and practical assignments that guide users through various aspects of APEx, from setting up workspaces and accessing data in the Product Catalogue to using the IDE for development tasks.\n\n\n\n\n\n\n\nStay Tuned\n\n\n\nAdditional information will be shared on this page as the project progresses.",
    "crumbs": [
      "Project Tools",
      "User Workspace"
    ]
  },
  {
    "objectID": "instantiation/project_portal.html",
    "href": "instantiation/project_portal.html",
    "title": "Project Portal",
    "section": "",
    "text": "The Project Portal will provide a solution designed to support the needs of projects by providing a project website upon request. The portal, structured on a predefined base template, will offer the flexibility to further customise the overall user experience. Beyond the initial theming options, the service incorporates a Drupal Content Management System (CMS), empowering project members to perform advanced customisations and efficiently manage portal content.\nThe instantiated project portal will have the option to work in synergy with other services offered within APEx. This collaborative approach enables projects to extend their portal by instantiating other APEx services, such as a STAC catalogue, a Geospatial Explorer, a User Forum, and more, and linking them to the main project portal, resulting in a dedicated, streamlined project ecosystem.",
    "crumbs": [
      "Project Tools",
      "Project Portal"
    ]
  },
  {
    "objectID": "instantiation/project_portal.html#overview",
    "href": "instantiation/project_portal.html#overview",
    "title": "Project Portal",
    "section": "",
    "text": "The Project Portal will provide a solution designed to support the needs of projects by providing a project website upon request. The portal, structured on a predefined base template, will offer the flexibility to further customise the overall user experience. Beyond the initial theming options, the service incorporates a Drupal Content Management System (CMS), empowering project members to perform advanced customisations and efficiently manage portal content.\nThe instantiated project portal will have the option to work in synergy with other services offered within APEx. This collaborative approach enables projects to extend their portal by instantiating other APEx services, such as a STAC catalogue, a Geospatial Explorer, a User Forum, and more, and linking them to the main project portal, resulting in a dedicated, streamlined project ecosystem.",
    "crumbs": [
      "Project Tools",
      "Project Portal"
    ]
  },
  {
    "objectID": "instantiation/project_portal.html#showcase-scenarios",
    "href": "instantiation/project_portal.html#showcase-scenarios",
    "title": "Project Portal",
    "section": "Showcase Scenarios",
    "text": "Showcase Scenarios\nThe project portal can bring a lot of value to projects, including:\n\nEfficient project dissemination and community engagement\nFacilitates the rapid and broad dissemination of project information, ensuring that updates, results, and important announcements reach the intended audience quickly and effectively.\nProfessional branding with customisation possibilities\nThe portal comes with polished, consistent branding out of the box, enhancing the project’s credibility and professionalism and fostering trust among stakeholders and the public. Additionally, project teams can tailor the portal’s appearance to align with specific branding guidelines or aesthetic preferences, creating a unique and engaging user experience that reflects the project’s identity.\nContent management and moderation\nThe portal’s robust content management capabilities enable teams to streamline the creation, moderation, and updating of content. Advanced features like content moderation workflows and version control ensure that all shared information is accurate, current, and meets our high-quality standards. The system supports various roles and permissions, including administrators and content creators, fostering a self-regulated environment. This empowerment allows project teams to manage their content and user access autonomously, thereby enhancing operational efficiency.",
    "crumbs": [
      "Project Tools",
      "Project Portal"
    ]
  },
  {
    "objectID": "instantiation/project_portal.html#what-does-apex-offer",
    "href": "instantiation/project_portal.html#what-does-apex-offer",
    "title": "Project Portal",
    "section": "What does APEx offer?",
    "text": "What does APEx offer?\nAPEx provides a default Drupal installation for your project, including a basic theme. APEx offers basic theming support, such as changing the color scheme and project logo. Upon creation of your project’s portal, an administrator account will be created, allowing you to invite other project members. Using the built-in content management system, your project is free to manage the content of the full website.\nIn addition to providing your project portal, APEx ensures optimal performance and availability through continuous monitoring. Automated maintenance tasks, such as updates and backups, are conducted to minimize downtime and maintain data integrity. This approach establishes a robust and dependable environment for users.\nBy April 2026, the APEx Project Portal service will be integrated into the ESA Network of Resources (NoR). Until these services are onboarded, projects can reach out to the APEx team to explore how we can support their hosting needs. APEx also manages DNS setup, ensuring your project’s website is accessible on the internet without any additional technical burden.\nThis comprehensive service allows projects to focus on content creation and management while APEx takes care of the technical aspects, making it a valuable and efficient solution for project dissemination needs.",
    "crumbs": [
      "Project Tools",
      "Project Portal"
    ]
  },
  {
    "objectID": "instantiation/project_portal.html#what-software-is-used-by-apex",
    "href": "instantiation/project_portal.html#what-software-is-used-by-apex",
    "title": "Project Portal",
    "section": "What software is used by APEx?",
    "text": "What software is used by APEx?\nFor the project portal service, APEx uses the Drupal framework. Drupal is an open-source platform for creating and managing web content. In addition to its extensive content management features, it also allows for detailed customization.\n\n\n\n\n\n\nStay Tuned\n\n\n\nAdditional information will be shared on this page as the project progresses.",
    "crumbs": [
      "Project Tools",
      "Project Portal"
    ]
  },
  {
    "objectID": "instantiation/ide.html",
    "href": "instantiation/ide.html",
    "title": "Interactive Development Environment",
    "section": "",
    "text": "The Interactive Development Environment (IDE) within the APEx Project Tools primarily leverages the power of the Code Server solution (VS Code in the Cloud), as well as the JupyterLab solution, both made available through the User Workspace.\nThese IDE solutions allow developers to maintain a familiar environment and rich feature set while benefiting from the power and resources of server-side computing. This is particularly advantageous for those working on resource-intensive tasks or needing access to a consistent development environment from various locations and devices.\nThe server-based nature ensures that developers are not constrained by their local machine’s hardware capabilities, allowing them to harness the computational power of remote servers.\nTailored specifically for EO tasks, this environment furnishes developers with an array of tools and libraries fine-tuned for programming languages and productivity plugins or extensions.",
    "crumbs": [
      "Project Tools",
      "Interactive Development Environment"
    ]
  },
  {
    "objectID": "instantiation/ide.html#overview",
    "href": "instantiation/ide.html#overview",
    "title": "Interactive Development Environment",
    "section": "",
    "text": "The Interactive Development Environment (IDE) within the APEx Project Tools primarily leverages the power of the Code Server solution (VS Code in the Cloud), as well as the JupyterLab solution, both made available through the User Workspace.\nThese IDE solutions allow developers to maintain a familiar environment and rich feature set while benefiting from the power and resources of server-side computing. This is particularly advantageous for those working on resource-intensive tasks or needing access to a consistent development environment from various locations and devices.\nThe server-based nature ensures that developers are not constrained by their local machine’s hardware capabilities, allowing them to harness the computational power of remote servers.\nTailored specifically for EO tasks, this environment furnishes developers with an array of tools and libraries fine-tuned for programming languages and productivity plugins or extensions.",
    "crumbs": [
      "Project Tools",
      "Interactive Development Environment"
    ]
  },
  {
    "objectID": "instantiation/ide.html#code-server",
    "href": "instantiation/ide.html#code-server",
    "title": "Interactive Development Environment",
    "section": "Code Server",
    "text": "Code Server\nThe Code Server setup encapsulates all the capabilities of Microsoft’s popular VS Code editor and extends them to be run and accessed on a remote server. Beyond the core functionality of its desktop counterpart, the Code Server IDE offers additional features tailored for remote development, such as integrated Git support, debugging tools, and a plethora of extensions (for Code Server, from the VS Code Marketplace and for JupyterLab, from the PyPI.org registry). It provides support for programming languages like Python, R, and Java. Key libraries such as SNAP and GDAL are integrated, providing robust capabilities for EO data discovery, access, processing, and analytical needs.\nIt seamlessly adapts to containerised environments, enabling developers to create, test, and deploy applications within isolated, replicable, and consistent environments, ensuring consistent behaviour across development, staging, and production phases.",
    "crumbs": [
      "Project Tools",
      "Interactive Development Environment"
    ]
  },
  {
    "objectID": "instantiation/ide.html#jupyterlab",
    "href": "instantiation/ide.html#jupyterlab",
    "title": "Interactive Development Environment",
    "section": "JupyterLab",
    "text": "JupyterLab\nThe JupyterLab setup encapsulates a web-based interactive development environment for Jupyter notebooks, code, and data. It is the user interface for Project Jupyter, offering a flexible user interface and more features than the classic notebook UI. It is a web application providing a development environment in which processing algorithms and services can be developed, tested, and debugged. JupyterLab supports execution environments (called “kernels”) in several dozen languages, including Julia, R, Haskell, Ruby, and Python (via the IPython kernel). It seamlessly adapts to containerised environments, enabling developers to create, test, and deploy applications within isolated, replicable, and consistent environments, ensuring consistent behaviour across development, staging, and production phases.",
    "crumbs": [
      "Project Tools",
      "Interactive Development Environment"
    ]
  },
  {
    "objectID": "instantiation/ide.html#showcase-scenarios",
    "href": "instantiation/ide.html#showcase-scenarios",
    "title": "Interactive Development Environment",
    "section": "Showcase Scenarios",
    "text": "Showcase Scenarios\nThe Interactive Development Environment supports a variety of use cases, making it an essential tool for developers, researchers, and data scientists within the EO community. Some typical scenarios include:\n\nAlgorithm Development and Testing\nResearchers and developers can write, test, and debug new algorithms for processing satellite imagery or other EO data. For instance, a user might develop a script to detect deforestation using multi-temporal satellite images. To support the productivity of developers, both the Code Server solution and the JupyterLab solution use a mechanism of extensions. Code Server extensions are available from the Open VSX Registry.\nCollaborative Projects\nTeams can work collaboratively on projects, sharing code and resources in real-time. A group of data scientists might collaboratively develop a machine-learning model to predict crop yields based on various data inputs.\nData Science Notebooks\nThe term “Notebook” usually covers two different concepts, either the user-facing application to edit code and text (this originates from Project Jupyter‘s software product initially branded “Jupyter Notebooks”, nowadays “JupyterLab”), or more commonly the underlying file format which is interoperable across many IDE software solutions. Both solutions proposed for APEx, Code Server, and JupyterLab have multi-kernel language support (python, R, Ruby, …). Code Server supports local development of Jupyter Notebooks: the Jupyter extension for VS Code is a very popular extension in the VS Code Marketplace. JupyterLab is a very popular tool within the Open Science community for working with notebooks, with its native support for data science, data visualisation and reproducible environments.\nData Processing Pipelines\nUsers can develop and test data processing pipelines that automate the ingestion, processing, and analysis of large EO datasets. An example use case could be to create a workflow setting up a pipeline to preprocess satellite images and extract relevant features for further analysis.\n\n\n\n\n\n\n\nStay Tuned\n\n\n\nAdditional information will be shared on this page as the project progresses.",
    "crumbs": [
      "Project Tools",
      "Interactive Development Environment"
    ]
  },
  {
    "objectID": "interoperability/businessmodel.html",
    "href": "interoperability/businessmodel.html",
    "title": "Federated Business Model",
    "section": "",
    "text": "This section outlines the commercial aspects of algorithm hosting within APEx and will be further analysed and documented by the APEx project.",
    "crumbs": [
      "Interoperability and Compliance Guidelines",
      "Federated Business Model"
    ]
  },
  {
    "objectID": "interoperability/businessmodel.html#algorithm-hosting-platforms",
    "href": "interoperability/businessmodel.html#algorithm-hosting-platforms",
    "title": "Federated Business Model",
    "section": "Algorithm Hosting Platforms",
    "text": "Algorithm Hosting Platforms\nTable 1 provides an overview of the requirements and guidelines for the algorithm hosting platforms to support commercial algorithm hosting through ESA’s Network of Resources.\n\n\n\nTable 1: Requirements for supporting commercial algorithm hosting for algorithm hosting platforms\n\n\n\n\n\n\n\n\n\n\nID\nRequirement\nDescription\n\n\n\n\nBM-REQ-01\nThe algorithm hosting platform shall allow a fixed price per area unit.\nThis requirement ensures transparent pricing for the user, for example expressed in platform credits per km², hectare or by number of products.\n\n\nBM-REQ-02\nThe operator of the algorithm hosting platform should support registering the algorithm in the ESA network of resources, with a cost per km², hectare or by number of products.\nThis does not apply if the service can be invoked by the user ‘for free’, so costs are compensated by a different mechanism.",
    "crumbs": [
      "Interoperability and Compliance Guidelines",
      "Federated Business Model"
    ]
  },
  {
    "objectID": "interoperability/definitions.html",
    "href": "interoperability/definitions.html",
    "title": "Definitions & Actors",
    "section": "",
    "text": "This section introduces key terms and their meaning in the APEx context, which are crucial for building a common understanding of the complex ecosystem in which APEx operates. Along with the general definitions, Figure 1 visually demonstrates the interconnections between these concepts.\nflowchart TD\n    EO_PROJECT(\"EO Project\")\n    ALGORITHM(\"Application Algorithm Definition\")\n    ALGORITHM_GUIDELINES(\"Algorithm Provider Guidelines\")\n    OPENEO_OGC(\"openEO API or OGC Processes API\")\n    LAST_MILE(\"Last Mile Application\")\n    HOSTING_PLATFORM(\"Algorithm Hosting Platform\")\n    OPERATOR(\"Platform Operator\")\n    PLATFORM_GUIDELINES(\"Algorithm Hosting Platform Guidelines\")\n    CATALOGUE(\"APEx Algorithm Services Catalogue\")\n    APEX(\"APEx Consortium\")\n    \n\n    EO_PROJECT --&gt; | owns | ALGORITHM\n    ALGORITHM --&gt; | adheres to | ALGORITHM_GUIDELINES\n    ALGORITHM --&gt; | hosted on | OPENEO_OGC\n    OPENEO_OGC --&gt; | deployed on | HOSTING_PLATFORM\n    OPERATOR --&gt; | operated by | HOSTING_PLATFORM\n    HOSTING_PLATFORM --&gt; | adheres to | PLATFORM_GUIDELINES\n    ALGORITHM --&gt; | registered in | CATALOGUE\n    APEX ---&gt; | curates | CATALOGUE\n    OPENEO_OGC --&gt; | integrated in | LAST_MILE\n    LAST_MILE --&gt; | has access to | HOSTING_PLATFORM\n\n\n\n\nFigure 1: Concepts and relationships.",
    "crumbs": [
      "Interoperability and Compliance Guidelines",
      "Definitions & Actors"
    ]
  },
  {
    "objectID": "interoperability/definitions.html#concepts",
    "href": "interoperability/definitions.html#concepts",
    "title": "Definitions & Actors",
    "section": "Concepts",
    "text": "Concepts\n\nEO Algorithms, Workflows and Applications\nIn the context of APEx, the term algorithm refers to a specific piece of EO software that performs specific processing or data analytics tasks. EO Algorithms can vary substantially in size and complexity, for example, some only perform one specific task (e.g. cloud masking in optical imagery), while others generate value-added products based on complex workflows that integrate several individual algorithms. A related term is EO Application, which commonly refers to a comprehensive workflow implementation that commonly also utilises non-EO (geospatial) data sets and a variety of data analytics procedures to perform an application-specific processing task.\nFor the sake of simplicity in the context of APEx, we refer to these simply as (EO) “algorithms”.\n\n\nAlgorithm Service Implementation\nThe algorithm definition refers to a representation of the algorithm modules and interfaces that can be exposed by an API and/or processing platform. Typically, it includes a general description of the algorithm along with detailed information on its parameters, expected output, scientific method, and an overview of the different steps executed within the algorithm. Examples of algorithm definitions include openEO’s User Defined Processes (UDP) [1] and OGC Application Package [2], using the Common Workflow Language (CWL) [3].\n\n\nAlgorithm Services Catalogue\nThe APEx Algorithm Services Catalogue is a central register of algorithm definitions and the corresponding algorithm service instances that can be executed on APEx-compliant algorithm hosting platforms. Curated by APEx, the catalogue relies on automated checks to ensure that advertised algorithms service instances are available and functional. Whenever a malfunction is detected, this is reported to ESA and the EO project consortium, allowing them to determine a proper course of action.\n\n\nHosted Algorithm\nTo increase uptake and interoperability, APEx aims to enable the execution of algorithms via standardised web service APIs. This transitions algorithms from being rather arbitrary pieces of software with potentially complex requirements, in terms of execution environment, usage, inputs, …, into on-demand services that can readily be invoked by stakeholders. This transformation primarily involves converting a given algorithm into an APEx-compliant algorithm definition and making it available as a service on an algorithm hosting platform. The transition process into a hosted on-demand algorithm service is supported by the APEx Algorithm Services.\nAn important boundary condition for hosted algorithms is that they can be executed at a predictable cost for a given set of inputs. This predictability allows a service user to accurately estimate and determine the cost associated with the execution of the final deployed service instance.\n\n\nAlgorithm Hosting Platform\nAn EO algorithm hosting platform enables the execution of a standardised algorithm, represented by an algorithm definition. In APEx, an algorithm hosting platform specifically refers to platforms that support the openEO UDP and/or OGC Application Package Algorithm description standards. These platforms also enable algorithms to be executed by the openEO API or through OGC API Processes. For APEx, these platforms are considered existing providers available through ESA’s Network of Resources (NoR). Examples of such algorithm hosting platforms are the Copernicus Data Space Ecosystem for openEO or Ellip for OGC Application Packages.\nIt is important to note that APEx itself is not an algorithm hosting platform; rather, it promotes the reuse of existing platforms. A key property of algorithm hosting platforms is their long-term sustainability beyond the lifetime of a typical EO project. This ensures that algorithms can still be executed after the project ends.\nThe algorithm hosting platform has an important responsibility to ensure the continued availability of hosted algorithms. This responsibility is detailed in the requirements below, highlighting that the selection of the platform affects properties such as cost, performance, stability, and the amount of computing resources available to run the algorithm. Compliance with these requirements does not necessarily imply a high overall quality level across all aspects, ensuring that EO projects retain a sufficient degree of freedom in selecting their preferred platform.",
    "crumbs": [
      "Interoperability and Compliance Guidelines",
      "Definitions & Actors"
    ]
  },
  {
    "objectID": "interoperability/definitions.html#actors",
    "href": "interoperability/definitions.html#actors",
    "title": "Definitions & Actors",
    "section": "Actors",
    "text": "Actors\n\nEO Project & Algorithm PI\nAn EO project refers to the consortium that is responsible for building the EO algorithm and is also referred to as Algorithm PI (principal investigator). In certain ESA EOP procurement and ITTs, there is now a requirement included to comply with APEx Interoperability and Compliance Guidelines. When compliance is required, the consortium can utilise various services offered by APEx. Specifically, the APEx Algorithm Services aim to support the enhancement of algorithms on a technical and software level and facilitate the transition to hosted algorithms that can be included in the APEx Algorithm Services Catalogue.\nESA EO projects that do not have an explicit compliance requirement are also eligible to receive support. The APEx support can boost project impact, so projects are encouraged to inquire with their ESA technical officer about the possibilities.\nIt is important to note that during the execution of the project, the project retains full responsibility for the final quality of the algorithms and workflows.\n\n\nAPEx Consortium\nThe APEx team, composed of industry experts, operates the various services provided by APEx. To maximise the reuse of existing resources, the team leverages service offerings within [ESA’s Network of Resources (NoR)] (https://nor-discover.org/), drawing on the extensive ecosystem provided by the EO industry.\nAlthough members of the APEx consortium are involved in various EO services registered in NoR, APEx itself is not a new EO platform. Instead, it focuses on enabling interoperability among existing platforms. As a result, APEx remains open to integrating additional platforms, provided they meet the compliance requirements of the Algorithm Hosting Environment, as specified below.\n\n\nPlatform Operator\nThe platform operator plays a crucial role in managing and running the algorithm hosting platform. Their primary responsibility is to oversee the infrastructure that supports the execution of various algorithms. This includes providing the necessary computational resources to ensure the smooth and efficient operation of the platform. In addition to maintaining the technical environment, the platform operator offers user support to assist users in navigating and utilising the platform effectively. They are also accountable for meeting the SLAs established for the platform, ensuring that performance and reliability standards are consistently met. While the operator may be a partner outside of the APEx consortium, their role is integral to the success of APEx, focusing on both operational management and user satisfaction.\n\n\nLast Mile Applications\nA ‘last mile’ application, as seen from the EO perspective, bridges the final gap between the user at the end of the value chain, and the web service APIs that offer EO-derived information. This could, for instance, be the integration of parcel statistics into the field management software of a farmer. In the ESA context, Green Transition Information Factories can be considered examples of last mile applications.\nTypically, each user group and domain will have its own set of purpose-built tools. The IT integrator or vendor building these tools will retrieve EO algorithm results via web service APIs based on open standards. These results can then be further processed as needed, depending on the application. Note that these applications are not necessarily EO-centric or even geospatial, but could be using an EO algorithm as a small part or in the background.\nThe APEx Algorithm Services Catalog acts as a discovery tool for last mile application builders. It shows various technical parameters, the cost and potential limitations for specific use cases. This speeds up the discovery and selection process. Application builders will be required to create an account on the hosting platforms and ensure the necessary funds are available, if they want to test or use the API.",
    "crumbs": [
      "Interoperability and Compliance Guidelines",
      "Definitions & Actors"
    ]
  },
  {
    "objectID": "interoperability/algohosting.html",
    "href": "interoperability/algohosting.html",
    "title": "Algorithm Developer and Provider Guidelines",
    "section": "",
    "text": "Table 1 outlines the interoperability prerequisites required for algorithm providers, such as EO projects, to host their workflows and algorithms within APEx. By satisfying these requirements, APEx guarantees the successful integration of workflows and algorithms and ensures reusability within the broader EO community. It is important to highlight that the majority of these requirements apply to EO projects that build an on-demand service to be exposed via an HTTP-based API.\nIn terms of creating on-demand services, APEx currently supports two main interface standards: openEO [1] or OGC API Processes [2], as described in section 3. This selection should support almost any possible on-demand service.\nFinally, note that APEx also provides support to projects that need to fulfil these requirements. This support includes implementing the guidelines in this document, offering a framework to run automated tests, and providing packages to help improve the performance of algorithms. These are referred to as APEx Algorithm Services.\nIn general, the aim is to simplify the process of building high-quality on-demand services rather than to add complexity.\n\n\n\nTable 1: Interoperability requirements for algorithm providers\n\n\n\n\n\n\n\n\n\n\nID\nRequirement\nDescription\n\n\n\n\nPROV-REQ-01\nEO project results with respect to raster data, shall be delivered as cloud-native datasets.\nWhere possible, cloud optimized GeoTIFF [3] is preferred. For more complex datasets, CF-Compliant netCDF [4] is a good alternative. Use of the still evolving GeoZarr [5] format requires confirmation by APEx and may result in future incompatibility if the selected flavour is not standardised eventually.\n\n\nPROV-REQ-02\nEO project results with respect to vector data, shall be delivered as cloud-native datasets.\nSmall datasets can use GeoJSON [6], GeoParquet [7] is recommended for larger datasets.\n\n\nPROV-REQ-03\nEO project results with respect to data should be accompanied with metadata in a STAC [8] format, including applicable STAC extensions.\nThe specific STAC profiles to be applied will be defined throughout the project.\n\n\nPROV-REQ-04\nAlgorithms and applications shall include documentation that addresses the scientific and technical limitations of the algorithm.\nFor instance, the ability of the algorithm to generalize across space and time, input data requirements, error margins on predicted physical quantities.\n\n\nPROV-REQ-05\nThe algorithms shall be provided according to one of these options:\n\nProcess graphs encapsulated in an openEO User Defined Process (UDP) [9], as a JSON format, defined using the openEO community standard and using the openEO API [1].\nApplications written in a variety of coding languages (e.g., Python, R, Java, C++, C#, shell scripts) packaged according to the OGC Application Package Best Practice [10] as a CWL file [11].\n\nThis ensures that the algorithm can be hosted on one of the APEx-compliant algorithm hosting platforms. The APEx documentation will provide clear guidance and samples demonstrating these two options.\n\n\nPROV-REQ-06\nFor algorithms to be hosted, the algorithm provider shall demonstrate the code quality via static code analysis tools.\nFor Python code, tools such as pylint can be used for static code analysis.\n\n\nPROV-REQ-07\nFor algorithms to be hosted, validated outputs for a given set of input parameters shall be made available by the algorithm provider, preferably on a small area that still allows for relevant testing.\nThis allows to validate the correct functioning of the algorithm as changes are made.\n\n\nPROV-REQ-08\nFor algorithms to be hosted, automated tests shall be provided that compare the current output of the software against a persisted sample, for a representative area of interest.\nThese tests enable APEx to automate the periodic validation of algorithms, ensuring they remain functionally available even after the project has finished.\n\n\nPROV-REQ-09\nFor algorithms to be hosted, a versioning scheme shall be defined, preferably following a standardized approach such as https://semver.org.\nThe versioning scheme provides a clear framework for communicating algorithm updates to users. By adopting standard practices, projects can highlight breaking changes, ensuring that users have accurate expectations and can adapt accordingly.\n\n\nPROV-REQ-10\nFor algorithms to be hosted, the procedure for releasing new versions should be clearly documented.\nClear documentation ensures that updates and new versions of algorithms are consistently and correctly released, reducing errors and providing transparency for users who rely on the latest features and fixes. It also helps maintain version control, which is crucial for reproducibility and compliance.\n\n\nPROV-REQ-11\nFor open source software developed within the project, a changelog should be maintained by the project.\nThis outlines significant changes between versions, providing important information for users of your algorithm and the APEx consortium. These explanations help clarify any differences in outcomes or performance that could impact automated testing.\n\n\nPROV-REQ-12\nNon-code dependencies such as custom datasets or machine learning models shall either be packaged with the software or be clearly listed as external dependency.\nThis approach prevents issues related to missing dependencies and ensuring users can easily set up the environment for proper execution. It also promotes transparency and simplifies the deployment process.\n\n\nPROV-REQ-13\nAlgorithms shall expose a list of well-documented parameters, with examples showing valid combinations of parameters.\nGood documentation improves the usability of the algorithms by providing users with clear guidance on how to configure them correctly. Well-documented parameters and examples reduce the risk of incorrect usage.\n\n\nPROV-REQ-14\nAlgorithms shall clearly list software library dependencies, separated into testing, development, and minimal set of runtime dependencies. Supported versions or version ranges shall be indicated.\nBy clearly listing and categorizing dependencies, users can quickly set up the necessary environment, avoid conflicts, and ensure the algorithm functions as intended across different scenarios.\n\n\nPROV-REQ-15\nRuntime dependencies shall be minimized as much as possible.\nFor instance, libraries required for training a model should not be included in a version for inference.\n\n\nPROV-REQ-16\nCode should be written in a cross-platform manner, supporting at least Linux.\nThe support for Linux is considered crucial to enable the deployment of the code on a cloud-based environment.\n\n\nPROV-REQ-17\nExecutables shall offer at least one choice of a non-interactive command line interface, or an API for integration into a larger codebase.\nA non-interactive command line interface or API enables seamless automation and integration with other components and services.\n\n\nPROV-REQ-18\nAlgorithms shall be associated with and tested on at least one APEx compliant hosting platform.\nTesting and deployment on an APEx-compliant platform guarantees that the algorithm performs correctly within the intended environment and allows the algorithm to be executed as an on-demand service.",
    "crumbs": [
      "Interoperability and Compliance Guidelines",
      "Algorithm Developer and Provider Guidelines"
    ]
  },
  {
    "objectID": "interoperability/algohosting.html#requirements",
    "href": "interoperability/algohosting.html#requirements",
    "title": "Algorithm Developer and Provider Guidelines",
    "section": "",
    "text": "Table 1 outlines the interoperability prerequisites required for algorithm providers, such as EO projects, to host their workflows and algorithms within APEx. By satisfying these requirements, APEx guarantees the successful integration of workflows and algorithms and ensures reusability within the broader EO community. It is important to highlight that the majority of these requirements apply to EO projects that build an on-demand service to be exposed via an HTTP-based API.\nIn terms of creating on-demand services, APEx currently supports two main interface standards: openEO [1] or OGC API Processes [2], as described in section 3. This selection should support almost any possible on-demand service.\nFinally, note that APEx also provides support to projects that need to fulfil these requirements. This support includes implementing the guidelines in this document, offering a framework to run automated tests, and providing packages to help improve the performance of algorithms. These are referred to as APEx Algorithm Services.\nIn general, the aim is to simplify the process of building high-quality on-demand services rather than to add complexity.\n\n\n\nTable 1: Interoperability requirements for algorithm providers\n\n\n\n\n\n\n\n\n\n\nID\nRequirement\nDescription\n\n\n\n\nPROV-REQ-01\nEO project results with respect to raster data, shall be delivered as cloud-native datasets.\nWhere possible, cloud optimized GeoTIFF [3] is preferred. For more complex datasets, CF-Compliant netCDF [4] is a good alternative. Use of the still evolving GeoZarr [5] format requires confirmation by APEx and may result in future incompatibility if the selected flavour is not standardised eventually.\n\n\nPROV-REQ-02\nEO project results with respect to vector data, shall be delivered as cloud-native datasets.\nSmall datasets can use GeoJSON [6], GeoParquet [7] is recommended for larger datasets.\n\n\nPROV-REQ-03\nEO project results with respect to data should be accompanied with metadata in a STAC [8] format, including applicable STAC extensions.\nThe specific STAC profiles to be applied will be defined throughout the project.\n\n\nPROV-REQ-04\nAlgorithms and applications shall include documentation that addresses the scientific and technical limitations of the algorithm.\nFor instance, the ability of the algorithm to generalize across space and time, input data requirements, error margins on predicted physical quantities.\n\n\nPROV-REQ-05\nThe algorithms shall be provided according to one of these options:\n\nProcess graphs encapsulated in an openEO User Defined Process (UDP) [9], as a JSON format, defined using the openEO community standard and using the openEO API [1].\nApplications written in a variety of coding languages (e.g., Python, R, Java, C++, C#, shell scripts) packaged according to the OGC Application Package Best Practice [10] as a CWL file [11].\n\nThis ensures that the algorithm can be hosted on one of the APEx-compliant algorithm hosting platforms. The APEx documentation will provide clear guidance and samples demonstrating these two options.\n\n\nPROV-REQ-06\nFor algorithms to be hosted, the algorithm provider shall demonstrate the code quality via static code analysis tools.\nFor Python code, tools such as pylint can be used for static code analysis.\n\n\nPROV-REQ-07\nFor algorithms to be hosted, validated outputs for a given set of input parameters shall be made available by the algorithm provider, preferably on a small area that still allows for relevant testing.\nThis allows to validate the correct functioning of the algorithm as changes are made.\n\n\nPROV-REQ-08\nFor algorithms to be hosted, automated tests shall be provided that compare the current output of the software against a persisted sample, for a representative area of interest.\nThese tests enable APEx to automate the periodic validation of algorithms, ensuring they remain functionally available even after the project has finished.\n\n\nPROV-REQ-09\nFor algorithms to be hosted, a versioning scheme shall be defined, preferably following a standardized approach such as https://semver.org.\nThe versioning scheme provides a clear framework for communicating algorithm updates to users. By adopting standard practices, projects can highlight breaking changes, ensuring that users have accurate expectations and can adapt accordingly.\n\n\nPROV-REQ-10\nFor algorithms to be hosted, the procedure for releasing new versions should be clearly documented.\nClear documentation ensures that updates and new versions of algorithms are consistently and correctly released, reducing errors and providing transparency for users who rely on the latest features and fixes. It also helps maintain version control, which is crucial for reproducibility and compliance.\n\n\nPROV-REQ-11\nFor open source software developed within the project, a changelog should be maintained by the project.\nThis outlines significant changes between versions, providing important information for users of your algorithm and the APEx consortium. These explanations help clarify any differences in outcomes or performance that could impact automated testing.\n\n\nPROV-REQ-12\nNon-code dependencies such as custom datasets or machine learning models shall either be packaged with the software or be clearly listed as external dependency.\nThis approach prevents issues related to missing dependencies and ensuring users can easily set up the environment for proper execution. It also promotes transparency and simplifies the deployment process.\n\n\nPROV-REQ-13\nAlgorithms shall expose a list of well-documented parameters, with examples showing valid combinations of parameters.\nGood documentation improves the usability of the algorithms by providing users with clear guidance on how to configure them correctly. Well-documented parameters and examples reduce the risk of incorrect usage.\n\n\nPROV-REQ-14\nAlgorithms shall clearly list software library dependencies, separated into testing, development, and minimal set of runtime dependencies. Supported versions or version ranges shall be indicated.\nBy clearly listing and categorizing dependencies, users can quickly set up the necessary environment, avoid conflicts, and ensure the algorithm functions as intended across different scenarios.\n\n\nPROV-REQ-15\nRuntime dependencies shall be minimized as much as possible.\nFor instance, libraries required for training a model should not be included in a version for inference.\n\n\nPROV-REQ-16\nCode should be written in a cross-platform manner, supporting at least Linux.\nThe support for Linux is considered crucial to enable the deployment of the code on a cloud-based environment.\n\n\nPROV-REQ-17\nExecutables shall offer at least one choice of a non-interactive command line interface, or an API for integration into a larger codebase.\nA non-interactive command line interface or API enables seamless automation and integration with other components and services.\n\n\nPROV-REQ-18\nAlgorithms shall be associated with and tested on at least one APEx compliant hosting platform.\nTesting and deployment on an APEx-compliant platform guarantees that the algorithm performs correctly within the intended environment and allows the algorithm to be executed as an on-demand service.",
    "crumbs": [
      "Interoperability and Compliance Guidelines",
      "Algorithm Developer and Provider Guidelines"
    ]
  },
  {
    "objectID": "interoperability/algohosting.html#best-practices",
    "href": "interoperability/algohosting.html#best-practices",
    "title": "Algorithm Developer and Provider Guidelines",
    "section": "Best Practices",
    "text": "Best Practices\nThe following sections provide best practice guidelines for developing APEx-compliant algorithms. While these guidelines are not mandatory, adhering to them will enhance the integration process and improve the overall experience of using the algorithm.\n\nParameter naming & typing\nAPEx proposes to standardise openEO UDP [9] and CWL [11] parameter names and types that are exposed to the user. This is best illustrated by an example: parameters such as bounding_box, bbox, aoi, and spatial_extent likely refer to the same concept. However, without common conventions, algorithms might randomly select one of these variants, complicating the usability of the eventual algorithm library.\nAt the time of writing, the actual conventions have not yet been defined. This becomes relevant when the first algorithms reach a state where they can be published with a fixed interface. This best practice mostly targets new developments that do not have an existing user base or API.",
    "crumbs": [
      "Interoperability and Compliance Guidelines",
      "Algorithm Developer and Provider Guidelines"
    ]
  },
  {
    "objectID": "interoperability/algohosting.html#licensing-requirements",
    "href": "interoperability/algohosting.html#licensing-requirements",
    "title": "Algorithm Developer and Provider Guidelines",
    "section": "Licensing requirements",
    "text": "Licensing requirements\nFor algorithms to be hosted and curated, APEx requires the ability to execute the algorithms as on-demand services on an APEx-compliant algorithm hosting platform. This is straightforward for fully open-source algorithms, such as those licensed under the Apache 2.0 license. However, for algorithms with more restrictive licenses or those dependent on artefacts like trained machine learning models, the algorithm provider must be able to license APEx to execute the service without incurring additional costs beyond the normal resource usage. Without such a license, the automated benchmarking and testing provided by APEx may need to be disabled for the service in question.",
    "crumbs": [
      "Interoperability and Compliance Guidelines",
      "Algorithm Developer and Provider Guidelines"
    ]
  },
  {
    "objectID": "interoperability/references/index.html",
    "href": "interoperability/references/index.html",
    "title": "Normative References",
    "section": "",
    "text": "The documents referenced in the following chapters are cited in such a way that some or all of their content supports the requirements of this document. For dated references, only the edition cited applies. For undated references, the latest edition of the referenced document, including any amendments, applies.\n\n\n1. OpenEO User Defined Processes\n\n\n2. OGC Best Practice for Earth Observation Application Package\n\n\n3. Commonwl.org: Common Workflow Language Specifications\n\n\n4. OGC API — Processes — Part 1: Core Standard, 2021\n\n\n5. openEO\n\n\n6. OGC Cloud Optimized GeoTIFF\n\n\n7. OGC CF-netCDF\n\n\n8. Geozarr Specification\n\n\n9. GeoJSON\n\n\n10. GeoParquet 1.1.0\n\n\n11. STAC Specification\n\n\n12. OpenEO API 1.2.0\n\n\n13. OpenEO API Profiles\n\n\n14. OpenEO Processes Profiles\n\n\n15. OpenEO L1B Minimal Batch Jobs\n\n\n16. OpenEO Remote UDP extension\n\n\n17. WMS 1.3.0\n\n\n18. WMTS 1.0.0\n\n\n19. WFS 2.0\n\n\n20. OGC API Features 1.0\n\n\n21. STAC projection extension"
  },
  {
    "objectID": "guides/udp_writer_guide.html",
    "href": "guides/udp_writer_guide.html",
    "title": "Guide for writing openEO User Defined Processes",
    "section": "",
    "text": "User defined processes (UDPs) are one out of two standardised options that APEx offers to publish algorithms as a service. This guide gives some concrete steps and guidelines to ensure that your UDP works well for your users. These guidelines are written with APEx in mind, but can also serve as a general guide for openEO UDPs. Where needed, recommendations and choices are made to increase uniformity across theAPEx Algorithm Services Catalogue.\nFor more background on UDP’s, or a basic tutorial on creating them, the open source Python client provides a good starting point.\nKeep in mind that APEx offers Algorithm Porting and Algorithm Onboarding support to help you with transforming your algorithm into an openEO UDP and onboarding it onto an APEx-compliant hosting platform.\n\n\nThe best way to learn how to write a UDP is to look at existing examples:\n\nmax_ndvi_composite: UDP code and description.\n\n\n\n\nA UDP is simply a parametrized version of an openEO process graph, and as such we recommend that you use the same code to develop and test your algorithm, as you use to generate the UDP json. This ensures that your UDP is functionally equivalent to your code. Your code remains your own, and you only need to export the JSON UDP definition to share it with APEx.\nIt is however advisable for your UDP to link back to a public git repository if available, making your open source code more discoverable.\n\n\n\nDeciding on the granularity of your UDP is an important aspect of making your algorithm usable. There is no need to try and fit all possible use cases into a single UDP. Instead, consider to define pieces of functionality that can work with a limited set of parameters, and write a single UDP for each piece.\n\n\n\nThis section provides some recommendations on how to name parameters in your UDP. While these are not mandatory, we recommend to consider them to avoid that users of the APEx Algorithm Services Catalogue would be confused by variations in process parameter names.\nPlease let us know if you encounter a parameter that could use a convention, and we will add it here.\n\n\nAny algorithm requires spatial filtering, so we can make the life of our users easier if we all use the same naming.\nIn the standard openEO processes load_collection and load_stac, spatial filtering done through the argument called spatial_extent, which support multiple types: a bounding box object, an inline GeoJSON object, a vector cube, or it can even be left empty (null). For maximum flexibility, compatibility and the best user experience, we recommend to perform spatial filtering in your algorithm:\n\ndirectly in load_collection/load_stac (instead of separate filter_bbox/filter_spatial processes)\nusing a parameter named spatial_extent (unless there are good reasons otherwise) spatial filtering in these processes, and also use spatial_extent as the parameter name unless there are good reasons otherwise.\n\nIf you pass on the spatial_extent parameter to all load_collection processes in your UDP, then it is also allowed to perform filtering using vector data. This conveniently allows for advanced spatial filtering use cases!\nThe Python client has a convenience function called Parameter.spatial_extent() to create this parameter when building your UDP via Python.\n\n\n\nFor openEO processes that support arbitrary temporal ranges, we recommend using temporal_extent as the name of parameter to ensure consistency with other openEO processes, such as load_collection.\nMany cases also require a time range with a fixed length. In such a case, you can allow to specify only the start or end date, and use the date_shift process to construct the second date in the temporal interval, ensuring a fixed length. This avoids faulty user input.\n\n\n\n\nMost openEO process graphs end in a save_resultprocess. However, this is not recommended for UDPs, as the user may want to perform additional processing steps before generating the result. So having a DataCube (raster or vector) as the final output is recommended unless if your service wants to enforce specific settings on how the output is to be generated.\n\n\n\nThe description of your UDP should be quite extensive if you want users to be able to easily assess if it’s suitable for them.\nWe recommend including these sections:\n\nDescription: A short description of what the algorithm does.\nPerformance characteristics: Information on the computational efficiency of the algorithm. Include a relative cost if available.\nExamples: One or more examples of the algorithm in action, using images or plots to show a typical result. Point to a STAC metadata file with an actual output asset to give users full insight into what is generated.\nLiterature references: If your algorithm is based on scientific literature, provide references to the relevant publications.\nKnown limitations: Any known limitations of the algorithm, such as the type of data it works best with, or the size of the area it can process efficiently.\nKnown artifacts: Use images and plots to clearly show any known artifacts that the algorithm may produce. This helps users to understand what to expect from the algorithm.\n\nA template is available to help you structure your documentation.",
    "crumbs": [
      "Guides",
      "Creating openEO based services"
    ]
  },
  {
    "objectID": "guides/udp_writer_guide.html#sec-udp-writing",
    "href": "guides/udp_writer_guide.html#sec-udp-writing",
    "title": "Guide for writing openEO User Defined Processes",
    "section": "",
    "text": "User defined processes (UDPs) are one out of two standardised options that APEx offers to publish algorithms as a service. This guide gives some concrete steps and guidelines to ensure that your UDP works well for your users. These guidelines are written with APEx in mind, but can also serve as a general guide for openEO UDPs. Where needed, recommendations and choices are made to increase uniformity across theAPEx Algorithm Services Catalogue.\nFor more background on UDP’s, or a basic tutorial on creating them, the open source Python client provides a good starting point.\nKeep in mind that APEx offers Algorithm Porting and Algorithm Onboarding support to help you with transforming your algorithm into an openEO UDP and onboarding it onto an APEx-compliant hosting platform.\n\n\nThe best way to learn how to write a UDP is to look at existing examples:\n\nmax_ndvi_composite: UDP code and description.\n\n\n\n\nA UDP is simply a parametrized version of an openEO process graph, and as such we recommend that you use the same code to develop and test your algorithm, as you use to generate the UDP json. This ensures that your UDP is functionally equivalent to your code. Your code remains your own, and you only need to export the JSON UDP definition to share it with APEx.\nIt is however advisable for your UDP to link back to a public git repository if available, making your open source code more discoverable.\n\n\n\nDeciding on the granularity of your UDP is an important aspect of making your algorithm usable. There is no need to try and fit all possible use cases into a single UDP. Instead, consider to define pieces of functionality that can work with a limited set of parameters, and write a single UDP for each piece.\n\n\n\nThis section provides some recommendations on how to name parameters in your UDP. While these are not mandatory, we recommend to consider them to avoid that users of the APEx Algorithm Services Catalogue would be confused by variations in process parameter names.\nPlease let us know if you encounter a parameter that could use a convention, and we will add it here.\n\n\nAny algorithm requires spatial filtering, so we can make the life of our users easier if we all use the same naming.\nIn the standard openEO processes load_collection and load_stac, spatial filtering done through the argument called spatial_extent, which support multiple types: a bounding box object, an inline GeoJSON object, a vector cube, or it can even be left empty (null). For maximum flexibility, compatibility and the best user experience, we recommend to perform spatial filtering in your algorithm:\n\ndirectly in load_collection/load_stac (instead of separate filter_bbox/filter_spatial processes)\nusing a parameter named spatial_extent (unless there are good reasons otherwise) spatial filtering in these processes, and also use spatial_extent as the parameter name unless there are good reasons otherwise.\n\nIf you pass on the spatial_extent parameter to all load_collection processes in your UDP, then it is also allowed to perform filtering using vector data. This conveniently allows for advanced spatial filtering use cases!\nThe Python client has a convenience function called Parameter.spatial_extent() to create this parameter when building your UDP via Python.\n\n\n\nFor openEO processes that support arbitrary temporal ranges, we recommend using temporal_extent as the name of parameter to ensure consistency with other openEO processes, such as load_collection.\nMany cases also require a time range with a fixed length. In such a case, you can allow to specify only the start or end date, and use the date_shift process to construct the second date in the temporal interval, ensuring a fixed length. This avoids faulty user input.\n\n\n\n\nMost openEO process graphs end in a save_resultprocess. However, this is not recommended for UDPs, as the user may want to perform additional processing steps before generating the result. So having a DataCube (raster or vector) as the final output is recommended unless if your service wants to enforce specific settings on how the output is to be generated.\n\n\n\nThe description of your UDP should be quite extensive if you want users to be able to easily assess if it’s suitable for them.\nWe recommend including these sections:\n\nDescription: A short description of what the algorithm does.\nPerformance characteristics: Information on the computational efficiency of the algorithm. Include a relative cost if available.\nExamples: One or more examples of the algorithm in action, using images or plots to show a typical result. Point to a STAC metadata file with an actual output asset to give users full insight into what is generated.\nLiterature references: If your algorithm is based on scientific literature, provide references to the relevant publications.\nKnown limitations: Any known limitations of the algorithm, such as the type of data it works best with, or the size of the area it can process efficiently.\nKnown artifacts: Use images and plots to clearly show any known artifacts that the algorithm may produce. This helps users to understand what to expect from the algorithm.\n\nA template is available to help you structure your documentation.",
    "crumbs": [
      "Guides",
      "Creating openEO based services"
    ]
  },
  {
    "objectID": "guides/udp_writer_guide.html#sec-udp-integration",
    "href": "guides/udp_writer_guide.html#sec-udp-integration",
    "title": "Guide for writing openEO User Defined Processes",
    "section": "Integrating your openEO process (UDP) in APEx",
    "text": "Integrating your openEO process (UDP) in APEx\nOnce you have an eligible openEO process, you are ready to integrate it in APEx. At this point, you should have an HTTP link to a JSON document that defines the process, and is publicly accessible. The most common way to do this is to store it in a public git repository. Tagging a release is a good way to ensure that the link remains stable.\n\nRegistering your process\nNext, you will need to upload a generic JSON to the APEx Algorithm Catalogue to register your process. For now, this is done by creating a pull request in the APEx Algorithm Catalogue GitHub repository.\nAn example json is provided below. The properties to modify are listed here:\n\nid: a unique identifier for your process\ncreated and updated timestamps\ntitle: a descriptive title\ndescription: a detailed description of the process\ncontacts: a list of contacts, with at least one principal investigator\nkeywords: a list of free form keywords\nthemes: Applicable concepts from a scheme. Concepts can be found in the ESA Data Ontology\nlicense: the license under which the process is published. You can use the SPDX license list for this. Proprietary licenses are possible, within the terms of your ESA project.\n\nThe links section is crucial, the following “rel” values are mandatory:\n\nopeneo-process: exactly one link to the JSON document that defines the process\nservice: at least one link to an openEO backend that is able to execute the process\nexample: at least one link to a STAC metadata file that shows the output of the process\n\nThe type field of these links should be set to application/json. Please provide a descriptive title for each link, allowing to understand what the link is about.\n\n{\n  \"id\": \"max_ndvi_composite\",\n  \"type\": \"Feature\",\n  \"conformsTo\": [\n    \"http://www.opengis.net/spec/ogcapi-records-1/1.0/req/record-core\"\n  ],\n  \"properties\": {\n    \"created\": \"2024-09-06T00:00:00Z\",\n    \"updated\": \"2024-09-06T00:00:00Z\",\n    \"type\": \"apex_algorithm\",\n    \"title\": \"Max NDVI Composite based on Sentinel-2 data\",\n    \"description\": \"A compositing algorithm for Sentinel-2 L2A data, ranking observations by their maximum NDVI.\",\n    \"cost_estimate\": 1,\n    \"cost_unit\": \"platform credits per km\\u00b2\",\n    \"keywords\": [\n      \"vegetation\"\n    ],\n    \"language\": {\n      \"code\": \"en-US\",\n      \"name\": \"English (United States)\"\n    },\n    \"languages\": [\n      {\n        \"code\": \"en-US\",\n        \"name\": \"English (United States)\"\n      }\n    ],\n    \"contacts\": [\n      {\n        \"name\": \"Jeroen Dries\",\n        \"position\": \"Researcher\",\n        \"organization\": \"VITO\",\n        \"links\": [\n          {\n            \"href\": \"https://www.vito.be/\",\n            \"rel\": \"about\",\n            \"type\": \"text/html\"\n          },\n          {\n            \"href\": \"https://github.com/jdries\",\n            \"rel\": \"about\",\n            \"type\": \"text/html\"\n          }\n        ],\n        \"contactInstructions\": \"Contact via VITO\",\n        \"roles\": [\n          \"principal investigator\"\n        ]\n      },\n      {\n        \"name\": \"VITO\",\n        \"links\": [\n          {\n            \"href\": \"https://www.vito.be/\",\n            \"rel\": \"about\",\n            \"type\": \"text/html\"\n          }\n        ],\n        \"contactInstructions\": \"SEE WEBSITE\",\n        \"roles\": [\n          \"processor\"\n        ]\n      }\n    ],\n    \"themes\": [ {\n        \"concepts\": [\n          { \"id\": \"NORMALIZED DIFFERENCE VEGETATION INDEX (NDVI)\" },\n          { \"id\": \"Sentinel-2 MSI\" }\n        ],\n        \"scheme\": \"https://gcmd.earthdata.nasa.gov/kms/concepts/concept_scheme/sciencekeywords\"\n      }],\n    \"formats\": [\n      \"GeoTiff\", \"netCDF\"\n    ],\n    \"license\": \"CC-BY-4.0\"\n  },\n  \"linkTemplates\": [],\n  \"links\": [\n    {\n      \"rel\": \"openeo-process\",\n      \"type\": \"application/json\",\n      \"title\": \"openEO Process Definition\",\n      \"href\": \"https://raw.githubusercontent.com/ESA-APEx/apex_algorithms/max_ndvi_composite/openeo_udp/examples/max_ndvi_composite/max_ndvi_composite.json\"\n    },\n    {\n      \"rel\": \"service\",\n      \"type\": \"application/json\",\n      \"title\": \"CDSE openEO federation\",\n      \"href\": \"https://openeofed.dataspace.copernicus.eu\"\n    },\n    {\n      \"rel\": \"example\",\n      \"type\": \"application/json\",\n      \"title\": \"Example output\",\n      \"href\": \"https://radiantearth.github.io/stac-browser/#/external/s3.waw3-1.cloudferro.com/swift/v1/APEx-examples/max_ndvi_denmark/collection.json\"\n    }\n  ]\n}\n\n\n\nDefining a validation & benchmark scenario\nAPEx has the capability to automatically check if your openEO process is working as expected, and if the cost for specific scenarios is sufficiently stable over time. This is a very important feature to avoid that your users have a bad experience. It also helps to ensure that the openEO backend provider you selected is performing well, and that changes to the backend service do not break your process.\nAPEx requires at least one benchmark scenario, to be able to correctly mark processes that are (temporarily)unavailable. When this happens, the ‘principal investigator’, as defined in the JSON of the previous step, is informed, allowing to take action as desired.\nThe benchmark scenarios are defined as JSON files in the benchmark_scenarios folder. The schema of these files is defined (as JSON Schema)in the schema/benchmark_scenario.json file.\nExample benchmark definition:\n[\n  {\n    \"id\": \"max_ndvi\",\n    \"type\": \"openeo\",\n    \"backend\": \"openeofed.dataspace.copernicus.eu\",\n    \"process_graph\": {\n      \"maxndvi1\": {\n        \"process_id\": \"max_ndvi\",\n        \"namespace\": \"https://raw.githubusercontent.com/ESA-APEx/apex_algorithms/f99f351d74d291d628e3aaa07fd078527a0cb631/openeo_udp/examples/max_ndvi/max_ndvi.json\",\n        \"arguments\": {\n          \"temporal_extent\": [\"2023-08-01\", \"2023-09-30\"],\n          ...\n        },\n        \"result\": true\n      }\n    },\n    \"reference_data\": {\n      \"job-results.json\": \"https://s3.example/max_ndvi.json:max_ndvi:reference:job-results.json\",\n      \"openEO.tif\": \"https://s3.example/max_ndvi.json:max_ndvi:reference:openEO.tif\"\n    }\n  },\n  ...\n]\nNote how each benchmark scenario references:\n\nthe target openEO backend to use.\nan openEO process graph to execute. The process graph will typically just contain a single node pointing with the namespace field to the desired process definition at a URL, following the remote process definition extension. The URL will typically be a raw GitHub URL to the JSON file in the openeo_udp folder, but it can also be a URL to a different location.\nreference data to which actual results should be compared.\n\n\nDefining and updating benchmark reference data\nA benchmark scenario should define reference data, with which the actual results of benchmark runs should be compared. The reference_data field of a benchmark scenario is a JSON object that maps each of the openEO batch job result assets (by asset name) to a URL where the reference data can be found, e.g.\n{\n  \"reference_data\": {\n    \"job-results.json\": \"https://s3.example/max_ndvi.json:max_ndvi:reference:job-results.json\",\n    \"openEO.tif\": \"https://s3.example/max_ndvi.json:max_ndvi:reference:openEO.tif\"\n  }\n}\nThe reference data URLs should be publicly accessible. For example, as suggested by the example, as S3 storage URLs. While you are free to manage the hosting of the reference data yourself, it is recommended to just rely on the APEx infrastructure and workflows as follows.\nThe APEx benchmarking system will automatically store the actual results (if any)of failed benchmark runs in a public S3 bucket managed by APEx, to allow post-mortem inspection of these results. The URLs of these actual result assets will be reported in the benchmark report and can be used directly as reference data URLs for subsequent benchmark runs. Of course, make sure the actual results are properly validated and acceptable under your standards before using them as reference data for future benchmark runs.\n\nInitial reference data\nWhen you are initially defining a benchmark scenario, you may not have the full set of expected reference data and metadata yet. In this case, it’s fine to leave out the reference_data field, make it an empty mapping object, or use invalid placeholder URLs. When the benchmark scenario is executed and the underlying openEO batch job finishes successfully, the actual batch job results will be downloaded first. Next, the full actual result set will be compared to the reference data, which will be missing or incomplete, causing the benchmark run to fail. This failure will trigger the storage of the actual results in the APEx S3 bucket, and the corresponding URLs will be reported in the (pytest based) benchmark run output of the corresponding GitHub workflow run. It will look like this:\n-------------------- `upload_assets` stats: {'uploaded': 2} --------------------\n- tests/test_benchmarks.py::test_run_benchmark[max_ndvi]:\n  - 'actual/openEO.tif' uploaded to 'https://s3.example/gh-1234!max_ndvi!actual/openEO.tif'\n  - 'actual/job-results.json' uploaded to 'https://s3.example/gh-1234!max_ndvi!actual/job-results.json'\nThese URLs can be used as reference data URLs for subsequent benchmark runs.\n\n\nUpdating reference data\nWhen an algorithm is updated, or the underlying data changes for some reason, a benchmark scenario might start to fail on the reference data comparison. Updating the reference data is practically the same as defining it initially:look up the URLs of the actual results in the benchmark report, validate these new results, and update the benchmark scenario with the new reference data URLs.",
    "crumbs": [
      "Guides",
      "Creating openEO based services"
    ]
  },
  {
    "objectID": "guides/upscaling_openeo.html",
    "href": "guides/upscaling_openeo.html",
    "title": "Upscaling of openEO parametrized proces",
    "section": "",
    "text": "This notebook demonstrates how to authenticate with the OpenEO backend, create a spatial grid for a specific region, prepare jobs for geospatial analysis, run them in parallel, and visualize the job statuses using interactive maps.\nWe will go through the following steps: 1. Importing Required Packages 2. Authentication and Backend Initialization 3. Generating a Spatial Grid for the Antwerp Region 4. Visualizing the spatial grid 5. Preparing Jobs for processing 6. Visualizing Job Status Using Plotly Maps\n\n# 1. Importing Required Packages\n\nimport json\nimport openeo\nimport pandas as pd\nimport shapely\nfrom openeo.extra.job_management import MultiBackendJobManager, CsvJobDatabase\n\n\n2. Authentication and Backend Initialization\nWe start by connecting to the Copernicus Dataspace OpenEO backend and authenticating using OpenID Connect. The MultiBackendJobManager is initialized to manage jobs across multiple backends.\n\n# Authenticate and add the backend\nconnection = openeo.connect(url=\"openeofed.dataspace.copernicus.eu\").authenticate_oidc()\n\n# initialize the job manager\nmanager = MultiBackendJobManager()\nmanager.add_backend(\"cdse\", connection=connection, parallel_jobs=2)\n\nAuthenticated using refresh token.\n\n\n\n\n3. Generating a Spatial Grid for the Antwerp Region\nWe define a bounding box for Antwerp in WGS84 coordinates and convert it to UTM (Universal Transverse Mercator) coordinates. A grid is created using these UTM coordinates and then converted back to WGS84 for further processing.\nWe also save the grid as a GeoJSON file for future use.\n\n# 3. Generate the grid for Antwerp\nimport geopandas as gpd\nfrom shapely.geometry import box\nimport numpy as np\nfrom pyproj import Transformer\n\n# Define the bounding box, transformers, and grid size\ntransformer_to_utm = Transformer.from_crs(\"epsg:4326\", \"epsg:32631\", always_xy=True)\ntransformer_to_latlon = Transformer.from_crs(\"epsg:32631\", \"epsg:4326\", always_xy=True)\n\nmin_lon, min_lat = 4.35, 51.10\nmax_lon, max_lat = 4.45, 51.20\nminx, miny = transformer_to_utm.transform(min_lon, min_lat)\nmaxx, maxy = transformer_to_utm.transform(max_lon, max_lat)\ngrid_size_m = 5000\n\nx_coords = np.arange(minx, maxx, grid_size_m)\ny_coords = np.arange(miny, maxy, grid_size_m)\n\n# Create polygons for the grid\npolygons = [box(x, y, x + grid_size_m, y + grid_size_m) for x in x_coords for y in y_coords]\n\n# Create a GeoDataFrame and save it\ngrid_gdf_utm = gpd.GeoDataFrame({'geometry': polygons}, crs=\"EPSG:32631\")\ngrid_gdf_latlon = grid_gdf_utm.to_crs(\"EPSG:4326\")\ngrid_gdf_latlon['id'] = range(len(grid_gdf_latlon))\nimport os\n#os.mkdir(\"resources\")\ngrid_gdf_latlon.to_file(\"resources/antwerp_grid_5km.geojson\", driver=\"GeoJSON\")\n\n\nfrom plotly.offline import init_notebook_mode, iplot\n\ninit_notebook_mode()  \nimport plotly.io as pio\npio.renderers.default = 'iframe'\n\n\n\n4. Visualizing the Spatial Grid\nUsing Plotly, we visualize the spatial grid we just created.\n\n# 4. Visualizing the grid using Plotly\nimport plotly.express as px\n\nbboxes = gpd.read_file(\"./resources/antwerp_grid_5km.geojson\")\n\nfig = px.choropleth_mapbox(\n    bboxes,\n    geojson=bboxes.geometry,\n    locations=bboxes.index,\n    mapbox_style=\"carto-positron\",\n    center={\"lat\": 51.15, \"lon\": 4.4},\n    zoom=8,\n    title=\"Spatial Grid for Antwerp Region\"\n)\n\nfig.update_geos(fitbounds=\"locations\")\nfig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\nfig.show()\n\n\n\n\n\n\n5. Preparing Jobs for processing\nFor an existing openEO process, we use default parameters as well as our dataframe with spatial extents to initialize the jobs to process.\nMore documentation on this concept is available here\n\nfrom openeo.extra.job_management import (\n    MultiBackendJobManager,\n    create_job_db,\n    ProcessBasedJobCreator,\n)\n\n# Job creator, based on a parameterized openEO process\n# (specified by the remote process definition at given URL)\njob_starter = ProcessBasedJobCreator(\n    namespace=\"https://raw.githubusercontent.com/ESA-APEx/apex_algorithms/refs/heads/main/openeo_udp/bap_composite.json\",\n    parameter_defaults={\n        \"bands\": [\"B02\", \"B03\"],\n        \"temporal_extent\": [\"2023-05-01\",\"2023-07-01\"]\n    },\n)\n\njob_db = create_job_db(\"job_tracker.csv\",grid_gdf_latlon,on_exists=\"skip\")\njob_db.read()\n\n\n\n\n\n\n\n\ngeometry\nid\nstatus\nstart_time\nrunning_start_time\ncpu\nmemory\nduration\nbackend_name\n\n\n\n\n0\nPOLYGON ((4.42139 51.09915, 4.42277 51.14410, ...\ncdse-j-241105f07d1744dfbffb7df433bf0593\nfinished\n2024-11-05T14:22:06Z\n2024-11-05T14:23:52Z\n225 cpu-seconds\n1137013 mb-seconds\n154 seconds\ncdse\n\n\n1\nPOLYGON ((4.42277 51.14410, 4.42415 51.18905, ...\ncdse-j-2411059214eb4c699616820e3a6cec63\nfinished\n2024-11-05T14:22:36Z\n2024-11-05T14:24:38Z\n357 cpu-seconds\n2171980 mb-seconds\n163 seconds\ncdse\n\n\n2\nPOLYGON ((4.42415 51.18905, 4.42554 51.23400, ...\ncdse-j-2411051f49a6403c887d564dbda02931\nfinished\n2024-11-05T14:25:55Z\n2024-11-05T14:27:14Z\n170 cpu-seconds\n908083 mb-seconds\n155 seconds\ncdse\n\n\n3\nPOLYGON ((4.49277 51.09826, 4.49422 51.14321, ...\nNaN\nnot_started\n2024-11-05T14:26:11Z\nNaN\nNaN\nNaN\nNaN\ncdse\n\n\n4\nPOLYGON ((4.49422 51.14321, 4.49568 51.18816, ...\nNaN\nnot_started\n2024-11-05T14:27:15Z\nNaN\nNaN\nNaN\nNaN\ncdse\n\n\n5\nPOLYGON ((4.49568 51.18816, 4.49713 51.23310, ...\nNaN\nnot_started\n2024-11-05T14:27:30Z\nNaN\nNaN\nNaN\nNaN\ncdse\n\n\n\n\n\n\n\n\n\n6. Visualizing Job Status\nSet up a threaded approach to run the jobs and visualise at the same time.\nWe load the job tracker file and visualize the status of each job in the spatial grid using a Plotly choropleth map.\n\n# Step 5: Initialize job database\nimport plotly.express as px\nfrom shapely import geometry, from_wkt\nimport json\nimport geopandas as gpd\nimport pandas as pd\nimport time\nfrom plotly import offline\nfrom IPython.display import clear_output\n\n# Update colors based on job status\ncolor_dict = {\n    \"not_started\": 'lightgrey', \n    \"created\": 'gold', \n    \"queued\": 'lightsteelblue', \n    \"running\": 'navy', \n    \"finished\": 'lime',\n    \"error\": 'darkred',\n    \"skipped\": 'darkorange',\n    None: 'grey'  # Default color for no status\n}\n\n# Step 6: Start job manager in a separate thread\nmanager.start_job_thread(start_job=job_starter, job_db=job_db)\n\n# Step 7: Visualization Loop\n# Initialize the figure outside the loop\n\nwhile not manager._stop_thread:\n    try:\n        # Read job statuses from the tracker\n        status_df = job_db.read()\n\n        # Use the 'status' column to determine colors, with a fallback for NaNs or None\n        status_df['color'] = status_df['status'].map(color_dict).fillna(color_dict[None])\n\n        minx, miny, maxx, maxy = status_df.total_bounds\n        center_lat = (miny + maxy) / 2\n        center_lon = (minx + maxx) / 2\n\n        fig = px.choropleth_mapbox(\n            status_df,\n            geojson=status_df.geometry.__geo_interface__,  # Use the correct GeoJSON representation\n            locations=status_df.index,\n            color='status',  # Use 'status' for the color\n            color_discrete_map=color_dict,  # Map colors directly from the dictionary\n            mapbox_style=\"carto-positron\",\n            center={\"lat\": center_lat, \"lon\": center_lon},  # Center on your area of interest\n            zoom=8,\n            title=\"Job Status Overview\",\n            labels={'status': 'Job Status'} \n        )\n        fig.update_geos(fitbounds=\"locations\")\n        fig.update_layout(margin={\"r\": 0, \"t\": 0, \"l\": 0, \"b\": 0})\n\n        # Display the updated figure\n        clear_output()\n        offline.iplot(fig)\n\n        # Check if all jobs are done\n        if status_df['status'].isin([\"not_started\", \"created\", \"queued\", \"running\"]).sum() == 0:\n            manager.stop_job_thread()\n\n        time.sleep(15)  # Wait before the next update\n\n    except KeyboardInterrupt:\n        break",
    "crumbs": [
      "Guides",
      "Upscaling openEO based services"
    ]
  },
  {
    "objectID": "guides/stac_catalog_ingest.html",
    "href": "guides/stac_catalog_ingest.html",
    "title": "APEx STAC catalogue editor guide",
    "section": "",
    "text": "This notebook demonstrates how to add metadata to an APEx STAC catalogue.\nThe STAC API is a standard, so other STAC documentation is also relevant.",
    "crumbs": [
      "Guides",
      "Ingest STAC metadata"
    ]
  },
  {
    "objectID": "guides/stac_catalog_ingest.html#creating-stac-metadata-from-scratch",
    "href": "guides/stac_catalog_ingest.html#creating-stac-metadata-from-scratch",
    "title": "APEx STAC catalogue editor guide",
    "section": "Creating STAC metadata from scratch",
    "text": "Creating STAC metadata from scratch\nIn this first part, we show how to start from zero with a very basic example.\nIn most real-world cases, it is not needed nor recommended to author STAC metadata from scratch directly. Instead, various tools and platforms will generate STAC metadata for you. The STAC metadata generated here is entirely fictional, and also very much useless. It is not an example of how to create high quality metadata that complies with FAIR principles.\nThis example mostly serves as a demonstration of the API, which is very simply to use. Most of the more complex examples and tools will avoid this type of direct interaction.\n\n\nimport requests\nfrom owslib.ogcapi.records import Records\nfrom owslib.util import Authentication\n\n\n\n\nclass BearerAuth(requests.auth.AuthBase):\n    def __init__(self, token):\n        self.token = token\n    def __call__(self, r):\n        r.headers[\"authorization\"] = \"Bearer \" + self.token\n        return r\n    \n\n\nauth = BearerAuth(\"\"\"PROVIDE_OIDC_TOKEN\"\"\")\n\n\n\nr = Records(\"https://catalogue.project-a.apex.esa.int\",auth=Authentication(auth_delegate=auth))\nr\n\n&lt;owslib.ogcapi.records.Records at 0x7809e804b050&gt;\n\n\nTo get some information about our daaset, we can run a simple gdalinfocommand against a representative asset.\n\n!GDAL_DISABLE_READDIR_ON_OPEN=EMPTY_DIR gdalinfo -json \"/vsicurl/https://eoresults.esa.int/d/APEX_TEST/2020/03/01/europe_aggr-orgc_00-020_mean_100_202003-202210/europe_aggr-orgc_00-020_mean_100_202003-202210.tif\"\n\n{\n  \"description\":\"/vsicurl/https://eoresults.esa.int/d/APEX_TEST/2020/03/01/europe_aggr-orgc_00-020_mean_100_202003-202210/europe_aggr-orgc_00-020_mean_100_202003-202210.tif\",\n  \"driverShortName\":\"GTiff\",\n  \"driverLongName\":\"GeoTIFF\",\n  \"files\":[\n    \"/vsicurl/https://eoresults.esa.int/d/APEX_TEST/2020/03/01/europe_aggr-orgc_00-020_mean_100_202003-202210/europe_aggr-orgc_00-020_mean_100_202003-202210.tif\"\n  ],\n  \"size\":[\n    40888,\n    41712\n  ],\n  \"coordinateSystem\":{\n    \"wkt\":\"PROJCRS[\\\"ETRS89-extended / LAEA Europe\\\",\\n    BASEGEOGCRS[\\\"ETRS89\\\",\\n        ENSEMBLE[\\\"European Terrestrial Reference System 1989 ensemble\\\",\\n            MEMBER[\\\"European Terrestrial Reference Frame 1989\\\"],\\n            MEMBER[\\\"European Terrestrial Reference Frame 1990\\\"],\\n            MEMBER[\\\"European Terrestrial Reference Frame 1991\\\"],\\n            MEMBER[\\\"European Terrestrial Reference Frame 1992\\\"],\\n            MEMBER[\\\"European Terrestrial Reference Frame 1993\\\"],\\n            MEMBER[\\\"European Terrestrial Reference Frame 1994\\\"],\\n            MEMBER[\\\"European Terrestrial Reference Frame 1996\\\"],\\n            MEMBER[\\\"European Terrestrial Reference Frame 1997\\\"],\\n            MEMBER[\\\"European Terrestrial Reference Frame 2000\\\"],\\n            MEMBER[\\\"European Terrestrial Reference Frame 2005\\\"],\\n            MEMBER[\\\"European Terrestrial Reference Frame 2014\\\"],\\n            MEMBER[\\\"European Terrestrial Reference Frame 2020\\\"],\\n            ELLIPSOID[\\\"GRS 1980\\\",6378137,298.257222101,\\n                LENGTHUNIT[\\\"metre\\\",1]],\\n            ENSEMBLEACCURACY[0.1]],\\n        PRIMEM[\\\"Greenwich\\\",0,\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433]],\\n        ID[\\\"EPSG\\\",4258]],\\n    CONVERSION[\\\"Europe Equal Area 2001\\\",\\n        METHOD[\\\"Lambert Azimuthal Equal Area\\\",\\n            ID[\\\"EPSG\\\",9820]],\\n        PARAMETER[\\\"Latitude of natural origin\\\",52,\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433],\\n            ID[\\\"EPSG\\\",8801]],\\n        PARAMETER[\\\"Longitude of natural origin\\\",10,\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433],\\n            ID[\\\"EPSG\\\",8802]],\\n        PARAMETER[\\\"False easting\\\",4321000,\\n            LENGTHUNIT[\\\"metre\\\",1],\\n            ID[\\\"EPSG\\\",8806]],\\n        PARAMETER[\\\"False northing\\\",3210000,\\n            LENGTHUNIT[\\\"metre\\\",1],\\n            ID[\\\"EPSG\\\",8807]]],\\n    CS[Cartesian,2],\\n        AXIS[\\\"northing (Y)\\\",north,\\n            ORDER[1],\\n            LENGTHUNIT[\\\"metre\\\",1]],\\n        AXIS[\\\"easting (X)\\\",east,\\n            ORDER[2],\\n            LENGTHUNIT[\\\"metre\\\",1]],\\n    USAGE[\\n        SCOPE[\\\"Statistical analysis.\\\"],\\n        AREA[\\\"Europe - European Union (EU) countries and candidates. Europe - onshore and offshore: Albania; Andorra; Austria; Belgium; Bosnia and Herzegovina; Bulgaria; Croatia; Cyprus; Czechia; Denmark; Estonia; Faroe Islands; Finland; France; Germany; Gibraltar; Greece; Hungary; Iceland; Ireland; Italy; Kosovo; Latvia; Liechtenstein; Lithuania; Luxembourg; Malta; Monaco; Montenegro; Netherlands; North Macedonia; Norway including Svalbard and Jan Mayen; Poland; Portugal including Madeira and Azores; Romania; San Marino; Serbia; Slovakia; Slovenia; Spain including Canary Islands; Sweden; Switzerland; Türkiye (Turkey); United Kingdom (UK) including Channel Islands and Isle of Man; Vatican City State.\\\"],\\n        BBOX[24.6,-35.58,84.73,44.83]],\\n    ID[\\\"EPSG\\\",3035]]\",\n    \"dataAxisToSRSAxisMapping\":[\n      2,\n      1\n    ]\n  },\n  \"geoTransform\":[\n    2515000.0,\n    100.0,\n    0.0,\n    5512800.0,\n    0.0,\n    -100.0\n  ],\n  \"metadata\":{\n    \"\":{\n      \"AREA_OR_POINT\":\"Area\"\n    },\n    \"IMAGE_STRUCTURE\":{\n      \"LAYOUT\":\"COG\",\n      \"COMPRESSION\":\"DEFLATE\",\n      \"INTERLEAVE\":\"BAND\",\n      \"PREDICTOR\":\"2\"\n    }\n  },\n  \"cornerCoordinates\":{\n    \"upperLeft\":[\n      2515000.0,\n      5512800.0\n    ],\n    \"lowerLeft\":[\n      2515000.0,\n      1341600.0\n    ],\n    \"lowerRight\":[\n      6603800.0,\n      1341600.0\n    ],\n    \"upperRight\":[\n      6603800.0,\n      5512800.0\n    ],\n    \"center\":[\n      4559400.0,\n      3427200.0\n    ]\n  },\n  \"wgs84Extent\":{\n    \"type\":\"Polygon\",\n    \"coordinates\":[\n      [\n        [\n          -35.0421748,\n          67.1549879\n        ],\n        [\n          -9.3033749,\n          33.067326\n        ],\n        [\n          34.1876844,\n          31.8603398\n        ],\n        [\n          62.8622654,\n          64.3475852\n        ],\n        [\n          -35.0421748,\n          67.1549879\n        ]\n      ]\n    ]\n  },\n  \"bands\":[\n    {\n      \"band\":1,\n      \"block\":[\n        512,\n        512\n      ],\n      \"type\":\"UInt16\",\n      \"colorInterpretation\":\"Gray\",\n      \"noDataValue\":65535.0,\n      \"overviews\":[\n        {\n          \"size\":[\n            20444,\n            20856\n          ]\n        },\n        {\n          \"size\":[\n            10222,\n            10428\n          ]\n        },\n        {\n          \"size\":[\n            5111,\n            5214\n          ]\n        },\n        {\n          \"size\":[\n            2555,\n            2607\n          ]\n        },\n        {\n          \"size\":[\n            1277,\n            1303\n          ]\n        },\n        {\n          \"size\":[\n            638,\n            651\n          ]\n        },\n        {\n          \"size\":[\n            319,\n            325\n          ]\n        }\n      ],\n      \"metadata\":{}\n    }\n  ],\n  \"stac\":{\n    \"proj:shape\":[\n      41712,\n      40888\n    ],\n    \"proj:wkt2\":\"PROJCRS[\\\"ETRS89-extended / LAEA Europe\\\",\\n    BASEGEOGCRS[\\\"ETRS89\\\",\\n        ENSEMBLE[\\\"European Terrestrial Reference System 1989 ensemble\\\",\\n            MEMBER[\\\"European Terrestrial Reference Frame 1989\\\"],\\n            MEMBER[\\\"European Terrestrial Reference Frame 1990\\\"],\\n            MEMBER[\\\"European Terrestrial Reference Frame 1991\\\"],\\n            MEMBER[\\\"European Terrestrial Reference Frame 1992\\\"],\\n            MEMBER[\\\"European Terrestrial Reference Frame 1993\\\"],\\n            MEMBER[\\\"European Terrestrial Reference Frame 1994\\\"],\\n            MEMBER[\\\"European Terrestrial Reference Frame 1996\\\"],\\n            MEMBER[\\\"European Terrestrial Reference Frame 1997\\\"],\\n            MEMBER[\\\"European Terrestrial Reference Frame 2000\\\"],\\n            MEMBER[\\\"European Terrestrial Reference Frame 2005\\\"],\\n            MEMBER[\\\"European Terrestrial Reference Frame 2014\\\"],\\n            MEMBER[\\\"European Terrestrial Reference Frame 2020\\\"],\\n            ELLIPSOID[\\\"GRS 1980\\\",6378137,298.257222101,\\n                LENGTHUNIT[\\\"metre\\\",1]],\\n            ENSEMBLEACCURACY[0.1]],\\n        PRIMEM[\\\"Greenwich\\\",0,\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433]],\\n        ID[\\\"EPSG\\\",4258]],\\n    CONVERSION[\\\"Europe Equal Area 2001\\\",\\n        METHOD[\\\"Lambert Azimuthal Equal Area\\\",\\n            ID[\\\"EPSG\\\",9820]],\\n        PARAMETER[\\\"Latitude of natural origin\\\",52,\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433],\\n            ID[\\\"EPSG\\\",8801]],\\n        PARAMETER[\\\"Longitude of natural origin\\\",10,\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433],\\n            ID[\\\"EPSG\\\",8802]],\\n        PARAMETER[\\\"False easting\\\",4321000,\\n            LENGTHUNIT[\\\"metre\\\",1],\\n            ID[\\\"EPSG\\\",8806]],\\n        PARAMETER[\\\"False northing\\\",3210000,\\n            LENGTHUNIT[\\\"metre\\\",1],\\n            ID[\\\"EPSG\\\",8807]]],\\n    CS[Cartesian,2],\\n        AXIS[\\\"northing (Y)\\\",north,\\n            ORDER[1],\\n            LENGTHUNIT[\\\"metre\\\",1]],\\n        AXIS[\\\"easting (X)\\\",east,\\n            ORDER[2],\\n            LENGTHUNIT[\\\"metre\\\",1]],\\n    USAGE[\\n        SCOPE[\\\"Statistical analysis.\\\"],\\n        AREA[\\\"Europe - European Union (EU) countries and candidates. Europe - onshore and offshore: Albania; Andorra; Austria; Belgium; Bosnia and Herzegovina; Bulgaria; Croatia; Cyprus; Czechia; Denmark; Estonia; Faroe Islands; Finland; France; Germany; Gibraltar; Greece; Hungary; Iceland; Ireland; Italy; Kosovo; Latvia; Liechtenstein; Lithuania; Luxembourg; Malta; Monaco; Montenegro; Netherlands; North Macedonia; Norway including Svalbard and Jan Mayen; Poland; Portugal including Madeira and Azores; Romania; San Marino; Serbia; Slovakia; Slovenia; Spain including Canary Islands; Sweden; Switzerland; Türkiye (Turkey); United Kingdom (UK) including Channel Islands and Isle of Man; Vatican City State.\\\"],\\n        BBOX[24.6,-35.58,84.73,44.83]],\\n    ID[\\\"EPSG\\\",3035]]\",\n    \"proj:epsg\":3035,\n    \"proj:projjson\":{\n      \"$schema\":\"https://proj.org/schemas/v0.7/projjson.schema.json\",\n      \"type\":\"ProjectedCRS\",\n      \"name\":\"ETRS89-extended / LAEA Europe\",\n      \"base_crs\":{\n        \"name\":\"ETRS89\",\n        \"datum_ensemble\":{\n          \"name\":\"European Terrestrial Reference System 1989 ensemble\",\n          \"members\":[\n            {\n              \"name\":\"European Terrestrial Reference Frame 1989\",\n              \"id\":{\n                \"authority\":\"EPSG\",\n                \"code\":1178\n              }\n            },\n            {\n              \"name\":\"European Terrestrial Reference Frame 1990\",\n              \"id\":{\n                \"authority\":\"EPSG\",\n                \"code\":1179\n              }\n            },\n            {\n              \"name\":\"European Terrestrial Reference Frame 1991\",\n              \"id\":{\n                \"authority\":\"EPSG\",\n                \"code\":1180\n              }\n            },\n            {\n              \"name\":\"European Terrestrial Reference Frame 1992\",\n              \"id\":{\n                \"authority\":\"EPSG\",\n                \"code\":1181\n              }\n            },\n            {\n              \"name\":\"European Terrestrial Reference Frame 1993\",\n              \"id\":{\n                \"authority\":\"EPSG\",\n                \"code\":1182\n              }\n            },\n            {\n              \"name\":\"European Terrestrial Reference Frame 1994\",\n              \"id\":{\n                \"authority\":\"EPSG\",\n                \"code\":1183\n              }\n            },\n            {\n              \"name\":\"European Terrestrial Reference Frame 1996\",\n              \"id\":{\n                \"authority\":\"EPSG\",\n                \"code\":1184\n              }\n            },\n            {\n              \"name\":\"European Terrestrial Reference Frame 1997\",\n              \"id\":{\n                \"authority\":\"EPSG\",\n                \"code\":1185\n              }\n            },\n            {\n              \"name\":\"European Terrestrial Reference Frame 2000\",\n              \"id\":{\n                \"authority\":\"EPSG\",\n                \"code\":1186\n              }\n            },\n            {\n              \"name\":\"European Terrestrial Reference Frame 2005\",\n              \"id\":{\n                \"authority\":\"EPSG\",\n                \"code\":1204\n              }\n            },\n            {\n              \"name\":\"European Terrestrial Reference Frame 2014\",\n              \"id\":{\n                \"authority\":\"EPSG\",\n                \"code\":1206\n              }\n            },\n            {\n              \"name\":\"European Terrestrial Reference Frame 2020\",\n              \"id\":{\n                \"authority\":\"EPSG\",\n                \"code\":1382\n              }\n            }\n          ],\n          \"ellipsoid\":{\n            \"name\":\"GRS 1980\",\n            \"semi_major_axis\":6378137,\n            \"inverse_flattening\":298.257222101\n          },\n          \"accuracy\":\"0.1\",\n          \"id\":{\n            \"authority\":\"EPSG\",\n            \"code\":6258\n          }\n        },\n        \"coordinate_system\":{\n          \"subtype\":\"ellipsoidal\",\n          \"axis\":[\n            {\n              \"name\":\"Geodetic latitude\",\n              \"abbreviation\":\"Lat\",\n              \"direction\":\"north\",\n              \"unit\":\"degree\"\n            },\n            {\n              \"name\":\"Geodetic longitude\",\n              \"abbreviation\":\"Lon\",\n              \"direction\":\"east\",\n              \"unit\":\"degree\"\n            }\n          ]\n        },\n        \"id\":{\n          \"authority\":\"EPSG\",\n          \"code\":4258\n        }\n      },\n      \"conversion\":{\n        \"name\":\"Europe Equal Area 2001\",\n        \"method\":{\n          \"name\":\"Lambert Azimuthal Equal Area\",\n          \"id\":{\n            \"authority\":\"EPSG\",\n            \"code\":9820\n          }\n        },\n        \"parameters\":[\n          {\n            \"name\":\"Latitude of natural origin\",\n            \"value\":52,\n            \"unit\":\"degree\",\n            \"id\":{\n              \"authority\":\"EPSG\",\n              \"code\":8801\n            }\n          },\n          {\n            \"name\":\"Longitude of natural origin\",\n            \"value\":10,\n            \"unit\":\"degree\",\n            \"id\":{\n              \"authority\":\"EPSG\",\n              \"code\":8802\n            }\n          },\n          {\n            \"name\":\"False easting\",\n            \"value\":4321000,\n            \"unit\":\"metre\",\n            \"id\":{\n              \"authority\":\"EPSG\",\n              \"code\":8806\n            }\n          },\n          {\n            \"name\":\"False northing\",\n            \"value\":3210000,\n            \"unit\":\"metre\",\n            \"id\":{\n              \"authority\":\"EPSG\",\n              \"code\":8807\n            }\n          }\n        ]\n      },\n      \"coordinate_system\":{\n        \"subtype\":\"Cartesian\",\n        \"axis\":[\n          {\n            \"name\":\"Northing\",\n            \"abbreviation\":\"Y\",\n            \"direction\":\"north\",\n            \"unit\":\"metre\"\n          },\n          {\n            \"name\":\"Easting\",\n            \"abbreviation\":\"X\",\n            \"direction\":\"east\",\n            \"unit\":\"metre\"\n          }\n        ]\n      },\n      \"scope\":\"Statistical analysis.\",\n      \"area\":\"Europe - European Union (EU) countries and candidates. Europe - onshore and offshore: Albania; Andorra; Austria; Belgium; Bosnia and Herzegovina; Bulgaria; Croatia; Cyprus; Czechia; Denmark; Estonia; Faroe Islands; Finland; France; Germany; Gibraltar; Greece; Hungary; Iceland; Ireland; Italy; Kosovo; Latvia; Liechtenstein; Lithuania; Luxembourg; Malta; Monaco; Montenegro; Netherlands; North Macedonia; Norway including Svalbard and Jan Mayen; Poland; Portugal including Madeira and Azores; Romania; San Marino; Serbia; Slovakia; Slovenia; Spain including Canary Islands; Sweden; Switzerland; Türkiye (Turkey); United Kingdom (UK) including Channel Islands and Isle of Man; Vatican City State.\",\n      \"bbox\":{\n        \"south_latitude\":24.6,\n        \"west_longitude\":-35.58,\n        \"north_latitude\":84.73,\n        \"east_longitude\":44.83\n      },\n      \"id\":{\n        \"authority\":\"EPSG\",\n        \"code\":3035\n      }\n    },\n    \"proj:transform\":[\n      2515000.0,\n      100.0,\n      0.0,\n      5512800.0,\n      0.0,\n      -100.0\n    ],\n    \"raster:bands\":[\n      {\n        \"data_type\":\"uint16\",\n        \"nodata\":null\n      }\n    ],\n    \"eo:bands\":[\n      {\n        \"name\":\"b1\",\n        \"description\":\"Gray\"\n      }\n    ]\n  }\n}\n\n\n\nfrom datetime import datetime\nimport pystac\n\nspatial_extent = pystac.SpatialExtent(bboxes=[[4,51,5,52]])\ncollection_interval = sorted([datetime(2020,1,1), datetime(2022,1,1)])\ntemporal_extent = pystac.TemporalExtent(intervals=[collection_interval])\n\ndescription_markdown = \"\"\"\nSOC content in the 0-20 cm top soil expressed in g kg-1 for 100 m resolution pixels\n\nApplications:\n\n- First globally consistent and contiguous complete gridded soil property map of Europe\n- Indicator of soil health, as per Mission Area Soil Health and Food\n- By 2030, at least 75% of soils in each EU Member State should be in healthy condition or show a\nsignificant improvement towards meeting accepted thresholds of indicators, to support ecosystem\nservices\n\n*Reliability*: More than 83 % of the cross-validation points fall within the 70% prediction interval for the bare soil model.\nFor the vegetated area model 94 % of the points fall within the 90 % prediction interval.\n\"\"\"\n\nitem_assets = {\n    \"SOC\": {\n       \"type\": \"image/tiff; application=geotiff; profile=cloud-optimized\",\n        \"title\": \"SOC\",\n        \"description\": \"SOC content in the 0-20 cm top soil expressed in g kg-1\",\n        \"data_type\": \"uint16\",\n        \"nodata\": 65535,\n        \"unit\": \"g kg-1\",\n        \"roles\": [\n            \"data\"\n        ]\n    }\n}\n\ncollection_extent = pystac.Extent(spatial=spatial_extent, temporal=temporal_extent)\ncollection = pystac.Collection(id='esa-worldsoils',\n                               description=description_markdown,\n                               extent=collection_extent,\n                               extra_fields=dict(item_assets=item_assets),\n                               license='CC-BY-SA-4.0')\n\ncollection.summaries.add(\"proj:code\",[\"EPSG:3035\"])\ncollection.summaries.add(\"gsd\",[100])\ncollection.summaries.add(\"bands\",[{\n    \"title\": \"SOC\",\n    \"gsd\": 100\n}])\n\n\ncollection\n\n\n\n\n\n    \n        \n            \n                \n                    \n        \n            type\n            \"Collection\"\n        \n    \n                \n            \n                \n                    \n        \n            id\n            \"esa-worldsoils\"\n        \n    \n                \n            \n                \n                    \n        \n            stac_version\n            \"1.0.0\"\n        \n    \n                \n            \n                \n                    \n        \n            description\n            \"\nSOC content in the 0-20 cm top soil expressed in g kg-1 for 100 m resolution pixels\n\nApplications:\n\n- First globally consistent and contiguous complete gridded soil property map of Europe\n- Indicator of soil health, as per Mission Area Soil Health and Food\n- By 2030, at least 75% of soils in each EU Member State should be in healthy condition or show a\nsignificant improvement towards meeting accepted thresholds of indicators, to support ecosystem\nservices\n\n*Reliability*: More than 83 % of the cross-validation points fall within the 70% prediction interval for the bare soil model.\nFor the vegetated area model 94 % of the points fall within the 90 % prediction interval.\n\"\n        \n    \n                \n            \n                \n                    \n        links[] 0 items\n        \n    \n                \n            \n                \n                    \n        \n            item_assets\n            \n        \n            \n                \n        \n            SOC\n            \n        \n            \n                \n        \n            type\n            \"image/tiff; application=geotiff; profile=cloud-optimized\"\n        \n    \n            \n        \n            \n                \n        \n            title\n            \"SOC\"\n        \n    \n            \n        \n            \n                \n        \n            description\n            \"SOC content in the 0-20 cm top soil expressed in g kg-1\"\n        \n    \n            \n        \n            \n                \n        \n            data_type\n            \"uint16\"\n        \n    \n            \n        \n            \n                \n        \n            nodata\n            65535\n        \n    \n            \n        \n            \n                \n        \n            unit\n            \"g kg-1\"\n        \n    \n            \n        \n            \n                \n        roles[] 1 items\n        \n            \n        \n            \n                \n        \n            0\n            \"data\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n                \n            \n                \n                    \n        \n            extent\n            \n        \n            \n                \n        \n            spatial\n            \n        \n            \n                \n        bbox[] 1 items\n        \n            \n        \n            \n                \n        0[] 4 items\n        \n            \n        \n            \n                \n        \n            0\n            4\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            1\n            51\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            2\n            5\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            3\n            52\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n            \n        \n            \n                \n        \n            temporal\n            \n        \n            \n                \n        interval[] 1 items\n        \n            \n        \n            \n                \n        0[] 2 items\n        \n            \n        \n            \n                \n        \n            0\n            \"2020-01-01T00:00:00Z\"\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            1\n            \"2022-01-01T00:00:00Z\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n                \n            \n                \n                    \n        \n            license\n            \"CC-BY-SA-4.0\"\n        \n    \n                \n            \n                \n                    \n        \n            summaries\n            \n        \n            \n                \n        proj:code[] 1 items\n        \n            \n        \n            \n                \n        \n            0\n            \"EPSG:3035\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n            \n                \n        gsd[] 1 items\n        \n            \n        \n            \n                \n        \n            0\n            100\n        \n    \n            \n        \n    \n        \n    \n            \n        \n            \n                \n        bands[] 1 items\n        \n            \n        \n            \n                \n        \n            0\n            \n        \n            \n                \n        \n            title\n            \"SOC\"\n        \n    \n            \n        \n            \n                \n        \n            gsd\n            100\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n                \n            \n        \n    \n\n\n\n\ncoll_dict = collection.to_dict()\n\ndefault_auth = {\n    \"_auth\": {\n        \"read\": [\"anonymous\"],\n        \"write\": [\"stac-openeo-admin\", \"stac-openeo-editor\"]\n    }\n}\n\ncoll_dict.update(default_auth)\n\nresponse = requests.post(\"https://catalogue.project-a.apex.esa.int/collections\", auth=auth,json=coll_dict)\nresponse\n\n&lt;Response [201]&gt;\n\n\n\nimport shapely\n\n\ngeometry = {\n    \"type\":\"Polygon\",\n    \"coordinates\":[\n      [\n        [\n          -35.0421748,\n          67.1549879\n        ],\n        [\n          -9.3033749,\n          33.067326\n        ],\n        [\n          34.1876844,\n          31.8603398\n        ],\n        [\n          62.8622654,\n          64.3475852\n        ],\n        [\n          -35.0421748,\n          67.1549879\n        ]\n      ]\n    ]\n  }\nitem_bbox = shapely.geometry.shape(geometry).bounds\ncollection_item = pystac.Item(id='europe_aggr-orgc_00-020_mean_100_202003-202210',\n                              geometry=geometry,\n                              bbox = item_bbox,\n                              datetime=datetime(2022,3,1),\n                              collection=collection.id,\n                              properties={})\n\ncollection_item.common_metadata.gsd = 100\n\nasset = pystac.Asset(href=\"https://eoresults.esa.int/d/APEX_TEST/2020/03/01/europe_aggr-orgc_00-020_mean_100_202003-202210/europe_aggr-orgc_00-020_mean_100_202003-202210.tif\", \n                      media_type=pystac.MediaType.GEOTIFF, roles=[\"data\"])\ncollection_item.add_asset(\"SOC\", asset)\ncollection_item\n\n\n\n\n\n    \n        \n            \n                \n                    \n        \n            type\n            \"Feature\"\n        \n    \n                \n            \n                \n                    \n        \n            stac_version\n            \"1.0.0\"\n        \n    \n                \n            \n                \n                    \n        \n            id\n            \"europe_aggr-orgc_00-020_mean_100_202003-202210\"\n        \n    \n                \n            \n                \n                    \n        \n            properties\n            \n        \n            \n                \n        \n            gsd\n            100\n        \n    \n            \n        \n            \n                \n        \n            datetime\n            \"2022-03-01T00:00:00Z\"\n        \n    \n            \n        \n    \n        \n    \n                \n            \n                \n                    \n        \n            geometry\n            \n        \n            \n                \n        \n            type\n            \"Polygon\"\n        \n    \n            \n        \n            \n                \n        coordinates[] 1 items\n        \n            \n        \n            \n                \n        0[] 5 items\n        \n            \n        \n            \n                \n        0[] 2 items\n        \n            \n        \n            \n                \n        \n            0\n            -35.0421748\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            1\n            67.1549879\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        1[] 2 items\n        \n            \n        \n            \n                \n        \n            0\n            -9.3033749\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            1\n            33.067326\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        2[] 2 items\n        \n            \n        \n            \n                \n        \n            0\n            34.1876844\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            1\n            31.8603398\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        3[] 2 items\n        \n            \n        \n            \n                \n        \n            0\n            62.8622654\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            1\n            64.3475852\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        4[] 2 items\n        \n            \n        \n            \n                \n        \n            0\n            -35.0421748\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            1\n            67.1549879\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n                \n            \n                \n                    \n        links[] 0 items\n        \n    \n                \n            \n                \n                    \n        \n            assets\n            \n        \n            \n                \n        \n            SOC\n            \n        \n            \n                \n        \n            href\n            \"https://eoresults.esa.int/d/APEX_TEST/2020/03/01/europe_aggr-orgc_00-020_mean_100_202003-202210/europe_aggr-orgc_00-020_mean_100_202003-202210.tif\"\n        \n    \n            \n        \n            \n                \n        \n            type\n            \"image/tiff; application=geotiff\"\n        \n    \n            \n        \n            \n                \n        roles[] 1 items\n        \n            \n        \n            \n                \n        \n            0\n            \"data\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n                \n            \n                \n                    \n        bbox[] 4 items\n        \n            \n        \n            \n                \n        \n            0\n            -35.0421748\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            1\n            31.8603398\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            2\n            62.8622654\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            3\n            67.1549879\n        \n    \n            \n        \n    \n        \n    \n                \n            \n                \n                    \n        stac_extensions[] 0 items\n        \n    \n                \n            \n                \n                    \n        \n            collection\n            \"esa-worldsoils\"\n        \n    \n                \n            \n        \n    \n\n\n\n\nr.collection_item_create(collection.id, collection_item.to_dict())\n\nTrue",
    "crumbs": [
      "Guides",
      "Ingest STAC metadata"
    ]
  },
  {
    "objectID": "guides/stac_catalog_ingest.html#cleaning-up",
    "href": "guides/stac_catalog_ingest.html#cleaning-up",
    "title": "APEx STAC catalogue editor guide",
    "section": "Cleaning up",
    "text": "Cleaning up\nWhen the demo is done, we can simply clean up by deleting our (useless) test collection.\n\nrequests.delete(\"https://stac-openeo-dev.vgt.vito.be/collections/\" + collection.id, auth=auth)\n\n&lt;Response [204]&gt;",
    "crumbs": [
      "Guides",
      "Ingest STAC metadata"
    ]
  },
  {
    "objectID": "propagation/ondemandservices.html",
    "href": "propagation/ondemandservices.html",
    "title": "On-Demand EO Services",
    "section": "",
    "text": "Resulting on-demand services should be compliant to FAIR data principles.\nTherefore, currently APEx-compliant on-demand services should either adopt an openEO User Defined Process (UDP) or an OGC API Process implementation. There are additional APEx compliance and interoperability guidelines which help ensuring FAIR compliance.",
    "crumbs": [
      "Algorithm Services",
      "On-Demand EO Services"
    ]
  },
  {
    "objectID": "propagation/ondemandservices.html#what-is-an-on-demand-service",
    "href": "propagation/ondemandservices.html#what-is-an-on-demand-service",
    "title": "On-Demand EO Services",
    "section": "",
    "text": "Resulting on-demand services should be compliant to FAIR data principles.\nTherefore, currently APEx-compliant on-demand services should either adopt an openEO User Defined Process (UDP) or an OGC API Process implementation. There are additional APEx compliance and interoperability guidelines which help ensuring FAIR compliance.",
    "crumbs": [
      "Algorithm Services",
      "On-Demand EO Services"
    ]
  },
  {
    "objectID": "propagation/ondemandservices.html#how-to-build-an-on-demand-service",
    "href": "propagation/ondemandservices.html#how-to-build-an-on-demand-service",
    "title": "On-Demand EO Services",
    "section": "How to build an on-demand service?",
    "text": "How to build an on-demand service?\nThe easiest and most cost-effective way to build these on-demand services is to use an existing platform that is already part of the ESA Network of Resources (NoR). It is important to note that APEx itself is not a processing platform, but rather an initiative to streamline and aggregate EO based services building on top of existing EO platforms. Consequently, APEx treats compliant EO platforms or hosting environments neutrally and algorithm PIs can decide where to deploy the resulting service.\n\nWhich option to choose?\nTo help out projects, we created dedicated pages to explain the benefits of Application Package via OGC API Processes and openEO based services.\n\n\nBuilding an openEO UDP based service\nTo integrate your algorithm with openEO [1], you need to convert it into a User Defined Process ( UDP)[2]. If your application is already built using an openEO process graph or if your workflow is simple enough, the conversion to a UDP should be straightforward. If not, APEx can provide the guidance and support you need to efficiently translate your algorithm into an openEO implementation and UDP service.\nOnce your workflow is ready as an openEO process graph, you can create a UDP by parameterizing the process graph. This notebook demonstrates exactly how to do that.\nAfter your UDP is created, it can be registered in the APEx Algorithm Services Catalogue and associated with one or more backends or platforms capable of running it. As APEx continues to evolve, additional tools will become available that aid this process, including functionality to check if your service is still functional.\nFor further guidance on creating and using UDPs, you can explore the following resources:\n\nAPEx openEO process (UDP) writer guide\nAPEx openEO process (UDP) integration guide\nUDP support in Python\nJupyter notebook example\nOfficial openEO specification\n\n\n\nBuilding an OGC Application Package based service\nAnother option is to package your software using Docker containers. This approach is very generic and is particularly relevant if you have an existing piece of complex software that you want to make available as a service.\nAnother APEx compliant cloud service implementation option is to package your software using Docker containers and CWL ( Common Workflow Language)[3] wrappers, together forming an OGC Application Package [4]. This approach is very generic and is particularly relevant if you have an existing piece of complex software that you want to make available as a service.\nThis tutorial explains more about this option.\n\n\nWhat are the platforms to choose from?\nTo offer your algorithm as an on-demand service, it must be available for execution on an APEx-compliant algorithm hosting platform. Currently, APEx supports the integration of services on platforms that adhere to the APEx compliance guidelines. It is important to note that this list of supported platforms is expected to grow as APEx evolves.\nFor openEO UDPs:\n\nopenEO platform\nCDSE openEO federation\n\nFor OGC Processes/Application Package:\n\nELLIP\n\n\n\n\n\n\n\nIs Your Platform APEx-Compliant?\n\n\n\nIf your platform meets the APEx Interoperability and Compliance Guidelines, and you have onboarded your platform to the ESA Network of Resources (or plan to do that), we encourage you to reach out to the APEx team to get it added to the list.\n\n\n\n\nOnboarding your service on the ESA Network of Resources\nTo further enhance the reach of your developed EO service and to offer it in the ESA NoR portfolio as a commercial service and take advantage of the ESA NOR sponsorship, akey feature of the resulting on-demand services is that they can be onboarded in the ESA Network of Resources (NoR). This integration enhances the service’s visibility and opens up a potential additional revenue stream for the project.\nAn important step of the onboarding on the NoR is setting the price, expressed in euros per km². The final price of the service is determined by the project consortium and can be supported by APEx tooling. In general, the price needs to cover:\n\nThe processing cost\nA margin for fluctuation in processing cost\nOptionally, a revenue margin for the project\n\nCurrently, it is the responsibility of the algorithm hosting platform to add services to their NoR offering. This approach ensures that APEx retains a neutral position, allowing for direct negotiations between platform provider and service provider. Consequently, it is also the responsibility of the platforms to provide an SLA (Service Level Agreement) to the end user. In case of service malfunctions, the end user will interact directly with the platform for support.",
    "crumbs": [
      "Algorithm Services",
      "On-Demand EO Services"
    ]
  },
  {
    "objectID": "propagation/ondemandservices.html#how-can-apex-support-your-project",
    "href": "propagation/ondemandservices.html#how-can-apex-support-your-project",
    "title": "On-Demand EO Services",
    "section": "How can APEx support your project?",
    "text": "How can APEx support your project?\nThe level of support available depends on your project’s status:\n\nOngoing or upcoming ESA EOP projects\nWe recommend discussing APEx support directly with your TO.\nCompleted ESA EOP projects\nPlease reach out to us to explore how APEx can help propagate and maintain your project’s results.\nNon-ESA projects\nAPEx Algorithm Services will be available through the ESA Network of Resources (NoR) starting in 2026.\n\nThe goal of APEx is to simplify the complex task of building efficient, operational, cloud-based on-demand services. ESA recognizes that achieving this often requires specific skills that may not be present in the project consortium or that could significantly impact the project budget. To assist your project in these activities, APEx can provide the following Algorithm Services:\n\nAlgorithm Porting\nDiscover how APEx can assist your project in implementing your algorithm using APEx-compliant technologies, such as openEO User Defined Processes (UDP) or OGC Application Packages.\nAlgorithm Onboarding\nExplore how APEx can assist in deploying your algorithms within an APEx-compatible hosting platform and sharing them across the EO community through the APEx Algorithm Services Catalogue.\nAlgorithm Upscaling\nLearn how APEx facilitates and simplifies the execution of algorithms at scale.\nAlgorithm Enhancement\nDiscover how APEx supports improving the computational performance of your algorithms or workflows.\nToolbox Cloudification\nTransform the key functionalities of your favorite EO toolbox into cloud-based services with APEx.\nAlgorithm Intercomparison\nLearn how APEx facilitates comparing algorithms and workflows.\n\n\n\n\n\n\n\nNot sure which services are useful for your project?\n\n\n\nVisit our use cases page to explore various project scenarios and see how APEx can support your specific needs.",
    "crumbs": [
      "Algorithm Services",
      "On-Demand EO Services"
    ]
  },
  {
    "objectID": "propagation/intercomparison.html",
    "href": "propagation/intercomparison.html",
    "title": "Algorithm Intercomparison",
    "section": "",
    "text": "APEx facilitates algorithm intercomparison exercises, enabling the benchmarking of various algorithms against each other, providing a quality metric for EO services that implement the same capability. This aids end users in making informed decisions about which algorithm to choose.\nAdditionally, APEx assists in identifying the algorithms that would most benefit from cloudification or enhancement, ensuring that only the most effective and efficient tasks are pursued.\nThe intercomparison activities are currently under development and will continue to evolve throughout the APEx project, as it relies on APEx instantiation services and propagation services to provide the necessary tools and user interfaces for conducting intercomparison exercises.",
    "crumbs": [
      "Algorithm Services",
      "Algorithm Intercomparison"
    ]
  },
  {
    "objectID": "propagation/intercomparison.html#examples",
    "href": "propagation/intercomparison.html#examples",
    "title": "Algorithm Intercomparison",
    "section": "Examples",
    "text": "Examples\n\nIntercomparison of Sentinel-1 backscatter computation\nThe APEx framework allows to simplify intercomparison by offering different implementations of the same algorithm through the same interface. One such example is Sentinel-1 backscatter computation, for which there are three openEO implementations, all accessible via the sar_backscatter openEO process on different openEO backends:\n\n\n\nBackend\nImplementation\n\n\n\n\nEODC\nSentinel-1 toolbox ‘SNAP’ implementation\n\n\nCDSE\nOrfeo implementation\n\n\nTerrascope\nA custom implementation on the Sentinelhub\n\n\n\nWhile all three implementations implement sar_backscatter, they may vary in the range of parameters they accept. Therefore, the range of supported parameters becomes an important point of comparison, alongside the cost specified in platform credits and the correctness of the results.",
    "crumbs": [
      "Algorithm Services",
      "Algorithm Intercomparison"
    ]
  },
  {
    "objectID": "propagation/intercomparison.html#call-for-service-providers",
    "href": "propagation/intercomparison.html#call-for-service-providers",
    "title": "Algorithm Intercomparison",
    "section": "Call for service providers",
    "text": "Call for service providers\nAPEx is looking for a first category of algorithms to be intercompared. If you have an algorithm, consider onboarding it using the APEx Toolbox Cloudification support, and request your peers to do the same so that a first intercomparison can be initiated.\nIf you believe your implementation outperforms one currently available in the APEx catalog, feel free to contact us. We will further evaluate your case to determine if you have a potential candidate for intercomparison.",
    "crumbs": [
      "Algorithm Services",
      "Algorithm Intercomparison"
    ]
  },
  {
    "objectID": "propagation/onboarding.html",
    "href": "propagation/onboarding.html",
    "title": "Algorithm Onboarding",
    "section": "",
    "text": "APEx offers algorithm onboarding support as part of its propagation services to ensure that project workflows and algorithms remain accessible to the EO community beyond the project’s completion. This service not only aims to sustain and promote the use of algorithms developed during ESA projects, ensuring their longevity and continued utilisation, but also ensures alignment with the FAIR and open science data principles. This is achieved through the implementation of APEx Interoperability and Compliance Guidelines and by registering the onboarded services in the APEx Algorithm Services Catalogue for broader visibility and accessibility.\nFor the engagement with stakeholders of the algorithm, APEx collaborates closely with initiatives such as the Stakeholder Engagement Facility (SEF). Such initiatives provide outreach and training towards users to maximise the uptake of the onboarded algorithms.",
    "crumbs": [
      "Algorithm Services",
      "Algorithm Onboarding"
    ]
  },
  {
    "objectID": "propagation/onboarding.html#support-overview",
    "href": "propagation/onboarding.html#support-overview",
    "title": "Algorithm Onboarding",
    "section": "Support Overview",
    "text": "Support Overview\n\nHosting Platform Onboarding Support\nAPEx can help to identify the different hosting environments that the project can choose from based on criteria such as the chosen APEx-compliant standard (openEO or OGC API Processes) or the availability of specific datasets and processing capabilities that are required by the algorithm. If applicable, APEx can also assist in the hosting environment onboarding process, ensuring that your algorithm is available for execution on the selected platform and ready for further integration into the APEx Algorithm Services Catalogue.\n\n\n\n\n\n\nTip\n\n\n\nA full list of the known APEx-compliant algorithm hosting platforms is available here.\n\n\n\n\nAutomated Testing and Benchmarking\nTo ensure service integrity, APEx conducts periodic, automated testing and benchmarking in close collaboration with the respective algorithm developers. This ensures that the algorithms remain functional and accessible over time. APEx offers this testing service free of charge, reducing the maintenance burden on project teams.\nSince the proper functioning of services depends on various external factors, APEx asks project teams to define at least one test scenario to verify that the service remains operational. This approach provides several benefits for all parties involved:\n\nAlgorithm maintainers receive notifications about issues without needing to build their own testing and monitoring systems.\nAlgorithm hosting platforms are similarly notified of any problems, helping them address issues early.\nAPEx is able to maintain a catalogue of working services, which strengthens user trust and increases the uptake of reliable services.\nService providers on platforms with SLAs, such as those available on the NoR, can identify and resolve potential issues before users report SLA breaches. Algorithm hosting platforms may also use the automated test scenarios as part of their own reporting process for on-demand service performance.\n\nAPEx does not have a reporting procedure towards the NoR or a requirement to report on the state of a service or a potential SLA breach. Service providers are not forced to take action in case of a negative test. APEx may simply use a visual clue to indicate a (temporary) issue or may hide/remove services as part of regular catalogue housekeeping when the service provider indicates that the problem will not be resolved or is simply no longer reachable.\nTo run the defined tests, APEx also needs permission from the hosting platform and an account with sufficient credits. Collaboration of the platform may be required to fulfil this requirement.\n\n\nAPEx Algorithm Services Catalogue\nThrough comprehensive guidelines and documentation, projects can seamlessly integrate their hosted services in the APEx Algorithm Services Catalogue. This process facilitates easy browsing and access to available algorithms, fostering collaboration and innovation within the EO community.\nThe APEx Algorithm Services Catalogue allows visitors to discover and select algorithms for subsequent execution on the corresponding APEx-compliant hosting environment.\n\n\n\n\n\n\nDisclaimer\n\n\n\nAPEx relies on the APEx-compliant algorithm hosting environments for algorithm execution and the provisioning of the corresponding cloud resources. Users wishing to execute an algorithm will need a valid account on the relevant hosting platform or make a request through ESA’s Network of Resources (NoR).\n\n\nStay tuned for more detailed instructions on how to execute an algorithm through the APEx Algorithm Services Catalogue.",
    "crumbs": [
      "Algorithm Services",
      "Algorithm Onboarding"
    ]
  },
  {
    "objectID": "propagation/onboarding.html#considerations-and-project-responsibilities",
    "href": "propagation/onboarding.html#considerations-and-project-responsibilities",
    "title": "Algorithm Onboarding",
    "section": "Considerations and Project Responsibilities",
    "text": "Considerations and Project Responsibilities\n\nAlgorithm Maintenance\nWhile APEx ensures the long-term availability of the service within the catalogue, the creation, maintenance, and updating of the algorithm remain the responsibility of the original developer or the open-source community. It should be noted, however, that the APEx service solutions (openEO UDP and Application Package) already aim to limit the need for algorithm maintenance.\n\n\nAlgorithm Execution\nAlgorithm execution is carried out on one of the respective APEx-compliant algorithm hosting platforms. By default, APEx does not provide direct access to processing resources on these platforms. The user is in charge of ensuring platform access modalities, while some platforms, such as CDSE, may offer free credit tiers. Moreover, projects can request subscriptions to hosting platforms or credits for DPaaS-based algorithm execution through ESA’s Network of Resources (NoR). When resources are requested for a specific service, onboarded to the ESA NoR, APEx can facilitate access to cloud processing resources on the corresponding platform as a central gateway and catalogue of federated services.\nIn the long term, APEx plans to offer a graphical interface that simplifies the execution of onboarded services. However, it’s important to note that even with this user-friendly interface, users will still need to authenticate with the respective algorithm hosting platform and ensure they have access to the necessary processing resources on that platform.\n\n\nLarge-scale Data Processing\nOnce the algorithm has been integrated into the APEx Algorithm Services Catalogue, users can leverage it for large-scale data processing. For such activities, APEx offers additional support for algorithm upscaling.",
    "crumbs": [
      "Algorithm Services",
      "Algorithm Onboarding"
    ]
  },
  {
    "objectID": "propagation/onboarding.html#onboard-your-algorithm-on-the-apex-algorithm-services-catalogue",
    "href": "propagation/onboarding.html#onboard-your-algorithm-on-the-apex-algorithm-services-catalogue",
    "title": "Algorithm Onboarding",
    "section": "Onboard Your Algorithm on the APEx Algorithm Services Catalogue",
    "text": "Onboard Your Algorithm on the APEx Algorithm Services Catalogue\nStay tuned for more detailed instructions on how to onboard your algorithm into our catalogue.",
    "crumbs": [
      "Algorithm Services",
      "Algorithm Onboarding"
    ]
  },
  {
    "objectID": "propagation/porting.html",
    "href": "propagation/porting.html",
    "title": "Algorithm Porting",
    "section": "",
    "text": "APEx supports the porting or packaging of algorithms, ensuring they align with the APEx Interoperability and Compliance Guidelines. These guidelines are designed to ensure that algorithms are standardised and ready for integration into APEx via the Algorithm Onboarding support.\nThe goal of the algorithm porting activities is to provide additional technical support to guide projects in applying these guidelines, helping them navigate the transformation of their EO data processing workflows into APEx-compliant solutions.",
    "crumbs": [
      "Algorithm Services",
      "Algorithm Porting"
    ]
  },
  {
    "objectID": "propagation/porting.html#support-overview",
    "href": "propagation/porting.html#support-overview",
    "title": "Algorithm Porting",
    "section": "Support Overview",
    "text": "Support Overview\nAlgorithm porting is designed to assist projects at various stages of algorithm development, providing expert guidance on making EO algorithms more FAIR-compliant and APEx-compatible. The service includes several key features, as described in the following sections.\n\nInitial Algorithm Analysis\nThe first step in the algorithm porting process is an initial analysis of an algorithm to evaluate its readiness for cloud service implementation and general APEx compliance. This includes understanding the specific data inputs, outputs, programming language and processing requirements of the algorithm. Based on this analysis, APEx experts will recommend suitable technologies for implementing the algorithm, ensuring that it integrates seamlessly with APEx’s ecosystem. Currently, the possible options for the service implementation include:\n\nopenEO Process Graph (PG) and User Defined Process (UDP)\nOGC Application Package\n\nThis assessment phase ensures that the algorithm is mapped to the most appropriate APEx-compliant technology, setting a clear path for its porting. Another important outcome is an initial assessment of potential risks that may affect the outcome of the porting process. This allows the project’s ESA technical officer and APEx ESA responsible to decide if porting can continue. This initial assessment also includes an effort estimate.\n\n\nSupport for Porting Algorithms into openEO UDPs\nFor projects that already have established EO data processing or analytics workflows, APEx offers dedicated support to help transform these workflows into openEO User Defined Processes (UDP). This step optimises the algorithm for FAIR-compliant reuse and ensures that the algorithm meets the requirements set out in the APEx Interoperability and Compliance Guidelines.\nThrough time-limited expert consultations, APEx’s openEO specialists will work closely with project teams, offering hands-on guidance and resolving any technical challenges. This support covers the entire process, from adapting your existing workflow to ensuring it can be executed within an APEx-compatible EO platform as an openEO UDP.\n\n\nPackaging Your Algorithm as an OGC Application Package\nAlternatively, for existing pieces of complex software that need to be made available as a service, APEx offers support for packaging it as an OGC Application Package. Leveraging time-limited expert consultations, this process adheres to the Best Practices for Earth Observation Application Packaging, as defined by the Open Geospatial Consortium (OGC) and the EO Exploitation Platform Common Architecture (EOEPCA), championed by the European Space Agency (ESA). These practices ensure that EO applications are reproducible and portable across diverse execution environments.",
    "crumbs": [
      "Algorithm Services",
      "Algorithm Porting"
    ]
  },
  {
    "objectID": "propagation/porting.html#pathway-to-onboarding",
    "href": "propagation/porting.html#pathway-to-onboarding",
    "title": "Algorithm Porting",
    "section": "Pathway to Onboarding",
    "text": "Pathway to Onboarding\nOnce your algorithm has been successfully ported into APEx-compliant technology, the next step is to make it accessible to a broader audience. This involves onboarding it onto an APEx-compliant hosting platform, offering it as an on-demand service, and publishing it in the APEx Algorithm Services Catalogue. By being part of the APEx ecosystem, your algorithm can be reused and scaled by other users and projects, significantly increasing its visibility and impact. This approach helps to maximize the value of your work, fostering collaboration and innovation across the APEx community.\nFor detailed guidance on how APEx can assist your project in these tasks, please visit the APEx Algorithm Onboarding support page.",
    "crumbs": [
      "Algorithm Services",
      "Algorithm Porting"
    ]
  },
  {
    "objectID": "propagation/porting.html#get-started",
    "href": "propagation/porting.html#get-started",
    "title": "Algorithm Porting",
    "section": "Get Started",
    "text": "Get Started\nIf you’re ready to begin the porting process, or if you’d like more information about how APEx can support your project, please contact us. Our team is here to help you transform your algorithms into powerful, reusable tools that can thrive in the APEx ecosystem.",
    "crumbs": [
      "Algorithm Services",
      "Algorithm Porting"
    ]
  },
  {
    "objectID": "propagation/upscaling.html",
    "href": "propagation/upscaling.html",
    "title": "Algorithm Upscaling",
    "section": "",
    "text": "APEx can facilitate the execution of your openEO UDP or OGC application package-based algorithm over larger geographic areas. To support these operations, APEx requires that the algorithm is already optimised for cost-effective scalability, deployed as a service, and registered in the APEx Algorithm Services Catalogue. If needed, APEx can provide additional support to ensure these prerequisites are met.\nTo execute large-scale data processing activities, APEx provides a range of services (from guidance to liaison with Platform operators, or even the provision of tools, in some cases self-service tools) that project teams or end-users can utilise to run an algorithm at scale, using the processing capabilities of the platforms that are compatible with the APEx guidelines. This set of APEx services is designed to simplify the complexities and challenges associated with geographical and temporal upscaling, such as tiling, input/output management, and job orchestration, ensuring an efficient processing experience.",
    "crumbs": [
      "Algorithm Services",
      "Algorithm Upscaling"
    ]
  },
  {
    "objectID": "propagation/upscaling.html#job-orchestration-client-tools",
    "href": "propagation/upscaling.html#job-orchestration-client-tools",
    "title": "Algorithm Upscaling",
    "section": "Job Orchestration Client Tools",
    "text": "Job Orchestration Client Tools\nTo address the complexities inherent in job orchestration, APEx adopts a stepped approach to ensure that user requirements are fully addressed while ensuring the technical compatibility of APEx-compliant technologies and platforms. The goal is to abstract the underlying complexities, providing users with a simplified and efficient way of executing upscaling tasks.\nAPEx will start with establishing a formal procedure to gather in a structured way the upscaling requirements from ESA projects. These requirements will serve as inputs for the consortium to prepare the environment for the upscaling activities. This includes configuring a tailored environment and the pre-configuration of the job orchestration tools that enable a simplified execution of upscaling tasks.\nIn the subsequent phase, APEx will explore potential enhancements to the user experience by further automating the process.\nFor the openEO-powered platforms, users of the APEx upscaling service currently have access to the following client tools, that will be pre-configured for them by the APEx Upscaling team, based on the provided ESA project requirements:\n\nAn openEO batch job manager (Python library), which supports the creation of large-scale processing tasks for openEO developers by dividing them into separate batch jobs that can be automatically monitored. The openEO batch job manager is built on top of the openEO API and can therefore be used independently from the underlying platform.\n\nDuring the following phase of APEx, the range of openEO client tools and their usability will be further enhanced to better support comprehensive upscaling activities.\nFor the Application Package compatible platforms, users of the APEx upscaling service currently have access to the following client tools that will be pre-configured for them by the APEx Upscaling team, based on the provided ESA project requirements:\n\nAn Application Package / OGC API Process systematic processing manager (Python library), which supports the creation of large-scale processing tasks for Application Package users having technical skills to run such tool. The systematic processing job manager is built on top of the OGC API Process standard interface and can, therefore, be used to communicate with the underlying compatible platform.\n\nDuring the following phase of APEx, the range of Application Package tools and their usability will be further integrated when providing comprehensive support to upscaling activities.\nOverall, the envisioned goal is to support medium-sized upscaling activities through a tool or service requiring no coding. We define ‘medium-sized’ as an activity that does not require specific coordination with the platform in terms of required resources (more details are available here) and can be finished in less than 3 days of processing. This distinction in size of activity is important in the sense that automatic tooling cannot (yet) reliably handle all activities that are normally performed by human operators, and thus constitutes a risk. For medium-sized activities, with relatively low processing costs, this risk is smaller.",
    "crumbs": [
      "Algorithm Services",
      "Algorithm Upscaling"
    ]
  },
  {
    "objectID": "propagation/upscaling.html#data-management-client-tools",
    "href": "propagation/upscaling.html#data-management-client-tools",
    "title": "Algorithm Upscaling",
    "section": "Data Management Client Tools",
    "text": "Data Management Client Tools\nAPEx is promoting the use of state-of-the-art technologies for data processing and data visualisation, based on S3 Cloud storage as COG files encoding and with data assets registered on a STAC-compliant catalogue. Based on this, a system shall provide the level of abstraction (tiling, CRS and projections, tiles and intermediate files) for the upscaling scenarios part of the APEX upscaling activities. APEx is taking care of selecting Platforms which are supporting this state-of-the-art.\nIn terms of tooling, APEx can provide access to a dedicated Geospatial Explorer instance, meant to leverage this technology and that will be available to the ESA projects coming to APEx.",
    "crumbs": [
      "Algorithm Services",
      "Algorithm Upscaling"
    ]
  },
  {
    "objectID": "propagation/upscaling.html#support-overview",
    "href": "propagation/upscaling.html#support-overview",
    "title": "Algorithm Upscaling",
    "section": "Support Overview",
    "text": "Support Overview\nAs geographical and temporal upscaling can be challenging, APEx offers the possibility to request time-limited support for ESA projects. Based on the project needs, APEx can provide:\n\nBasic training in the use of the upscaling tools.\nSupport in defining an appropriate upscaling strategy tailored to the project’s needs.\nSelection of the most appropriate platform for the project’s upscaling task.\n\nTo make use of the upscaling tools, the following aspects should be considered by the project requesting support:\n\nThe upscaling activities and tools are designed with APEx guidelines in mind, primarily supporting algorithms that have been onboarded to the APEx Algorithm Services Catalogue. While users are able to utilise these tools for other algorithms, their proper functionality cannot be guaranteed if the algorithms do not adhere to APEx guidelines. If the algorithm has not yet been registered, APEx offers support to assist projects in integrating their algorithms into the catalogue.\nThe performance of the service can drastically affect the feasibility of upscaling. Projects can get additional support through APEx’s algorithm enhancement activities to improve the performance of their algorithm.\nBasic metrics (e.g. cost per km², total runtime per unit, e.g. per 100x100km) from the algorithm onboarding process are used to evaluate the overall feasibility of the upscaling request.\nBefore the actual production can start, preparation work may take a significant amount of time, depending on the project’s own experience and the complexity of the task. APEx does not provide operators to monitor the production process, even when a client application is provided by APEx to interact with a processing service API on the selected platform to run an upscaling scenario. This responsibility of such monitoring lies with the project team or, if available, with the platform provider’s support team.\n\nIt’s also advisable that projects contact their platform provider, as they may offer support packages specific to their environment.\n\n\n\n\n\n\nContact us\n\n\n\nIf you think your project could benefit from this service, please contact us.",
    "crumbs": [
      "Algorithm Services",
      "Algorithm Upscaling"
    ]
  },
  {
    "objectID": "propagation/upscaling.html#requirements-for-the-selection-of-platforms",
    "href": "propagation/upscaling.html#requirements-for-the-selection-of-platforms",
    "title": "Algorithm Upscaling",
    "section": "Requirements for the selection of Platforms",
    "text": "Requirements for the selection of Platforms\nUpscaling puts additional requirements on the platform providers that are not necessarily part of the APEx compliance guidelines for platforms because such requirements might make the guidelines too restrictive.\n\nThe selected platform needs to support the input dataset access for the spatiotemporal extent targeted by the upscaling (if not available natively when the provider is selected, this might be an option incurring additional costs).\n\nA platform compatible with openEO or with OGC Processes API should allow to submit and manage the number of asynchronous jobs targeted by the upscaling. The platform shall advertise the number of concurrent jobs that it can support.\nWhen the platform onboards an APEx algorithm as a service described in the NoR, it should consider upscaling in both the pricing model and the terms and conditions. The platform is allowed and encouraged to put clear constraints on the usage of the algorithm for upscaling purposes. APEx can not support users in upscaling if the platform does not advertise its capacity for scalable data processing workloads.\nThe selected platform may provide in their NoR offer an option for the long-term Cloud storage of result files ( optionally cold archive + a wake-up process). For example, the upscaled processing over continental Africa would result in thousands of Sentinel tiles being processed. The ESA Project Results Repository (PRR) is a possible option here to take care of long-term storage. For short-term storage, it can also be an object storage that is not part of the platform offering per se and for which some data move operations will be needed.\n\nThe selected platform should support the provisioning of dedicated compute power and storage service for the upscaling scenario to be executed.\n\nFor both openEO-based and Application Package-based algorithms, Platform Providers can choose whether to offer this through dedicated capacity or by utilising a large pool of elastic capacity.\nIn particular, for Application Package-based upscaling, resource provisioning can be done by:\n\nProvisioning of dedicated Cloud processing resources (for instance, Kubernetes dedicated PODs) for the hosting of the algorithm(s) selected by the project for the processing of large-scale geographical areas.\nProvisioning of dedicated HPC resources when available. For such environments, Toil is the recommended workflow engine optimised for distributed computing environments, enabling the execution of CWL workflows across multiple compute nodes. Toil can manage large-scale data processing tasks by integrating with batch processing systems (e.g., SLURM, AWS Batch).\n\n\nFor openEO-based upscaling, the following extra requirements are considered valuable for upscaling:\n\nThe authentication should support non-interactive machine-to-machine-based methods such as OIDC client credentials.\nThe export_workspace process is used to export generated results directly to object storage buckets and, in some cases, STAC API’s. Upscaling without these features is also possible but more limited.",
    "crumbs": [
      "Algorithm Services",
      "Algorithm Upscaling"
    ]
  },
  {
    "objectID": "propagation/upscaling.html#related-background-material",
    "href": "propagation/upscaling.html#related-background-material",
    "title": "Algorithm Upscaling",
    "section": "Related background material",
    "text": "Related background material\n\nLarge scale processing, by openEO platform and CDSE openEO\nJob management in openEO, a Jupyter notebook``",
    "crumbs": [
      "Algorithm Services",
      "Algorithm Upscaling"
    ]
  }
]